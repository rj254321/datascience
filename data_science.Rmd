---
title: "visualization"
author: "rj2543"
date: "2/29/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(maps)
library(mapproj)
library(nycflights13)
library(lvplot)
library(hexbin)
library(modelr)
library(hms)
library(microbenchmark)
library(stringi)
library(tidytidbits)
library(lubridate)
library(magrittr)
```

```{r data load}
data(mpg)
mpg
```

## Aesthetic

```{r aes}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
  
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy), color = "blue")
# mpg %>% 
#   ggplot() + 
#   geom_point(aes(x = displ, y = hwy, shape = cty))
# Error: A continuous variable can not be mapped to shape
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, size = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))
mpg %>% 
  ggplot() + 
  geom_point(aes(x = displ, y = hwy, colour = displ < 5))
# ?geom_point
# vignette("ggplot2-specs")
# The "munsell" package makes it easy to specific colours using a system designed by Alfred Munsell. If you invest a little in learning the system, it provides a convenient way of specifying aesthetically pleasing colours.
```

## Facets

One way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.

```{r facet}
# To facet your plot by a single variable, use facet_wrap(). The variable that you pass to facet_wrap() should be discrete.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 3)
# To facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(. ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ .)
# When using facet_grid() you should usually put the variable with more unique levels in the columns.
```

## Geometric objects

```{r geom}
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
# In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),
              show.legend = F)
# If you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
```

## Statistical transformation

```{r trans}
# You can generally use geoms and stats interchangeably. For example, you can recreate the plot using stat_count() instead of geom_bar().
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
# You might want to override the default stat. In the code below, I change the stat of geom_bar() from count (the default) to identity. This lets me map the height of the bars to the raw values of a y variable. Unfortunately when people talk about bar charts casually, they might be referring to this type of bar chart, where the height of the bar is already present in the data, or the previous bar chart where the height of the bar is generated by counting rows.
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)
demo %>% 
  mutate(cut = factor(cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
# You might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportion, rather than count.
# To find the variables computed by the stat, look for the help section titled “computed variables”.
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
# You might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarises the y values for each unique x value, to draw attention to the summary that you’re computing.
# ggplot2 provides over 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g. ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
?stat_summary
?geom_col # geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.
?stat_smooth
?geom_bar
vignette("ggplot2-specs")
```

## Position adjustment

```{r position}
# if you map the "fill" aesthetic to another variable, like "clarity": the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
# position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
# position = "dodge" places overlapping objects directly beside one another. This makes it easier to compare individual values.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
# position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
mpg %>% 
  ggplot() +
  geom_jitter(mapping = aes(x = displ, y = hwy))
?geom_count #counts the number of observations at each location, then maps the count to point area. It useful when you have discrete data and overplotting.
mpg %>% 
  ggplot() +
  geom_count(mapping = aes(x = displ, y = hwy))
mpg %>% 
  ggplot() +
  geom_boxplot(mapping = aes(y = displ, x = trans, fill = trans), show.legend = F, position = "dodge")
# the default position adjustment for geom_boxplot() is "dodge"
```

## Coordinate systems

```{r coord}
# coord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
# coord_quickmap() sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2.
nz <- map_data("nz")
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_map()
# coord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  #labs(x = NULL, y = NULL) + 
  coord_flip()
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)+ 
  coord_polar()
# coord_map projects a portion of the earth, which is approximately spherical, onto a flat 2D plane using any projection defined by the mapproj package. Map projections do not, in general, preserve straight lines, so this requires considerable computation. coord_quickmap is a quick approximation that does preserve straight lines. It works best for smaller areas closer to the equator.
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

## Layered grammar of graphics

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>

## Work flow

Don’t be lazy and use =: it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). 

```{r}
# This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen.
(y <- seq(1, 10, length.out = 5))
library(tidyverse)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
# Press Alt + Shift + K. Keyboard Shortcut Quick Reference
```

## Data Transformation

```{r filter}
(dec25 <- flights %>% 
   filter(month == 12, day == 25))
# floating point numbers: Computers use finite precision arithmetic, so remember that every number you see is an approximation.
sqrt(2)^2 == 2
near(sqrt(2) ^ 2,  2)
1/49*49 == 1
near(1 / 49 * 49, 1)
# almost any operation involving an unknown value will also be unknown
# 1. Had an arrival delay of two or more hours
flights %>% 
  filter(arr_delay >= 120)
# 2. Flew to Houston (IAH or HOU)
flights %>% 
  filter(dest %in% c("IAH", "HOU"))
# 3. Were operated by United, American, or Delta
flights %>% 
  filter(carrier %in% c("UA", "AA", "DL"))
# 4. Departed in summer (July, August, and September)
flights %>% 
  filter(month %in% c(7, 8, 9))
flights %>% 
  filter(between(month, 7, 9))
# 5. Arrived more than two hours late, but didn’t leave late
flights %>% 
  filter(arr_delay > 120 & dep_delay <= 0)
# 6. Were delayed by at least an hour, but made up over 30 minutes in flight
flights %>% 
  filter(dep_delay >= 60 & (dep_delay - arr_delay) >= 30)
# 7. Departed between midnight and 6am (inclusive)
flights %>% 
  filter(dep_time == 2400 | dep_time <= 0600)
flights %>% 
  filter(between(dep_time, 0, 600) | dep_time == 2400)
?between # This is a shortcut for x >= left & x <= right, between(x, left, right)
flights %>% 
  filter(is.na(dep_time)) # canceled flights
NA ^ 0 # = 1
NA | TRUE # TRUE
FALSE & NA # FALSE
NA * 0 # NA
```

```{r arrange}
# Missing values are always sorted at the end.
```

```{r select}
flights %>% 
  select(-(year:day))
# starts_with("abc"): matches names that begin with “abc”.
# ends_with("xyz"): matches names that end with “xyz”.
# contains("ijk"): matches names that contain “ijk”.
# matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. 
# num_range("x", 1:3): matches x1, x2 and x3.
# one_of(): Matches variable names in a character vector.
# everything(): if you have a handful of variables you’d like to move to the start of the data frame.
# last_col(): Select last variable, possibly with an offset.
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
flights %>% 
  select(one_of(vars))
flights %>% 
  select(contains("TIME")) # default in the contains() helper is: ignore.case = TRUE
flights %>% 
  select(contains("TIME", ignore.case = FALSE))
```

```{r mutate}
# you can refer to columns that you’ve just created
# If you only want to keep the new variables, use transmute():
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
# Modular arithmetic: %/% (integer division) and %% (remainder), where x == y * (x %/% y) + (x %% y).
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
# Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)). They are most useful in conjunction with group_by().
(x <- 1:10)
lag(x) # previous
lead(x) # next
# Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); and dplyr provides cummean() for cumulative means. If you need rolling aggregates (i.e. a sum computed over a rolling window), try the RcppRoll package.
# what is "a rolling window"?
cumsum(x)
cummean(x)
# Ranking: min_rank() does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks.
y <- c(1, 2, 2, NA, 3, 4)
min_rank(y)
min_rank(desc(y))
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
# ntile(y)
flights %>% 
  transmute(air_time, arr_time - dep_time)
flights %>% 
  transmute(air_time, arr_time - dep_time, (arr_time %/% 100 * 60 + arr_time %% 100) - (dep_time %/% 100 * 60 + dep_time %% 100))
```

```{r summarise}
# grouped summaries: It collapses a data frame to a single row
# Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data.
# median absolute deviation: mad(x)
# Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)]
# To count the number of distinct (unique) values, use n_distinct(x).

# When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
((per_year   <- summarise(per_month, flights = sum(flights))))

flights %>% group_by(carrier, dest) %>% summarise(n())

count(sort = TRUE)

vignette("window-functions")
# A window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The output of a window function depends on all its input values, so window functions don’t include functions that work element-wise, like + or round(). Window functions include variations on aggregate functions, like cumsum() and cummean(), functions for ranking and ordering, like rank(), and functions for taking offsets, like lead() and lag().
# lead() and lag() have an optional argument order_by. If set, instead of using the row order to determine which value comes before another, they will use another variable. This is important if you have not already sorted the data, or you want to sort one way and lag another.
```

# Exploratory Data Analysis

## Variation

```{r variation}
diamonds %>% 
  count(cut_width(carat, 0.5))

smaller <- diamonds %>% 
  filter(carat < 3)
smaller %>% 
  ggplot(aes(x = carat, color = cut)) +
  geom_freqpoly(binwidth = 0.1)

smaller %>% 
  ggplot(aes(x = carat)) + 
  geom_histogram(binwidth = 0.01)

# outlier: To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian()
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) + # count:0 - 12000
  coord_cartesian(ylim = c(0, 50))
# It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.
```

## Missing values

```{r missing value}
# Instead of dropping the entire observation/row with the strange values, replace the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA:
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))

flights %>% 
  transmute(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
    geom_histogram(mapping = aes(fill = cancelled), binwidth = 1/4)

flights %>% 
  transmute(cancelled = is.na(dep_time),
            sched_dep_time = (sched_dep_time %/% 100) + (sched_dep_time %% 100)/60) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 1/4)
```

## Covariation

If variation describes the behavior within a variable, covariation describes the behavior between variables. 
Covariation is the tendency for the values of two or more variables to vary together in a related way. 
The best way to spot covariation is to visualise the relationship between two or more variables.

```{r categorical vs continuous}
# Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.
diamonds %>% 
  ggplot(mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(color = cut), binwidth = 500)

# IQR. In the middle of the box is a line that displays the median. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.
# Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.
# A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip() # if variable names are long

# For large datasets (10,000 - 100,000), the letter-value box plot addresses both these shortcomings: it conveys more detailed information in the tails using letter values, only out to the depths where the letter values are reliable estimates of their corresponding quantiles (corresponding to tail areas of roughly 2^{-i}); “outliers” are defined as a function of the most extreme letter value shown.
diamonds %>% 
  ggplot(mapping = aes(x = cut, y = price)) + 
  geom_lv(mapping = aes(fill = cut))

ggplot(data = mpg) +
  geom_violin(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class))

ggplot(data = mpg) +
  geom_jitter(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, color = class), alpha = 0.4)
```

```{r categorical vs categorical}
# To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination.
# Covariation will appear as a strong correlation between specific x values and specific y values.
diamonds %>% 
  ggplot() +
  geom_count(mapping = aes(x = cut, y = color))

diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = n))

flights %>% 
  mutate(month = factor(month)) %>%
  group_by(dest, month) %>% 
  summarise(mean_delay = mean(arr_delay)) %>% 
  ggplot(mapping = aes(x = dest, y = month)) +
  geom_tile(mapping = aes(fill = mean_delay), na.rm = T)
```

```{r continuous vs continuous}
# geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().
ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))

# Another option is to bin one continuous variable so it acts like a categorical variable.
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), varwidth = T) # divide carat into bins of width = 0.1; make the width of the boxplot proportional to the number of points

ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20))) # display approximately the same number of points in each bin

# Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # zoom
```

## Patterns and models

```{r patterns and models}
#  The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

# Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

# Tibbles

```{r tibble}
vignette("tibble")

as_tibble()

# tibble() will automatically recycle inputs of length 1, and allows you to refer to variables that you just created.
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)

# transposed tibble: tribble() is customised for data entry in code: column headings are defined by formulas (i.e. they start with ~), and entries are separated by commas. This makes it possible to lay out small amounts of data in easy-to-read form.
tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 2, 3.6,
  "b", 1, 8.5
)

# First, you can explicitly print() the data frame and control the number of rows (n) and the width of the display. width = Inf will display all columns:
flights %>% 
  print(n = 10, width = Inf)

?tibble::enframe() # convert named atomic vectors or lists to one- or two-column data frames
```

# Data import

```{r intro}
# read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.

# Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = "#" to drop all lines that start with (e.g.) #.
read_csv(
  "# The first line of metadata
  # The second line of metadata
  x,y,z
  1,2,3", 
  comment = "#")

# na: this specifies the value (or values) that are used to represent missing values in your file
read_csv("a, b, c \n 1, 2, .", na = ".")
```

```{r parse}
# parse_*() functions take a character vector and return a more specialised vector like a logical, integer, or date
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))

parse_double("1,23", locale = locale(decimal_mark = ","))

# parse_number() ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text.
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45.")
parse_number("$123,456,789") # ignore the grouping mark

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)

# Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors.
challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
# This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
type_convert(df)

# If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats.

# write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS.

# The feather package implements a fast binary file format that can be shared across programming languages. " .feather"

# library(haven) reads SPSS, Stata, and SAS files.
# library(readxl) reads excel files (both .xls and .xlsx).
# library(DBI), along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.
# For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. 
```

# Tidy data

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

## Pivoting

```{r pivot}
# pivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. pivot_longer() makes wide tables narrower and longer
table4a = table4a %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "cases")
table4b = table4b %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "population")
left_join(table4a, table4b)

# pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows. pivot_wider() makes long tables shorter and wider
stocks <- tibble(
  year   = c(2015, 2015, 2016, 2016),
  half  = c(   1,    2,     1,    2),
  return = c(1.88, 0.59, 0.92, 0.17)
)
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer("2015":"2016", names_to = "year", values_to = "return", names_ptype = list(year = double()))
```

## Separating and uniting

```{r separate}
# it leaves the type of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better types using convert = TRUE.
table3 %>% 
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)

# You can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into.
table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
table3 %>% 
  separate(year, into = c("century", "year"), sep = -2)
```

```{r unite}
# In this case we also need to use the sep argument. The default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ""
table5 %>% 
  unite(new, century, year)
table5 %>% 
  unite(new, century, year, sep = "")
```

## Missing values

```{r missing}
(stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
))

# The way that a dataset is represented can make implicit values explicit. 
stocks %>% 
  pivot_wider(names_from = year, values_from = return)

# set values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer(
    cols = c("2015", "2016"), 
    names_to = "year", 
    values_to = "return", 
    values_drop_na = TRUE
  )

# complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary.
stocks %>% 
  complete(year, qtr)

# There’s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward.
(treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
))
# You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward, LOCF).
treatment %>% 
  fill(person)
```

## Case study

```{r case}
(who_tidy = who %>% 
  pivot_longer(
    cols = new_sp_m014:newrel_f65, 
    names_to = "key", 
    values_to = "cases", 
    values_drop_na = TRUE
  ) %>% 
  mutate(key = str_replace(key, "newrel", "new_rel")) %>% 
  separate(key, c("new", "type", "sexage"), sep = "_") %>% 
  select(-iso2, -iso3, -new) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>% 
  mutate(age_group = case_when(
    age == "014" ~ "0-14",
    age == "1524" ~ "15-24",
    age == "2534" ~ "25-34", 
    age == "3544" ~ "35-44",
    age == "4554" ~ "45-54",
    age == "5564" ~ "55-64",
    age == "65" ~ "65+",
    TRUE ~ NA_character_
  )) %>% 
  select(-age))

who_tidy %>% 
  group_by(country, year, sex) %>% 
  summarise(total_cases = sum(cases)) %>% 
  mutate(sex = ifelse(sex == "f", "female", "male")) %>% 
  ggplot(mapping = aes(x = year, y = total_cases, fill = sex)) +
  geom_col() +
  coord_cartesian(xlim = c(1995, 2013))
```

# Relational data

## Keys

* A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.

* A foreign key uniquely identifies an observation in another table. For example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.

If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key.

A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.

```{r key}
flights %>% 
  mutate(surrogate = row_number()) %>% 
  select(surrogate, everything())
```

## Mutating joins

```{r mutating join}
(flights2 <- flights %>% 
  select(year:day, hour, origin, dest, tailnum, carrier))

flights2 %>%
  select(-origin, -dest) %>% 
  left_join(airlines, by = "carrier")

# full_join()
```

```{r duplicate keys}
# One table has duplicate keys: This is useful when you want to add in additional information as there is typically a one-to-many relationship. Here, "key" is a primary key in y and a foreign key in x.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     1, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2"
))
left_join(x, y, by = "key")

# Both tables have duplicate keys: This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     3, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     2, "y3",
     3, "y4"
))
left_join(x, y, by = "key")
```

```{r define key columns}
# The default, by = NULL, uses all variables that appear in both tables, the so called natural join. e.g., the flights and weather tables match on their common variables: year, month, day, hour and origin.
flights2 %>% 
  left_join(weather) # Joining, by = c("year", "month", "day", "hour", "origin")

# A character vector, by = "x". This is like a natural join, but uses only some of the common variables. e.g., flights and planes have year variables, but they mean different things so we only want to join by tailnum.
flights2 %>% 
  left_join(planes, by = "tailnum") # year.x, year.y

# A named character vector: by = c("a" = "b"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. e.g., if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to.
flights2 %>% 
  left_join(airports, by = c("dest" = "faa"))
flights2 %>% 
  left_join(airports, by = c("origin" = "faa"))
```

```{r exercise}
(flights_avgdelay = flights %>% 
  group_by(dest) %>% 
  summarise(avgdelay = mean(arr_delay, na.rm = T)))

airports %>%
  inner_join(flights_avgdelay, by = c("faa" = "dest")) %>%
  ggplot(mapping = aes(x = lon, y = lat)) +
    borders("state") +
    geom_point(mapping = aes(size = avgdelay, color = avgdelay, alpha = 0.5)) +
    coord_quickmap()

# Joining different variables between the tables, e.g. inner_join(x, y, by = c("a" = "b")) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins) ?
```

## Filtering joins

```{r filtering join}
# Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables.

# semi_join(x, y) keeps all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows.
(top_desc <- flights %>% 
   count(dest, sort = TRUE) %>% 
   head(10))
semi_join(flights, top_desc)

# anti_join(x, y) drops all observations in x that have a match in y. Anti-joins are useful for diagnosing join mismatches. 
```

## Set operations

```{r set}
# All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets.
# intersect(x, y): return only observations in both x and y.
# union(x, y): return unique observations in x and y.
# setdiff(x, y): return observations in x, but not in y.

(df1 <- tribble(
  ~x, ~y,
   1,  1,
   2,  1
))
(df2 <- tribble(
  ~x, ~y,
   1,  1,
   1,  2
))
intersect(df1, df2)
union(df1, df2)
setdiff(df1, df2)
setdiff(df2, df1)
```

# Strings

## String basics

```{r basic}
# To include a literal single or double quote in a string you can use \ to “escape” it.
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
# if you want to include a literal backslash, you’ll need to double it up: "\\"

# the printed representation shows the escapes. To see the raw contents of the string, use writeLines()
(x <- c("\"", "\\"))
writeLines(x)

# "\n": newline; "\t": tab. complete list: ?'"', or ?"'". like "\u00b5", this is a way of writing non-English characters that works on all platforms.
(x <- "\u00b5")
writeLines(x)
```

### String length

```{r length}
# str_length() tells you the number of characters in a string.
str_length(c("a", "R for data science", NA)) # include space

# The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions.
```

### Combining strings

```{r combine}
str_c("x", "y")
str_c("x", "y", "z")
str_c("x", "y", sep = ", ")

# missing values are contagious. If you want them to print as "NA", use str_replace_na().
x <- c("abc", NA)
str_c("|-", x, "-|")
str_c("|-", str_replace_na(x), "-|")

# As shown above, str_c() is vectorised, and it automatically recycles shorter vectors to the same length as the longest.
str_c("prefix-", c("a", "b", "c"), "-suffix")

# Objects of length 0 are silently dropped. This is particularly useful in conjunction with if.
name <- "Hadley"
time_of_day <- "morning"
birthday <- FALSE
str_c(
  "Good ", time_of_day, " ", name,
  if (birthday) " and HAPPY BIRTHDAY",
  "."
)

# To collapse a vector of strings into a single string, use collapse.
str_c(c("x", "y", "z"), collapse = ", ")
```

### Subsetting strings

```{r subset}
# str_sub() takes start and end arguments which give the (inclusive) position of the substring.
(x <- c("Apple", "Banana", "Pear"))
str_sub(x, 1, 3) # vectorized
str_sub(x, -3, -1) # negative numbers count backwards from end

# str_sub() won’t fail if the string is too short: it will just return as much as possible.
str_sub("ab", 1, 5)

str_sub(x, 1, 1) <- str_to_lower(str_sub(x, 1, 1))
```

### Locales

```{r locale}
dog <- "The quick brown dog in the rain"
str_to_upper(dog)
str_to_lower(dog)
str_to_title(dog)
str_to_sentence("the quick brown dog in the rain")

# different languages have different rules for changing case. You can pick which set of rules to use by specifying a locale.
# Turkish has two i's: with and without a dot, and it has a different rule for capitalising them.
str_to_upper(c("i", "ı"))
str_to_upper(c("i", "ı"), locale = "tr")

# The base R order() and sort() functions sort strings using the current locale. If you want robust behaviour across different computers, you may want to use str_sort() and str_order() which take an additional locale argument.
x <- c("apple", "eggplant", "banana")
str_sort(x, locale = "en")  # English
str_sort(x, locale = "haw") # Hawaiian

?str_wrap
?str_trim
```

## Matching patterns with regular expressions

### Basic matches

```{r basic match}
# the simplest patterns match exact strings
x <- c("apple", "banana", "pear")
str_view(x, "an")

# .: which matches any character (except a newline)
str_view(x, ".a.")

# So to match an ".", you need the regexp "\.". Unfortunately this creates a problem. We use strings to represent regular expressions, and "\" is also used as an escape symbol in strings. So to create the regular expression "\." we need the string "\\."
# To create the regular expression, we need \\
(dot <- "\\.")
# But the expression itself only contains one:
writeLines(dot)
# And this tells R to look for an explicit.
str_view(c("abc", "a.c", "bef"), "a\\.c")

# to match a literal \ you need to write "\\\\"
(x <- "a\\b")
writeLines(x)
str_view(x, "\\\\")
```

### Anchors

```{r anchor}
# ^ to match the start of the string.
# $ to match the end of the string.
x <- c("apple", "banana", "pear")
str_view(x, "^a")
str_view(x, "a$")
# if you begin with power (^), you end up with money ($).

# To force a regular expression to only match a complete string, anchor it with both ^ and $.
x <- c("apple pie", "apple", "apple cake")
str_view(x, "apple")
str_view(x, "^apple$")

# You can also match the boundary between words with \b. when I want to find the name of a function that’s a component of other functions
x <- c("sum", "summarise", "summary", "rowsum")
str_view(x, "\bsum\b") # ???

str_view(x, "^sum$", match = TRUE)
```

### Character classes and alternatives

```{r character class}
# \d: matches any digit.
# \s: matches any whitespace (e.g. space, tab, newline).
# [abc]: matches a, b, or c.
# [^abc]: matches anything except a, b, or c.

# A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex.
str_view(c("abc", "a.c", "a*c", "a c"), "a[.]c")
str_view(c("abc", "a.c", "a*c", "a c"), ".[*]c")
str_view(c("abc", "a.c", "a*c", "a c"), "a[ ]")

# You can use alternation to pick between one or more alternative patterns. e.g., abc|d..f will match either ‘“abc”’, or "deaf". Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz.
str_view(c("grey", "gray"), "gr(e|a)y")
str_view(c("grey", "gray"), "gray|grey")
```

### Repetition

```{r repeat}
# control how many times a pattern matches:
# ?: 0 or 1
# +: 1 or more
# *: 0 or more
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?")
str_view(x, "CC+")
str_view(x, "C[LX]+")

# Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+.

# You can also specify the number of matches precisely:
# {n}: exactly n
# {n,}: n or more
# {,m}: at most m
# {n,m}: between n and m
str_view(x, "C{2}")
str_view(x, "C{2,}")
str_view(x, "C{2,3}")

# By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them.
str_view(x, "C{2,3}?")
str_view(x, "C[LX]+?")
```

### Grouping and backreferences

```{r group}
# Parentheses create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \1, \2 etc. e.g., the following regular expression finds all fruits that have a repeated pair of letters.
str_view(fruit, "(..)\\1", match = TRUE)
str_view(fruit, "(.)(.)\\2\\1", match = T)
str_view(fruit, "(.).\\1.\\1", match = T)
```

## Tools

### Detect matches

```{r detect}
x <- c("apple", "banana", "pear")
str_detect(x, "e")

# How many common words start with t?
sum(str_detect(words, "^t"))
# What proportion of common words end with a vowel?
mean(str_detect(words, "[aeiou]$"))

words[str_detect(words, "x$")]
str_subset(words, "x$")

(df <- tibble(
  word = words, 
  i = seq_along(word) # i = row_number(word)
))
df %>% 
  filter(str_detect(word, "x$"))

# rather than a simple yes or no, it tells you how many matches there are in a string
x <- c("apple", "banana", "pear")
str_count(x, "a")
# On average, how many vowels per word?
mean(str_count(words, "[aeiou]"))

df %>% 
  mutate(
    vowels = str_count(word, "[aeiou]"),
    consonants = str_count(word, "[^aeiou]")
  )

# many stringr functions come in pairs: one function works with a single match, and the other works with all matches. The second function will have the suffix _all.
str_view_all("abababa", "aba")
```

### Extract matches

```{r extract}
colours <- c("red", "orange", "yellow", "green", "blue", "purple")
(colour_match <- str_c(colours, collapse = "|"))
has_colour <- str_subset(sentences, colour_match)
(matches <- str_extract(has_colour, colour_match)) # str_extract() only extracts the first match.

more <- sentences[str_count(sentences, colour_match) > 1]
str_view_all(more, colour_match)
# str_extract(more, colour_match) # str_extract() only extracts the first match
str_extract_all(more, colour_match) # return a list
# If you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest.
str_extract_all(more, colour_match, simplify = TRUE)
x <- c("a", "a b", "a b c")
str_extract_all(x, "[a-z]", simplify = TRUE)
```

### Grouped matches

```{r group}
# You can also use parentheses to extract parts of a complex match. e.g., we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space.
noun <- "(a|the) ([^ ]+)"
has_noun <- sentences %>%
  str_subset(noun) %>%
  head(10)
has_noun %>% 
  str_extract(noun) # str_extract() gives us the complete match

# str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group。
has_noun %>% 
  str_match(noun)
# Like str_extract(), if you want all matches for each string, you’ll need str_match_all().

# If your data is in a tibble, it’s often easier to use tidyr::extract(). It works like str_match() but requires you to name the matches, which are then placed in new column.
tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("article", "noun"), "(a|the) ([^ ]+)", 
    remove = FALSE
  )
```

### Replacing matches

```{r replace}
x <- c("apple", "pear", "banana")
str_replace(x, "[aeiou]", "-")
str_replace_all(x, "[aeiou]", "-")

# With str_replace_all() you can perform multiple replacements by supplying a named vector.
x <- c("1 house", "2 cars", "3 people")
str_replace_all(x, c("1" = "one", "2" = "two", "3" = "three"))

# you can use backreferences to insert components of the match. In the following code, flip the order of the second and third words.
sentences %>% 
  str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") %>% 
  head(5)
```

### Splitting

```{r split}
sentences %>%
  head(5) %>% 
  str_split(" ") # each component might contain a different number of pieces, return a list

"a|b|c|d" %>% 
  str_split("\\|") %>% 
  .[[1]] # working with a length-1 vector, just extract the first element of the list

sentences %>%
  head(5) %>% 
  str_split(" ", simplify = TRUE) # return a matrix

# request a maximum number of pieces
fields <- c("Name: Hadley", "Country: NZ", "Age: 35")
fields %>% 
  str_split(": ", n = 2, simplify = TRUE)

# Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary().
x <- "This is a sentence.  This is another sentence."
str_view_all(x, boundary()) # each character
str_view_all(x, boundary("word")) # each word
# str_view_all(x, boundary("line_break"))
str_view_all(x, boundary("sentence")) # each sentence
str_split(x, " ")[[1]] # including extra " " between
str_split(x, boundary("word"))[[1]] # no extra " ", no punctuation
```

### Find matches

str_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them.

## Other types of pattern

```{r other pattern}
# The regular call: str_view(fruit, "nana") is shorthand for str_view(fruit, regex("nana")).

# ignore_case = TRUE allows characters to match either their uppercase or lowercase forms.
bananas <- c("banana", "Banana", "BANANA")
str_view(bananas, regex("banana", ignore_case = TRUE))

# multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string.
x <- "Line 1\nLine 2\nLine 3"
writeLines(x)
str_extract_all(x, "^Line")[[1]]
str_extract_all(x, regex("^Line", multiline = TRUE))[[1]]

# comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: "\\ ".
phone <- regex("
  \\(?     # optional opening parens
  (\\d{3}) # area code
  [) -]?   # optional closing parens, space, or dash
  (\\d{3}) # another three numbers
  [ -]?    # optional space or dash
  (\\d{4}) # four more numbers
  ", comments = TRUE)
str_match("514-791-8141", phone)
str_match("(347)937-0461", phone)

# dotall = TRUE allows . to match everything, including \n.

# fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example.
microbenchmark(
  fixed = str_detect(sentences, fixed("the")),
  regex = str_detect(sentences, "the"),
  times = 20
)

# coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. Unfortunately different parts of the world use different rules!

x <- "This is a sentence."
str_view_all(x, boundary("word"))
str_extract_all(x, boundary("word"))
```

## Other uses of regular expressions

```{r regular expression}
# apropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function.
apropos("replace")

# dir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with:
dir(pattern = "\\.Rmd$")

# (If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx()). ?
```

## Stringi

Stringi package contains almost every function you might ever need: stringi has 244 functions to stringr’s 49.

If you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_.

# Factors

## Creating factors

```{r create factor}
x1 <- c("Dec", "Apr", "Jan", "Mar")
(y1 <- factor(x1, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")))
sort(y1)

# any values not in the set will be silently converted to NA.
(y2 <- factor(c("Dec", "Apr", "Jam", "Mar"), levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))) 
parse_factor(c("Dec", "Apr", "Jam", "Mar"), levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) # warning

# If you omit the levels, they’ll be taken from the data in alphabetical order.

# Sometimes you’d prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to unique(x), or after the fact, with fct_inorder().
(f1 <- factor(x1, levels = unique(x1)))
(f2 <- x1 %>% factor() %>% fct_inorder())

# If you ever need to access the set of valid levels directly, you can do so with levels().
levels(f2)
```

## General Social Survey

```{r GSS}
gss_cat

ggplot(gss_cat, aes(race)) +
  geom_bar() # By default, ggplot2 will drop levels that don’t have any values. 
# You can force them to display with:
ggplot(gss_cat, aes(race)) +
  geom_bar() +
  scale_x_discrete(drop = FALSE)

ggplot(gss_cat, aes(rincome)) + 
  geom_bar() +
  coord_flip()

gss_cat %>% 
  count_by(relig) %>% 
  arrange(desc(n))

gss_cat %>% 
  count_by(partyid) %>% 
  arrange(desc(n))

gss_cat %>% 
  select(relig, denom)
gss_cat %>% 
  count(relig, denom) %>% 
  arrange(desc(n))
gss_cat %>% 
  ggplot(aes(x = relig, y = denom)) +
  geom_point()
gss_cat %>% 
  filter(denom != "Not applicable") %>% 
  count(relig, denom)
# denom is related to "Protestant" in relig
```

## Modifying factor order

```{r modify order}
relig_summary <- gss_cat %>%
  group_by(relig) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(relig_summary, aes(tvhours, relig)) + geom_point()

# fct_reorder() takes three arguments:
# f, the factor whose levels you want to modify.
# x, a numeric vector that you want to use to reorder the levels.
# Optionally, fun, a function that’s used if there are multiple values of x for each value of f. The default value is median.
ggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +
  geom_point()

# fct_relevel(): It takes a factor, f, and then any number of levels that you want to move to the front of the line.
rincome_summary <- gss_cat %>%
  group_by(rincome) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(rincome_summary, aes(age, fct_relevel(rincome, "Not applicable"))) +
  geom_point()

# fct_reorder2() reorders the factor by the y values associated with the largest x values. This makes the plot easier to read because the line colours line up with the legend.
by_age <- gss_cat %>%
  filter(!is.na(age)) %>%
  count(age, marital) %>%
  group_by(age) %>%
  mutate(prop = n / sum(n))
ggplot(by_age, aes(age, prop, colour = marital)) +
  geom_line(na.rm = TRUE)
ggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +
  geom_line() +
  labs(colour = "marital") # change the name of the legend from "fct_reorder2(marital, age, prop)" to "marital"
# fct_reorder() is useful for 1d displays where the factor is mapped to position; fct_reorder2() for 2d displays where the factor is mapped to a non-position aesthetic. fct_reorder2(.f, .x, .y, .fun = last2, ..., .desc = TRUE), last2() finds the last value of y when sorted by x. 

Usage

# For bar plots, you can use fct_infreq() to order levels in increasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. You may want to combine with fct_rev().
gss_cat %>%
  mutate(marital = marital %>% fct_infreq()) %>%
  ggplot(aes(marital)) +
    geom_bar() # decreasing
gss_cat %>%
  mutate(marital = marital %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(marital)) +
    geom_bar() # increasing
```

## Modifying factor levels

```{r modify level}
# More powerful than changing the orders of the levels is changing their values. This allows you to clarify labels for publication, and collapse levels for high-level displays.
gss_cat %>% count(partyid)
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat"
  )) %>%
  count(partyid)
# fct_recode() will leave levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.

# To combine groups, you can assign multiple old levels to the same new level.
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat",
    "Other"                 = "No answer",
    "Other"                 = "Don't know",
    "Other"                 = "Other party"
  )) %>%
  count(partyid)

# If you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels.
gss_cat %>%
  mutate(partyid = fct_collapse(partyid,
    "other" = c("No answer", "Don't know", "Other party"),
    "rep" = c("Strong republican", "Not str republican"),
    "ind" = c("Ind,near rep", "Independent", "Ind,near dem"),
    "dem" = c("Not str democrat", "Strong democrat")
  )) %>%
  count(partyid)

# Sometimes you just want to lump together all the small groups to make a plot or table simpler.
gss_cat %>%
  mutate(relig = fct_lump(relig)) %>%
  count(relig)
# The default behaviour is to progressively lump together the smallest groups, ensuring that the aggregate is still the smallest group. In this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’ve probably over collapsed. Instead, we can use the n parameter to specify how many groups (excluding other) we want to keep.
gss_cat %>%
  mutate(relig = fct_lump(relig, n = 10)) %>%
  count(relig, sort = TRUE)
  # %>% print(n = Inf)
```

# Dates and times

## Creating date/times

### From strings

```{r date-time from string}
today()
now()

ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")
ymd(20170131)

ymd_hms("2017-01-31 20:11:59")
mdy_hm("01/31/2017 08:01")

# force the creation of a date-time from a date by supplying a timezone
ymd(20170131, tz = "UTC")
str(ymd(20170131, tz = "UTC"))
```

### From individual components

```{r date-time from components}
flights %>% 
  select(year, month, day) %>% 
  mutate(depart_day = make_date(year, month, day))
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute))

make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}
(flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time")))

flights_dt %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day
flights_dt %>% 
  filter(dep_time < ymd(20130102)) %>% # date: 20130101
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 600) # 600 seconds = 10 minutes
# Note that when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means 1 day. For dates, 1 means 1 day.
```

### From other types

```{r date-time from other}
# switch between a date-time and a date
as_datetime(today())
as_date(now())

# Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().
as_datetime(60 * 60 * 10)
as_date(365 * 10 + 2)

d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
mdy(d1)
ymd(d2)
dmy(d3)
mdy(d4)
mdy(d5)
```

## Date-time components

### Getting components

```{r get components}
# pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second().
mday(today())
yday(today())
wday(today()) # 1-7, Sunday is the 1st day
hour(now())
minute(now())
second(now())

# For month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.
month(today(), label = TRUE, abbr = FALSE)
wday(today(), label = TRUE, abbr = FALSE)

flights_dt %>% 
  mutate(wday = wday(dep_time, label = TRUE)) %>% 
  ggplot(aes(x = wday)) +
    geom_bar()

flights_dt %>% 
  mutate(minute = minute(dep_time)) %>% 
  group_by(minute) %>% 
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    n = n()) %>% 
  ggplot(aes(minute, avg_delay)) +
    geom_line()

sched_dep <- flights_dt %>% 
  mutate(minute = minute(sched_dep_time)) %>% 
  group_by(minute) %>% 
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    n = n())
ggplot(sched_dep, aes(minute, avg_delay)) +
  geom_line()
ggplot(sched_dep, aes(minute, n)) +
  geom_line()
```

### Rounding

```{r round}
# floor_date(), round_date(), and ceiling_date(): takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to.
flights_dt %>% 
  count(week = floor_date(dep_time, "week")) %>% # plot the number of flights per week
  ggplot(aes(week, n)) +
    geom_line()
```

### Setting components

```{r set components}
(datetime <- ymd_hms("2016-07-08 12:34:56"))
year(datetime) <- 2020
datetime
month(datetime) <- 01
datetime
hour(datetime) <- hour(datetime) + 1
datetime

update(datetime, year = 2020, month = 2, mday = 2, hour = 2)

# If values are too big, they will roll-over.
ymd("2015-02-01") %>% 
  update(mday = 30)
ymd("2015-02-01") %>% 
  update(hour = 400)
ymd("2020-01-31") %>% 
  update(month = 2)

# show the distribution of flights across the course of the day for every day of the year
flights_dt %>% 
  mutate(dep_hour = update(dep_time, yday = 1)) %>% 
  ggplot(aes(dep_hour)) +
    geom_freqpoly(binwidth = 300)
# Setting larger components of a date to a constant is a powerful technique that allows you to explore patterns in the smaller components.
```

## Time spans

### Durations

Durations: represent an exact number of seconds.

```{r duration}
# How old are you?
(h_age <- today() - ymd(19941021)) # a difftime class object

as.duration(h_age) # in second
dseconds(15)
dminutes(10)
dhours(c(12, 24))
ddays(0:5)
dweeks(3)
dyears(1)
# Larger units are created by converting minutes, hours, days, weeks, and years to seconds at the standard rate (60 seconds in a minute, 60 minutes in an hour, 24 hours in day, 7 days in a week, 365 days in a year).

2 * dyears(1)
dyears(1) + dweeks(12) + dhours(15)
(tomorrow <- today() + ddays(1))
(last_year <- today() - dyears(1)) # be careful of leap year
(one_pm <- ymd_hms("2016-03-12 13:00:00", tz = "America/New_York"))
one_pm + ddays(1) # The time zone has changed. Because of DST, March 12 only has 23 hours, so if we add a full days worth of seconds we end up with a different time.
```

### Periods

Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months.

```{r period}
seconds(15)
minutes(10)
hours(c(12, 24))
days(7)
months(1:6)
weeks(3)
years(1)

10 * (months(6) + days(1))
days(50) + hours(25) + minutes(2)

# A leap year
ymd("2016-01-01") + dyears(1) # 2016-12-31
ymd("2016-01-01") + years(1)
# # Daylight Savings Time
one_pm + ddays(1)
one_pm + days(1)

flights_dt %>% 
  filter(arr_time < dep_time) 
# Overnight flights: We used the same date information for both the departure and the arrival times, but these flights arrived on the following day.
flights_dt <- flights_dt %>% 
  mutate(
    overnight = arr_time < dep_time, # TRUE/FALSE = 0/1
    arr_time = arr_time + days(overnight * 1),
    sched_arr_time = sched_arr_time + days(overnight * 1)
  )
flights_dt %>% 
  filter(overnight, arr_time < dep_time) 
```

### Intervals

```{r interval}
years(1)/days(1)

# An interval is a duration with a starting point: that makes it precise so you can determine exactly how long it is.
next_year <- today() + years(1)
(today() %--% next_year) / ddays(1)
(today() %--% next_year) %/% days(1)
```

### Summary

If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.

## Time zones

```{r time zone}
Sys.timezone()

length(OlsonNames())
head(OlsonNames())

# the time zone is an attribute of the date-time that only controls printing
(x1 <- ymd_hms("2015-06-01 12:00:00", tz = "America/New_York"))
(x2 <- ymd_hms("2015-06-01 18:00:00", tz = "Europe/Copenhagen"))
(x3 <- ymd_hms("2015-06-02 04:00:00", tz = "Pacific/Auckland"))
x1-x2
x2-x3

#  Operations that combine date-times, like c(), will often drop the time zone. In that case, the date-times will display in your local time zone.
(x4 <- c(x1, x2, x3))

# Keep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display.
(x4a <- with_tz(x4, tzone = "Australia/Lord_Howe"))
x4a - x4
# Change the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.
(x4b <- force_tz(x4, tzone = "Australia/Lord_Howe"))
x4b - x4
```

# Pipes

## Piping alternatives

### Intermediate steps

The simplest approach is to save each step as a new object.

### Overwrite the original

### Function composition

Another approach is to abandon assignment and just string the function calls together.

### Use the pipe

```{r pipe}
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mice) %>%
  bop(on = head)
# The pipe works by performing a “lexical transformation”: behind the scenes, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. When you run a pipe like the one above, magrittr does something like this:
my_pipe <- function(.) {
  . <- hop(., through = forest)
  . <- scoop(., up = field_mice)
  bop(., on = head)
}
my_pipe(foo_foo)

# pipe won’t work for two classes of functions:
# 1. Functions that use the current environment. e.g., assign() will create a new variable with the given name in the current environment:
assign("x", 10)
x
"x" %>% assign(100)
x
# The use of assign with the pipe does not work because it assigns it to a temporary environment used by %>%. If you do want to use assign with the pipe, you must be explicit about the environment:
env <- environment()
"x" %>% assign(100, envir = env)
x
# Other functions with this problem include get() and load().
# 2. Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behaviour.
# One place that this is a problem is tryCatch(), which lets you capture and handle errors:
tryCatch(stop("!"), error = function(e) "An error")
stop("!") %>% 
  tryCatch(error = function(e) "An error")
# There are a relatively wide class of functions with this behaviour, including try(), suppressMessages(), and suppressWarnings() in base R.
```

## When not to use the pipe

* Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent.

* You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe.

* You are starting to think about a directed graph with a complex dependency structure. Pipes are fundamentally linear and expressing complex relationships with them will typically yield confusing code.

## Other tools from magrittr

```{r other tool}
# When working with more complex pipes, it’s sometimes useful to call a function for its side-effects. Maybe you want to print out the current object, or plot it, or save it to disk. Many times, such functions don’t return anything, effectively terminating the pipe. you can use the “tee” pipe. %T>% works like %>% except that it returns the left-hand side instead of the right-hand side. ?
rnorm(100) %>%
  matrix(ncol = 2) %>%
  plot() %>%
  str()
#>  NULL
rnorm(100) %>%
  matrix(ncol = 2) %T>%
  plot() %>%
  str()

# If you’re working with functions that don’t have a data frame based API (i.e. you pass them individual vectors, not a data frame and expressions to be evaluated in the context of that data frame), you might find %$% useful. It “explodes” out the variables in a data frame so that you can refer to them explicitly.
mtcars %$%
  cor(disp, mpg)

# For assignment magrittr provides the %<>% operator.
mtcars <- mtcars %>% 
  transform(cyl = cyl * 2)
mtcars %<>% transform(cyl = cyl * 2)
```

# Functions

## When should you write a function?

```{r write function}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(c(0, 5, 10))

rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```

## Functions are for humans and computers

```{r human computer}
# Another important use of comments is to break up your file into easily readable chunks. Use long lines of - and = to make it easy to spot the breaks.

# Load data --------------------------------------

# Plot data --------------------------------------

# RStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl + Shift + R), and will display them in the code navigation drop-down at the bottom-left of the editor. ?
```

## Conditional execution

### Conditions

```{r condition}
# The condition must evaluate to either TRUE or FALSE. If it’s a vector, you’ll get a warning message; if it’s an NA, you’ll get an error. 
if (c(TRUE, FALSE)) {}
if (NA) {}

# You can use || (or) and && (and) to combine multiple logical expressions. These operators are “short-circuiting”: as soon as || sees the first TRUE it returns TRUE without computing anything else. As soon as && sees the first FALSE it returns FALSE.
# You should never use | or & in an if statement: these are vectorised operations that apply to multiple values (that’s why you use them in filter()). If you do have a logical vector, you can use any() or all() to collapse it to a single value.

# Be careful when testing for equality. == is vectorised, which means that it’s easy to get more than one output. Either check the length is already 1, collapse with all() or any(), or use the non-vectorised identical(). identical() is very strict: it always returns either a single TRUE or a single FALSE, and doesn’t coerce types. This means that you need to be careful when comparing integers and doubles.
identical(0L, 0)
```

### Multiple conditions

```{r multiple conditions}
if (this) {
  # do that
} else if (that) {
  # do something else
} else {
  # 
}

op = function(x, y, op) {
   switch(op,
     plus = x + y,
     minus = x - y,
     times = x * y,
     divide = x / y,
     stop("Unknown op!")
   )
}
op(1, 2, "plus")
op(1, 2, "minus")
op(1, 2, "times")
op(1, 2, "divide")
op(1, 2, "squareroot")

# cut(): discretise continuous variables
# Convert Numeric to Factor, divides the range of x into intervals and codes the values in x according to which interval they fall.
Z <- rnorm(10000)
table(cut(Z, breaks = -6:6))
table(cut(Z, breaks = -6:6, labels = FALSE))
hist(Z, breaks = -6:6, plot = FALSE)$counts
```

### Code style

```{r code style}
if (y < 0 && debug) {
  message("Y is negative")
}

if (y == 0) {
  log(x)
} else {
  y ^ x
}

# It’s ok to drop the curly braces if you have a very short if statement that can fit on one line.
y <- 10
x <- if (y < 20) "Too low" else "Too high"
if (y < 20) {
  "Too low"
} else {
  "Too high"
}

switch(x, 
  a = ,
  b = "ab",
  c = ,
  d = "cd"
)
```

## Function arguments

```{r argument}
# Generally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument.
# Compute confidence interval around mean using normal approximation
mean_ci <- function(x, conf = 0.95) {
  se <- sd(x) / sqrt(length(x))
  alpha <- 1 - conf
  mean(x) + se * qnorm(c(alpha/2, 1 - alpha/2))
}
x <- runif(100)
mean_ci(x)
mean_ci(x, conf = 0.99)
```

### Choosing names

* x, y, z: vectors.

* w: a vector of weights.

* df: a data frame.

* i, j: numeric indices (typically rows and columns).

* n: length, or number of rows.

* p: number of columns.

### Checking values

```{r check value}
# It’s good practice to check important preconditions, and throw an error with stop(), if they are not true.
wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  sum(w*x)/sum(w)
}
wt_mean(1:6, 1:3)

# stopifnot(): checks that each argument is TRUE, and produces a generic error message if not.
wt_mean <- function(x, w, na.rm = FALSE) {
  stopifnot(is.logical(na.rm), length(na.rm) == 1)
  stopifnot(length(x) == length(w))
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
wt_mean(1:6, 6:1, na.rm = "foo")
```

### Dot-dot-dot (...)

```{r ...}
# Many functions in R take an arbitrary number of inputs.
# ... (pronounced dot-dot-dot): captures any number of arguments that aren’t otherwise matched. 
# It’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function.
commas <- function(...) {
  str_c(..., collapse = ", ")
}
commas(letters[1:10])

rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
```

## Return values

### Explicit return statements

```{r explicit return}
# The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). A common reason to do this is because the inputs are empty.
complicated_function <- function(x, y, z) {
  if (length(x) == 0 || length(y) == 0) {
    return(0)
  }
    
  # Complicated code here
}

f <- function() {
  if (!x) {
    return(something_short)
  }

  # Do 
  # something
  # that
  # takes
  # many
  # lines
  # to
  # express
}
```

### Writing pipeable functions

```{r pipeable}
# With transformations, an object is passed to the function’s first argument and a modified object is returned. 
# With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline.
show_missings <- function(df) {
  n <- sum(is.na(df))
  cat("Missing values: ", n, "\n", sep = "")
  
  invisible(df)
}
# If we call it interactively, the invisible() means that the input df doesn’t get printed out.
show_missings(mtcars)
# But it’s still there, it’s just not printed by default.
x <- show_missings(mtcars)
class(x) # data.frame
dim(x) # 32*11
mtcars %>% 
  show_missings() %>% 
  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% 
  show_missings() 
```

# Vectors

## Vector basics

```{r basic}
# NULL is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0.
typeof(letters)
typeof(1:10)
typeof(c(1:10))
x <- list("a", "b", 1:10)
length(x)

# Vectors can also contain arbitrary additional metadata in the form of attributes. These attributes are used to create augmented vectors which build on additional behaviour.
# Factors are built on top of integer vectors.
# Dates and date-times are built on top of numeric vectors.
# Data frames and tibbles are built on top of lists.
```

## Important types of atomic vector

### Logical

```{r logical}
# they can take only three possible values: FALSE, TRUE, and NA.
typeof(1:10 %% 3 == 0)
typeof(c(TRUE, TRUE, FALSE, NA))
```

### Numeric

```{r numeric}
# Numbers are doubles by default. To make an integer, place an L after the number:
typeof(1)
typeof(1L)
1.5L # integer literal 1.5L contains decimal; using numeric value

# Doubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory.
(x <- sqrt(2) ^ 2)
x - 2 # 4.440892e-16
# Instead of comparing floating point numbers using ==, you should use dplyr::near() which allows for some numerical tolerance.

# Integers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf. All three special values NaN, Inf and -Inf can arise during division:
c(-1, 0, 1)/0
# Avoid using == to check for these other special values. Instead use the helper functions is.finite(), is.infinite(), and is.nan():
x <- c(0, -Inf, Inf, NA, NaN)
is.finite(x)
is.infinite(x)
is.na(x)
is.nan(x)
```

### Character

### Missing values

```{r missing}
typeof(NA)
typeof(NA_integer_)
typeof(NA_real_)
typeof(NA_character_)
```

## Using atomic vectors

### Coercion

```{r coercion}
# from integer to logical:
if (length(x)) {
  # do something
}
# 0 is converted to FALSE and everything else is converted to TRUE
# Instead be explicit: length(x) > 0

# create a vector containing multiple types with c(): the most complex type always wins.
typeof(c(TRUE, 1L))
typeof(c(1L, 1.5))
typeof(c(1.5, "a"))
```

### Test functions

```{r test function}
is_logical()	
is_integer()			
is_double()		
is_numeric() # integer & double		
is_character()
is_atomic()	# everything but list	
is_list()
is_vector() # atomic & list

# Each predicate also comes with a “scalar” version, like is_scalar_atomic(), which checks that the length is 1.
```

### Scalars and recycling rules

```{r scalar recycle}
# vector recycling: because the shorter vector is repeated, or recycled, to the same length as the longer vector.
1:10 + 1:2
# This is silent except when the length of the longer is not an integer multiple of the length of the shorter.
1:10 + 1:3

# the vectorised functions in tidyverse will throw errors when you recycle anything other than a scalar. If you do want to recycle, you’ll need to do it yourself with rep().
tibble(x = 1:4, y = 1:2)
tibble(x = 1:4, y = 1)
tibble(x = 1:4, y = rep(1:2, 2))
tibble(x = 1:4, y = rep(1:2, each = 2))
```

### Naming vectors

```{r name}
c(x = 1, y = 2, z = 4)
set_names(1:3, c("a", "b", "c"))
```

### Subsetting

```{r subset}
# x[]: A numeric vector containing only integers. The integers must either be all positive, all negative, or zero.
# Subsetting with positive integers keeps the elements at those positions.
x <- c("one", "two", "three", "four", "five")
x[c(3, 2, 5)]
x[c(1, 1, 5, 5, 5, 2)]
x[6]
# Negative values drop the elements at the specified positions.
x[c(-1, -3, -5)]
x[c(1, -2)]
x[0]
x[-6]

# x[]: Subsetting with a logical vector keeps all values corresponding to a TRUE value. This is most often useful in conjunction with the comparison functions.
x <- c(10, 3, NA, 5, 8, 1, NA)
# All non-missing values of x
x[!is.na(x)]
# All even (or missing!) values of x
x[x %% 2 == 0]

# x[]: If you have a named vector, you can subset it with a character vector.
x <- c(abc = 1, def = 2, xyz = 5)
x[c("xyz", "def")]
x[c("xyz", "xyz")]
x["efg"]

# if x is 2d, x[1, ] selects the first row and all the columns, and x[, -1] selects all rows and all columns except the first.

# There is an important variation of [ called [[. [[ only ever extracts a single element, and always drops names. It’s a good idea to use it whenever you want to make it clear that you’re extracting a single item, as in a for loop.

x <- c(-1, -2, 0, 1, 2)
x[-which(x > 0)]
x[x <= 0]
```

