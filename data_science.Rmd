---
title: "visualization"
author: "rj2543"
date: "2/29/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(maps)
library(mapproj)
library(nycflights13)
library(lvplot)
library(hexbin)
library(modelr)
library(hms)
library(microbenchmark)
library(stringi)
```

```{r data load}
data(mpg)
mpg
```

## Aesthetic

```{r aes}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
  
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy), color = "blue")
# mpg %>% 
#   ggplot() + 
#   geom_point(aes(x = displ, y = hwy, shape = cty))
# Error: A continuous variable can not be mapped to shape
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, size = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))
mpg %>% 
  ggplot() + 
  geom_point(aes(x = displ, y = hwy, colour = displ < 5))
# ?geom_point
# vignette("ggplot2-specs")
# The "munsell" package makes it easy to specific colours using a system designed by Alfred Munsell. If you invest a little in learning the system, it provides a convenient way of specifying aesthetically pleasing colours.
```

## Facets

One way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.

```{r facet}
# To facet your plot by a single variable, use facet_wrap(). The variable that you pass to facet_wrap() should be discrete.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 3)
# To facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(. ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ .)
# When using facet_grid() you should usually put the variable with more unique levels in the columns.
```

## Geometric objects

```{r geom}
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
# In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),
              show.legend = F)
# If you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
```

## Statistical transformation

```{r trans}
# You can generally use geoms and stats interchangeably. For example, you can recreate the plot using stat_count() instead of geom_bar().
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
# You might want to override the default stat. In the code below, I change the stat of geom_bar() from count (the default) to identity. This lets me map the height of the bars to the raw values of a y variable. Unfortunately when people talk about bar charts casually, they might be referring to this type of bar chart, where the height of the bar is already present in the data, or the previous bar chart where the height of the bar is generated by counting rows.
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)
demo %>% 
  mutate(cut = factor(cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
# You might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportion, rather than count.
# To find the variables computed by the stat, look for the help section titled “computed variables”.
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
# You might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarises the y values for each unique x value, to draw attention to the summary that you’re computing.
# ggplot2 provides over 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g. ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
?stat_summary
?geom_col # geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.
?stat_smooth
?geom_bar
vignette("ggplot2-specs")
```

## Position adjustment

```{r position}
# if you map the "fill" aesthetic to another variable, like "clarity": the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
# position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
# position = "dodge" places overlapping objects directly beside one another. This makes it easier to compare individual values.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
# position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
mpg %>% 
  ggplot() +
  geom_jitter(mapping = aes(x = displ, y = hwy))
?geom_count #counts the number of observations at each location, then maps the count to point area. It useful when you have discrete data and overplotting.
mpg %>% 
  ggplot() +
  geom_count(mapping = aes(x = displ, y = hwy))
mpg %>% 
  ggplot() +
  geom_boxplot(mapping = aes(y = displ, x = trans, fill = trans), show.legend = F, position = "dodge")
# the default position adjustment for geom_boxplot() is "dodge"
```

## Coordinate systems

```{r coord}
# coord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
# coord_quickmap() sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2.
nz <- map_data("nz")
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_map()
# coord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  #labs(x = NULL, y = NULL) + 
  coord_flip()
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)+ 
  coord_polar()
# coord_map projects a portion of the earth, which is approximately spherical, onto a flat 2D plane using any projection defined by the mapproj package. Map projections do not, in general, preserve straight lines, so this requires considerable computation. coord_quickmap is a quick approximation that does preserve straight lines. It works best for smaller areas closer to the equator.
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

## Layered grammar of graphics

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>

## Work flow

Don’t be lazy and use =: it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). 

```{r}
# This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen.
(y <- seq(1, 10, length.out = 5))
library(tidyverse)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
# Press Alt + Shift + K. Keyboard Shortcut Quick Reference
```

## Data Transformation

```{r filter}
(dec25 <- flights %>% 
   filter(month == 12, day == 25))
# floating point numbers: Computers use finite precision arithmetic, so remember that every number you see is an approximation.
sqrt(2)^2 == 2
near(sqrt(2) ^ 2,  2)
1/49*49 == 1
near(1 / 49 * 49, 1)
# almost any operation involving an unknown value will also be unknown
# 1. Had an arrival delay of two or more hours
flights %>% 
  filter(arr_delay >= 120)
# 2. Flew to Houston (IAH or HOU)
flights %>% 
  filter(dest %in% c("IAH", "HOU"))
# 3. Were operated by United, American, or Delta
flights %>% 
  filter(carrier %in% c("UA", "AA", "DL"))
# 4. Departed in summer (July, August, and September)
flights %>% 
  filter(month %in% c(7, 8, 9))
flights %>% 
  filter(between(month, 7, 9))
# 5. Arrived more than two hours late, but didn’t leave late
flights %>% 
  filter(arr_delay > 120 & dep_delay <= 0)
# 6. Were delayed by at least an hour, but made up over 30 minutes in flight
flights %>% 
  filter(dep_delay >= 60 & (dep_delay - arr_delay) >= 30)
# 7. Departed between midnight and 6am (inclusive)
flights %>% 
  filter(dep_time == 2400 | dep_time <= 0600)
flights %>% 
  filter(between(dep_time, 0, 600) | dep_time == 2400)
?between # This is a shortcut for x >= left & x <= right, between(x, left, right)
flights %>% 
  filter(is.na(dep_time)) # canceled flights
NA ^ 0 # = 1
NA | TRUE # TRUE
FALSE & NA # FALSE
NA * 0 # NA
```

```{r arrange}
# Missing values are always sorted at the end.
```

```{r select}
flights %>% 
  select(-(year:day))
# starts_with("abc"): matches names that begin with “abc”.
# ends_with("xyz"): matches names that end with “xyz”.
# contains("ijk"): matches names that contain “ijk”.
# matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. 
# num_range("x", 1:3): matches x1, x2 and x3.
# one_of(): Matches variable names in a character vector.
# everything(): if you have a handful of variables you’d like to move to the start of the data frame.
# last_col(): Select last variable, possibly with an offset.
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
flights %>% 
  select(one_of(vars))
flights %>% 
  select(contains("TIME")) # default in the contains() helper is: ignore.case = TRUE
flights %>% 
  select(contains("TIME", ignore.case = FALSE))
```

```{r mutate}
# you can refer to columns that you’ve just created
# If you only want to keep the new variables, use transmute():
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
# Modular arithmetic: %/% (integer division) and %% (remainder), where x == y * (x %/% y) + (x %% y).
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
# Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)). They are most useful in conjunction with group_by().
(x <- 1:10)
lag(x) # previous
lead(x) # next
# Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); and dplyr provides cummean() for cumulative means. If you need rolling aggregates (i.e. a sum computed over a rolling window), try the RcppRoll package.
# what is "a rolling window"?
cumsum(x)
cummean(x)
# Ranking: min_rank() does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks.
y <- c(1, 2, 2, NA, 3, 4)
min_rank(y)
min_rank(desc(y))
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
# ntile(y)
flights %>% 
  transmute(air_time, arr_time - dep_time)
flights %>% 
  transmute(air_time, arr_time - dep_time, (arr_time %/% 100 * 60 + arr_time %% 100) - (dep_time %/% 100 * 60 + dep_time %% 100))
```

```{r summarise}
# grouped summaries: It collapses a data frame to a single row
# Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data.
# median absolute deviation: mad(x)
# Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)]
# To count the number of distinct (unique) values, use n_distinct(x).

# When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
((per_year   <- summarise(per_month, flights = sum(flights))))

flights %>% group_by(carrier, dest) %>% summarise(n())

count(sort = TRUE)

vignette("window-functions")
# A window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The output of a window function depends on all its input values, so window functions don’t include functions that work element-wise, like + or round(). Window functions include variations on aggregate functions, like cumsum() and cummean(), functions for ranking and ordering, like rank(), and functions for taking offsets, like lead() and lag().
# lead() and lag() have an optional argument order_by. If set, instead of using the row order to determine which value comes before another, they will use another variable. This is important if you have not already sorted the data, or you want to sort one way and lag another.
```

# Exploratory Data Analysis

## Variation

```{r variation}
diamonds %>% 
  count(cut_width(carat, 0.5))

smaller <- diamonds %>% 
  filter(carat < 3)
smaller %>% 
  ggplot(aes(x = carat, color = cut)) +
  geom_freqpoly(binwidth = 0.1)

smaller %>% 
  ggplot(aes(x = carat)) + 
  geom_histogram(binwidth = 0.01)

# outlier: To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian()
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) + # count:0 - 12000
  coord_cartesian(ylim = c(0, 50))
# It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.
```

## Missing values

```{r missing value}
# Instead of dropping the entire observation/row with the strange values, replace the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA:
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))

flights %>% 
  transmute(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
    geom_histogram(mapping = aes(fill = cancelled), binwidth = 1/4)

flights %>% 
  transmute(cancelled = is.na(dep_time),
            sched_dep_time = (sched_dep_time %/% 100) + (sched_dep_time %% 100)/60) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 1/4)
```

## Covariation

If variation describes the behavior within a variable, covariation describes the behavior between variables. 
Covariation is the tendency for the values of two or more variables to vary together in a related way. 
The best way to spot covariation is to visualise the relationship between two or more variables.

```{r categorical vs continuous}
# Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.
diamonds %>% 
  ggplot(mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(color = cut), binwidth = 500)

# IQR. In the middle of the box is a line that displays the median. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.
# Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.
# A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip() # if variable names are long

# For large datasets (10,000 - 100,000), the letter-value box plot addresses both these shortcomings: it conveys more detailed information in the tails using letter values, only out to the depths where the letter values are reliable estimates of their corresponding quantiles (corresponding to tail areas of roughly 2^{-i}); “outliers” are defined as a function of the most extreme letter value shown.
diamonds %>% 
  ggplot(mapping = aes(x = cut, y = price)) + 
  geom_lv(mapping = aes(fill = cut))

ggplot(data = mpg) +
  geom_violin(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class))

ggplot(data = mpg) +
  geom_jitter(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, color = class), alpha = 0.4)
```

```{r categorical vs categorical}
# To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination.
# Covariation will appear as a strong correlation between specific x values and specific y values.
diamonds %>% 
  ggplot() +
  geom_count(mapping = aes(x = cut, y = color))

diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = n))

flights %>% 
  mutate(month = factor(month)) %>%
  group_by(dest, month) %>% 
  summarise(mean_delay = mean(arr_delay)) %>% 
  ggplot(mapping = aes(x = dest, y = month)) +
  geom_tile(mapping = aes(fill = mean_delay), na.rm = T)
```

```{r continuous vs continuous}
# geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().
ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))

# Another option is to bin one continuous variable so it acts like a categorical variable.
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), varwidth = T) # divide carat into bins of width = 0.1; make the width of the boxplot proportional to the number of points

ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20))) # display approximately the same number of points in each bin

# Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # zoom
```

## Patterns and models

```{r patterns and models}
#  The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

# Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

# Tibbles

```{r tibble}
vignette("tibble")

as_tibble()

# tibble() will automatically recycle inputs of length 1, and allows you to refer to variables that you just created.
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)

# transposed tibble: tribble() is customised for data entry in code: column headings are defined by formulas (i.e. they start with ~), and entries are separated by commas. This makes it possible to lay out small amounts of data in easy-to-read form.
tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 2, 3.6,
  "b", 1, 8.5
)

# First, you can explicitly print() the data frame and control the number of rows (n) and the width of the display. width = Inf will display all columns:
flights %>% 
  print(n = 10, width = Inf)

?tibble::enframe() # convert named atomic vectors or lists to one- or two-column data frames
```

# Data import

```{r intro}
# read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.

# Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = "#" to drop all lines that start with (e.g.) #.
read_csv(
  "# The first line of metadata
  # The second line of metadata
  x,y,z
  1,2,3", 
  comment = "#")

# na: this specifies the value (or values) that are used to represent missing values in your file
read_csv("a, b, c \n 1, 2, .", na = ".")
```

```{r parse}
# parse_*() functions take a character vector and return a more specialised vector like a logical, integer, or date
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))

parse_double("1,23", locale = locale(decimal_mark = ","))

# parse_number() ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text.
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45.")
parse_number("$123,456,789") # ignore the grouping mark

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)

# Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors.
challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
# This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
type_convert(df)

# If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats.

# write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS.

# The feather package implements a fast binary file format that can be shared across programming languages. " .feather"

# library(haven) reads SPSS, Stata, and SAS files.
# library(readxl) reads excel files (both .xls and .xlsx).
# library(DBI), along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.
# For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. 
```

# Tidy data

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

## Pivoting

```{r pivot}
# pivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. pivot_longer() makes wide tables narrower and longer
table4a = table4a %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "cases")
table4b = table4b %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "population")
left_join(table4a, table4b)

# pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows. pivot_wider() makes long tables shorter and wider
stocks <- tibble(
  year   = c(2015, 2015, 2016, 2016),
  half  = c(   1,    2,     1,    2),
  return = c(1.88, 0.59, 0.92, 0.17)
)
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer("2015":"2016", names_to = "year", values_to = "return", names_ptype = list(year = double()))
```

## Separating and uniting

```{r separate}
# it leaves the type of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better types using convert = TRUE.
table3 %>% 
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)

# You can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into.
table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
table3 %>% 
  separate(year, into = c("century", "year"), sep = -2)
```

```{r unite}
# In this case we also need to use the sep argument. The default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ""
table5 %>% 
  unite(new, century, year)
table5 %>% 
  unite(new, century, year, sep = "")
```

## Missing values

```{r missing}
(stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
))

# The way that a dataset is represented can make implicit values explicit. 
stocks %>% 
  pivot_wider(names_from = year, values_from = return)

# set values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer(
    cols = c("2015", "2016"), 
    names_to = "year", 
    values_to = "return", 
    values_drop_na = TRUE
  )

# complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary.
stocks %>% 
  complete(year, qtr)

# There’s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward.
(treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
))
# You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward, LOCF).
treatment %>% 
  fill(person)
```

## Case study

```{r case}
(who_tidy = who %>% 
  pivot_longer(
    cols = new_sp_m014:newrel_f65, 
    names_to = "key", 
    values_to = "cases", 
    values_drop_na = TRUE
  ) %>% 
  mutate(key = str_replace(key, "newrel", "new_rel")) %>% 
  separate(key, c("new", "type", "sexage"), sep = "_") %>% 
  select(-iso2, -iso3, -new) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>% 
  mutate(age_group = case_when(
    age == "014" ~ "0-14",
    age == "1524" ~ "15-24",
    age == "2534" ~ "25-34", 
    age == "3544" ~ "35-44",
    age == "4554" ~ "45-54",
    age == "5564" ~ "55-64",
    age == "65" ~ "65+",
    TRUE ~ NA_character_
  )) %>% 
  select(-age))

who_tidy %>% 
  group_by(country, year, sex) %>% 
  summarise(total_cases = sum(cases)) %>% 
  mutate(sex = ifelse(sex == "f", "female", "male")) %>% 
  ggplot(mapping = aes(x = year, y = total_cases, fill = sex)) +
  geom_col() +
  coord_cartesian(xlim = c(1995, 2013))
```

# Relational data

## Keys

* A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.

* A foreign key uniquely identifies an observation in another table. For example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.

If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key.

A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.

```{r key}
flights %>% 
  mutate(surrogate = row_number()) %>% 
  select(surrogate, everything())
```

## Mutating joins

```{r mutating join}
(flights2 <- flights %>% 
  select(year:day, hour, origin, dest, tailnum, carrier))

flights2 %>%
  select(-origin, -dest) %>% 
  left_join(airlines, by = "carrier")

# full_join()
```

```{r duplicate keys}
# One table has duplicate keys: This is useful when you want to add in additional information as there is typically a one-to-many relationship. Here, "key" is a primary key in y and a foreign key in x.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     1, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2"
))
left_join(x, y, by = "key")

# Both tables have duplicate keys: This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     3, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     2, "y3",
     3, "y4"
))
left_join(x, y, by = "key")
```

```{r define key columns}
# The default, by = NULL, uses all variables that appear in both tables, the so called natural join. e.g., the flights and weather tables match on their common variables: year, month, day, hour and origin.
flights2 %>% 
  left_join(weather) # Joining, by = c("year", "month", "day", "hour", "origin")

# A character vector, by = "x". This is like a natural join, but uses only some of the common variables. e.g., flights and planes have year variables, but they mean different things so we only want to join by tailnum.
flights2 %>% 
  left_join(planes, by = "tailnum") # year.x, year.y

# A named character vector: by = c("a" = "b"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. e.g., if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to.
flights2 %>% 
  left_join(airports, by = c("dest" = "faa"))
flights2 %>% 
  left_join(airports, by = c("origin" = "faa"))
```

```{r exercise}
(flights_avgdelay = flights %>% 
  group_by(dest) %>% 
  summarise(avgdelay = mean(arr_delay, na.rm = T)))

airports %>%
  inner_join(flights_avgdelay, by = c("faa" = "dest")) %>%
  ggplot(mapping = aes(x = lon, y = lat)) +
    borders("state") +
    geom_point(mapping = aes(size = avgdelay, color = avgdelay, alpha = 0.5)) +
    coord_quickmap()

# Joining different variables between the tables, e.g. inner_join(x, y, by = c("a" = "b")) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins) ?
```

## Filtering joins

```{r filtering join}
# Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables.

# semi_join(x, y) keeps all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows.
(top_desc <- flights %>% 
   count(dest, sort = TRUE) %>% 
   head(10))
semi_join(flights, top_desc)

# anti_join(x, y) drops all observations in x that have a match in y. Anti-joins are useful for diagnosing join mismatches. 
```

## Set operations

```{r set}
# All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets.
# intersect(x, y): return only observations in both x and y.
# union(x, y): return unique observations in x and y.
# setdiff(x, y): return observations in x, but not in y.

(df1 <- tribble(
  ~x, ~y,
   1,  1,
   2,  1
))
(df2 <- tribble(
  ~x, ~y,
   1,  1,
   1,  2
))
intersect(df1, df2)
union(df1, df2)
setdiff(df1, df2)
setdiff(df2, df1)
```

# Strings

## String basics

```{r basic}
# To include a literal single or double quote in a string you can use \ to “escape” it.
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
# if you want to include a literal backslash, you’ll need to double it up: "\\"

# the printed representation shows the escapes. To see the raw contents of the string, use writeLines()
(x <- c("\"", "\\"))
writeLines(x)

# "\n": newline; "\t": tab. complete list: ?'"', or ?"'". like "\u00b5", this is a way of writing non-English characters that works on all platforms.
(x <- "\u00b5")
writeLines(x)
```

### String length

```{r length}
# str_length() tells you the number of characters in a string.
str_length(c("a", "R for data science", NA)) # include space

# The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions.
```

### Combining strings

```{r combine}
str_c("x", "y")
str_c("x", "y", "z")
str_c("x", "y", sep = ", ")

# missing values are contagious. If you want them to print as "NA", use str_replace_na().
x <- c("abc", NA)
str_c("|-", x, "-|")
str_c("|-", str_replace_na(x), "-|")

# As shown above, str_c() is vectorised, and it automatically recycles shorter vectors to the same length as the longest.
str_c("prefix-", c("a", "b", "c"), "-suffix")

# Objects of length 0 are silently dropped. This is particularly useful in conjunction with if.
name <- "Hadley"
time_of_day <- "morning"
birthday <- FALSE
str_c(
  "Good ", time_of_day, " ", name,
  if (birthday) " and HAPPY BIRTHDAY",
  "."
)

# To collapse a vector of strings into a single string, use collapse.
str_c(c("x", "y", "z"), collapse = ", ")
```

### Subsetting strings

```{r subset}
# str_sub() takes start and end arguments which give the (inclusive) position of the substring.
(x <- c("Apple", "Banana", "Pear"))
str_sub(x, 1, 3) # vectorized
str_sub(x, -3, -1) # negative numbers count backwards from end

# str_sub() won’t fail if the string is too short: it will just return as much as possible.
str_sub("ab", 1, 5)

str_sub(x, 1, 1) <- str_to_lower(str_sub(x, 1, 1))
```

### Locales

```{r locale}
dog <- "The quick brown dog in the rain"
str_to_upper(dog)
str_to_lower(dog)
str_to_title(dog)
str_to_sentence("the quick brown dog in the rain")

# different languages have different rules for changing case. You can pick which set of rules to use by specifying a locale.
# Turkish has two i's: with and without a dot, and it has a different rule for capitalising them.
str_to_upper(c("i", "ı"))
str_to_upper(c("i", "ı"), locale = "tr")

# The base R order() and sort() functions sort strings using the current locale. If you want robust behaviour across different computers, you may want to use str_sort() and str_order() which take an additional locale argument.
x <- c("apple", "eggplant", "banana")
str_sort(x, locale = "en")  # English
str_sort(x, locale = "haw") # Hawaiian

?str_wrap
?str_trim
```

## Matching patterns with regular expressions

### Basic matches

```{r basic match}
# the simplest patterns match exact strings
x <- c("apple", "banana", "pear")
str_view(x, "an")

# .: which matches any character (except a newline)
str_view(x, ".a.")

# So to match an ".", you need the regexp "\.". Unfortunately this creates a problem. We use strings to represent regular expressions, and "\" is also used as an escape symbol in strings. So to create the regular expression "\." we need the string "\\."
# To create the regular expression, we need \\
(dot <- "\\.")
# But the expression itself only contains one:
writeLines(dot)
# And this tells R to look for an explicit.
str_view(c("abc", "a.c", "bef"), "a\\.c")

# to match a literal \ you need to write "\\\\"
(x <- "a\\b")
writeLines(x)
str_view(x, "\\\\")
```

### Anchors

```{r anchor}
# ^ to match the start of the string.
# $ to match the end of the string.
x <- c("apple", "banana", "pear")
str_view(x, "^a")
str_view(x, "a$")
# if you begin with power (^), you end up with money ($).

# To force a regular expression to only match a complete string, anchor it with both ^ and $.
x <- c("apple pie", "apple", "apple cake")
str_view(x, "apple")
str_view(x, "^apple$")

# You can also match the boundary between words with \b. when I want to find the name of a function that’s a component of other functions
x <- c("sum", "summarise", "summary", "rowsum")
str_view(x, "\bsum\b") # ???

str_view(x, "^sum$", match = TRUE)
```

### Character classes and alternatives

```{r character class}
# \d: matches any digit.
# \s: matches any whitespace (e.g. space, tab, newline).
# [abc]: matches a, b, or c.
# [^abc]: matches anything except a, b, or c.

# A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex.
str_view(c("abc", "a.c", "a*c", "a c"), "a[.]c")
str_view(c("abc", "a.c", "a*c", "a c"), ".[*]c")
str_view(c("abc", "a.c", "a*c", "a c"), "a[ ]")

# You can use alternation to pick between one or more alternative patterns. e.g., abc|d..f will match either ‘“abc”’, or "deaf". Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz.
str_view(c("grey", "gray"), "gr(e|a)y")
str_view(c("grey", "gray"), "gray|grey")
```

### Repetition

```{r repeat}
# control how many times a pattern matches:
# ?: 0 or 1
# +: 1 or more
# *: 0 or more
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?")
str_view(x, "CC+")
str_view(x, "C[LX]+")

# Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+.

# You can also specify the number of matches precisely:
# {n}: exactly n
# {n,}: n or more
# {,m}: at most m
# {n,m}: between n and m
str_view(x, "C{2}")
str_view(x, "C{2,}")
str_view(x, "C{2,3}")

# By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them.
str_view(x, "C{2,3}?")
str_view(x, "C[LX]+?")
```

### Grouping and backreferences

```{r group}
# Parentheses create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \1, \2 etc. e.g., the following regular expression finds all fruits that have a repeated pair of letters.
str_view(fruit, "(..)\\1", match = TRUE)
str_view(fruit, "(.)(.)\\2\\1", match = T)
str_view(fruit, "(.).\\1.\\1", match = T)
```

## Tools

### Detect matches

```{r detect}
x <- c("apple", "banana", "pear")
str_detect(x, "e")

# How many common words start with t?
sum(str_detect(words, "^t"))
# What proportion of common words end with a vowel?
mean(str_detect(words, "[aeiou]$"))

words[str_detect(words, "x$")]
str_subset(words, "x$")

(df <- tibble(
  word = words, 
  i = seq_along(word) # i = row_number(word)
))
df %>% 
  filter(str_detect(word, "x$"))

# rather than a simple yes or no, it tells you how many matches there are in a string
x <- c("apple", "banana", "pear")
str_count(x, "a")
# On average, how many vowels per word?
mean(str_count(words, "[aeiou]"))

df %>% 
  mutate(
    vowels = str_count(word, "[aeiou]"),
    consonants = str_count(word, "[^aeiou]")
  )

# many stringr functions come in pairs: one function works with a single match, and the other works with all matches. The second function will have the suffix _all.
str_view_all("abababa", "aba")
```

### Extract matches

```{r extract}
colours <- c("red", "orange", "yellow", "green", "blue", "purple")
(colour_match <- str_c(colours, collapse = "|"))
has_colour <- str_subset(sentences, colour_match)
(matches <- str_extract(has_colour, colour_match)) # str_extract() only extracts the first match.

more <- sentences[str_count(sentences, colour_match) > 1]
str_view_all(more, colour_match)
# str_extract(more, colour_match) # str_extract() only extracts the first match
str_extract_all(more, colour_match) # return a list
# If you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest.
str_extract_all(more, colour_match, simplify = TRUE)
x <- c("a", "a b", "a b c")
str_extract_all(x, "[a-z]", simplify = TRUE)
```

### Grouped matches

```{r group}
# You can also use parentheses to extract parts of a complex match. e.g., we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space.
noun <- "(a|the) ([^ ]+)"
has_noun <- sentences %>%
  str_subset(noun) %>%
  head(10)
has_noun %>% 
  str_extract(noun) # str_extract() gives us the complete match

# str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group。
has_noun %>% 
  str_match(noun)
# Like str_extract(), if you want all matches for each string, you’ll need str_match_all().

# If your data is in a tibble, it’s often easier to use tidyr::extract(). It works like str_match() but requires you to name the matches, which are then placed in new column.
tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("article", "noun"), "(a|the) ([^ ]+)", 
    remove = FALSE
  )
```

### Replacing matches

```{r replace}
x <- c("apple", "pear", "banana")
str_replace(x, "[aeiou]", "-")
str_replace_all(x, "[aeiou]", "-")

# With str_replace_all() you can perform multiple replacements by supplying a named vector.
x <- c("1 house", "2 cars", "3 people")
str_replace_all(x, c("1" = "one", "2" = "two", "3" = "three"))

# you can use backreferences to insert components of the match. In the following code, flip the order of the second and third words.
sentences %>% 
  str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") %>% 
  head(5)
```

### Splitting

```{r split}
sentences %>%
  head(5) %>% 
  str_split(" ") # each component might contain a different number of pieces, return a list

"a|b|c|d" %>% 
  str_split("\\|") %>% 
  .[[1]] # working with a length-1 vector, just extract the first element of the list

sentences %>%
  head(5) %>% 
  str_split(" ", simplify = TRUE) # return a matrix

# request a maximum number of pieces
fields <- c("Name: Hadley", "Country: NZ", "Age: 35")
fields %>% 
  str_split(": ", n = 2, simplify = TRUE)

# Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary().
x <- "This is a sentence.  This is another sentence."
str_view_all(x, boundary()) # each character
str_view_all(x, boundary("word")) # each word
# str_view_all(x, boundary("line_break"))
str_view_all(x, boundary("sentence")) # each sentence
str_split(x, " ")[[1]] # including extra " " between
str_split(x, boundary("word"))[[1]] # no extra " ", no punctuation
```

### Find matches

str_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them.

## Other types of pattern

```{r other pattern}
# The regular call: str_view(fruit, "nana") is shorthand for str_view(fruit, regex("nana")).

# ignore_case = TRUE allows characters to match either their uppercase or lowercase forms.
bananas <- c("banana", "Banana", "BANANA")
str_view(bananas, regex("banana", ignore_case = TRUE))

# multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string.
x <- "Line 1\nLine 2\nLine 3"
writeLines(x)
str_extract_all(x, "^Line")[[1]]
str_extract_all(x, regex("^Line", multiline = TRUE))[[1]]

# comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: "\\ ".
phone <- regex("
  \\(?     # optional opening parens
  (\\d{3}) # area code
  [) -]?   # optional closing parens, space, or dash
  (\\d{3}) # another three numbers
  [ -]?    # optional space or dash
  (\\d{4}) # four more numbers
  ", comments = TRUE)
str_match("514-791-8141", phone)
str_match("(347)937-0461", phone)

# dotall = TRUE allows . to match everything, including \n.

# fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example.
microbenchmark(
  fixed = str_detect(sentences, fixed("the")),
  regex = str_detect(sentences, "the"),
  times = 20
)

# coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. Unfortunately different parts of the world use different rules!

x <- "This is a sentence."
str_view_all(x, boundary("word"))
str_extract_all(x, boundary("word"))
```

## Other uses of regular expressions

```{r regular expression}
# apropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function.
apropos("replace")

# dir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with:
dir(pattern = "\\.Rmd$")

# (If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx()). ?
```

## Stringi

Stringi package contains almost every function you might ever need: stringi has 244 functions to stringr’s 49.

If you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_.

