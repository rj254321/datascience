---
title: "visualization"
author: "rj2543"
date: "2/29/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(maps)
library(mapproj)
library(nycflights13)
library(lvplot)
library(hexbin)
library(modelr)
library(hms)
library(microbenchmark)
library(stringi)
library(tidytidbits)
library(lubridate)
library(magrittr)
library(splines)
library(gapminder)
library(ggrepel)
```

```{r data load}
data(mpg)
mpg
```

## Aesthetic

```{r aes}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
  
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy), color = "blue")
# mpg %>% 
#   ggplot() + 
#   geom_point(aes(x = displ, y = hwy, shape = cty))
# Error: A continuous variable can not be mapped to shape
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, size = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))
mpg %>% 
  ggplot() + 
  geom_point(aes(x = displ, y = hwy, colour = displ < 5))
# ?geom_point
# vignette("ggplot2-specs")
# The "munsell" package makes it easy to specific colours using a system designed by Alfred Munsell. If you invest a little in learning the system, it provides a convenient way of specifying aesthetically pleasing colours.
```

## Facets

One way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.

```{r facet}
# To facet your plot by a single variable, use facet_wrap(). The variable that you pass to facet_wrap() should be discrete.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 3)
# To facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(. ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ .)
# When using facet_grid() you should usually put the variable with more unique levels in the columns.
```

## Geometric objects

```{r geom}
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
# In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),
              show.legend = F)
# If you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
```

## Statistical transformation

```{r trans}
# You can generally use geoms and stats interchangeably. For example, you can recreate the plot using stat_count() instead of geom_bar().
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
# You might want to override the default stat. In the code below, I change the stat of geom_bar() from count (the default) to identity. This lets me map the height of the bars to the raw values of a y variable. Unfortunately when people talk about bar charts casually, they might be referring to this type of bar chart, where the height of the bar is already present in the data, or the previous bar chart where the height of the bar is generated by counting rows.
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)
demo %>% 
  mutate(cut = factor(cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
# You might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportion, rather than count.
# To find the variables computed by the stat, look for the help section titled “computed variables”.
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
# You might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarises the y values for each unique x value, to draw attention to the summary that you’re computing.
# ggplot2 provides over 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g. ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
?stat_summary
?geom_col # geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.
?stat_smooth
?geom_bar
vignette("ggplot2-specs")
```

## Position adjustment

```{r position}
# if you map the "fill" aesthetic to another variable, like "clarity": the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
# position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
# position = "dodge" places overlapping objects directly beside one another. This makes it easier to compare individual values.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
# position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
mpg %>% 
  ggplot() +
  geom_jitter(mapping = aes(x = displ, y = hwy))
?geom_count #counts the number of observations at each location, then maps the count to point area. It useful when you have discrete data and overplotting.
mpg %>% 
  ggplot() +
  geom_count(mapping = aes(x = displ, y = hwy))
mpg %>% 
  ggplot() +
  geom_boxplot(mapping = aes(y = displ, x = trans, fill = trans), show.legend = F, position = "dodge")
# the default position adjustment for geom_boxplot() is "dodge"
```

## Coordinate systems

```{r coord}
# coord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
# coord_quickmap() sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2.
nz <- map_data("nz")
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_map()
# coord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  #labs(x = NULL, y = NULL) + 
  coord_flip()
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)+ 
  coord_polar()
# coord_map projects a portion of the earth, which is approximately spherical, onto a flat 2D plane using any projection defined by the mapproj package. Map projections do not, in general, preserve straight lines, so this requires considerable computation. coord_quickmap is a quick approximation that does preserve straight lines. It works best for smaller areas closer to the equator.
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

## Layered grammar of graphics

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>

## Work flow

Don’t be lazy and use =: it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). 

```{r}
# This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen.
(y <- seq(1, 10, length.out = 5))
library(tidyverse)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
# Press Alt + Shift + K. Keyboard Shortcut Quick Reference
```

## Data Transformation

```{r filter}
(dec25 <- flights %>% 
   filter(month == 12, day == 25))
# floating point numbers: Computers use finite precision arithmetic, so remember that every number you see is an approximation.
sqrt(2)^2 == 2
near(sqrt(2) ^ 2,  2)
1/49*49 == 1
near(1 / 49 * 49, 1)
# almost any operation involving an unknown value will also be unknown
# 1. Had an arrival delay of two or more hours
flights %>% 
  filter(arr_delay >= 120)
# 2. Flew to Houston (IAH or HOU)
flights %>% 
  filter(dest %in% c("IAH", "HOU"))
# 3. Were operated by United, American, or Delta
flights %>% 
  filter(carrier %in% c("UA", "AA", "DL"))
# 4. Departed in summer (July, August, and September)
flights %>% 
  filter(month %in% c(7, 8, 9))
flights %>% 
  filter(between(month, 7, 9))
# 5. Arrived more than two hours late, but didn’t leave late
flights %>% 
  filter(arr_delay > 120 & dep_delay <= 0)
# 6. Were delayed by at least an hour, but made up over 30 minutes in flight
flights %>% 
  filter(dep_delay >= 60 & (dep_delay - arr_delay) >= 30)
# 7. Departed between midnight and 6am (inclusive)
flights %>% 
  filter(dep_time == 2400 | dep_time <= 0600)
flights %>% 
  filter(between(dep_time, 0, 600) | dep_time == 2400)
?between # This is a shortcut for x >= left & x <= right, between(x, left, right)
flights %>% 
  filter(is.na(dep_time)) # canceled flights
NA ^ 0 # = 1
NA | TRUE # TRUE
FALSE & NA # FALSE
NA * 0 # NA
```

```{r arrange}
# Missing values are always sorted at the end.
```

```{r select}
flights %>% 
  select(-(year:day))
# starts_with("abc"): matches names that begin with “abc”.
# ends_with("xyz"): matches names that end with “xyz”.
# contains("ijk"): matches names that contain “ijk”.
# matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. 
# num_range("x", 1:3): matches x1, x2 and x3.
# one_of(): Matches variable names in a character vector.
# everything(): if you have a handful of variables you’d like to move to the start of the data frame.
# last_col(): Select last variable, possibly with an offset.
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
flights %>% 
  select(one_of(vars))
flights %>% 
  select(contains("TIME")) # default in the contains() helper is: ignore.case = TRUE
flights %>% 
  select(contains("TIME", ignore.case = FALSE))
```

```{r mutate}
# you can refer to columns that you’ve just created
# If you only want to keep the new variables, use transmute():
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
# Modular arithmetic: %/% (integer division) and %% (remainder), where x == y * (x %/% y) + (x %% y).
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
# Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)). They are most useful in conjunction with group_by().
(x <- 1:10)
lag(x) # previous
lead(x) # next
# Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); and dplyr provides cummean() for cumulative means. If you need rolling aggregates (i.e. a sum computed over a rolling window), try the RcppRoll package.
# what is "a rolling window"?
cumsum(x)
cummean(x)
# Ranking: min_rank() does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks.
y <- c(1, 2, 2, NA, 3, 4)
min_rank(y)
min_rank(desc(y))
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
# ntile(y)
flights %>% 
  transmute(air_time, arr_time - dep_time)
flights %>% 
  transmute(air_time, arr_time - dep_time, (arr_time %/% 100 * 60 + arr_time %% 100) - (dep_time %/% 100 * 60 + dep_time %% 100))
```

```{r summarise}
# grouped summaries: It collapses a data frame to a single row
# Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data.
# median absolute deviation: mad(x)
# Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)]
# To count the number of distinct (unique) values, use n_distinct(x).

# When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
((per_year   <- summarise(per_month, flights = sum(flights))))

flights %>% group_by(carrier, dest) %>% summarise(n())

count(sort = TRUE)

vignette("window-functions")
# A window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The output of a window function depends on all its input values, so window functions don’t include functions that work element-wise, like + or round(). Window functions include variations on aggregate functions, like cumsum() and cummean(), functions for ranking and ordering, like rank(), and functions for taking offsets, like lead() and lag().
# lead() and lag() have an optional argument order_by. If set, instead of using the row order to determine which value comes before another, they will use another variable. This is important if you have not already sorted the data, or you want to sort one way and lag another.
```

# Exploratory Data Analysis

## Variation

```{r variation}
diamonds %>% 
  count(cut_width(carat, 0.5))

smaller <- diamonds %>% 
  filter(carat < 3)
smaller %>% 
  ggplot(aes(x = carat, color = cut)) +
  geom_freqpoly(binwidth = 0.1)

smaller %>% 
  ggplot(aes(x = carat)) + 
  geom_histogram(binwidth = 0.01)

# outlier: To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian()
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) + # count:0 - 12000
  coord_cartesian(ylim = c(0, 50))
# It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.
```

## Missing values

```{r missing value}
# Instead of dropping the entire observation/row with the strange values, replace the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA:
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))

flights %>% 
  transmute(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
    geom_histogram(mapping = aes(fill = cancelled), binwidth = 1/4)

flights %>% 
  transmute(cancelled = is.na(dep_time),
            sched_dep_time = (sched_dep_time %/% 100) + (sched_dep_time %% 100)/60) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 1/4)
```

## Covariation

If variation describes the behavior within a variable, covariation describes the behavior between variables. 
Covariation is the tendency for the values of two or more variables to vary together in a related way. 
The best way to spot covariation is to visualise the relationship between two or more variables.

```{r categorical vs continuous}
# Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.
diamonds %>% 
  ggplot(mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(color = cut), binwidth = 500)

# IQR. In the middle of the box is a line that displays the median. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.
# Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.
# A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip() # if variable names are long

# For large datasets (10,000 - 100,000), the letter-value box plot addresses both these shortcomings: it conveys more detailed information in the tails using letter values, only out to the depths where the letter values are reliable estimates of their corresponding quantiles (corresponding to tail areas of roughly 2^{-i}); “outliers” are defined as a function of the most extreme letter value shown.
diamonds %>% 
  ggplot(mapping = aes(x = cut, y = price)) + 
  geom_lv(mapping = aes(fill = cut))

ggplot(data = mpg) +
  geom_violin(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class))

ggplot(data = mpg) +
  geom_jitter(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, color = class), alpha = 0.4)
```

```{r categorical vs categorical}
# To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination.
# Covariation will appear as a strong correlation between specific x values and specific y values.
diamonds %>% 
  ggplot() +
  geom_count(mapping = aes(x = cut, y = color))

diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = n))

flights %>% 
  mutate(month = factor(month)) %>%
  group_by(dest, month) %>% 
  summarise(mean_delay = mean(arr_delay)) %>% 
  ggplot(mapping = aes(x = dest, y = month)) +
  geom_tile(mapping = aes(fill = mean_delay), na.rm = T)
```

```{r continuous vs continuous}
# geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().
ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))

# Another option is to bin one continuous variable so it acts like a categorical variable.
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), varwidth = T) # divide carat into bins of width = 0.1; make the width of the boxplot proportional to the number of points

ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20))) # display approximately the same number of points in each bin

# Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # zoom
```

## Patterns and models

```{r patterns and models}
#  The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

# Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

# Tibbles

```{r tibble}
vignette("tibble")

as_tibble()

# tibble() will automatically recycle inputs of length 1, and allows you to refer to variables that you just created.
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)

# transposed tibble: tribble() is customised for data entry in code: column headings are defined by formulas (i.e. they start with ~), and entries are separated by commas. This makes it possible to lay out small amounts of data in easy-to-read form.
tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 2, 3.6,
  "b", 1, 8.5
)

# First, you can explicitly print() the data frame and control the number of rows (n) and the width of the display. width = Inf will display all columns:
flights %>% 
  print(n = 10, width = Inf)

?tibble::enframe() # convert named atomic vectors or lists to one- or two-column data frames
```

# Data import

```{r intro}
# read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.

# Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = "#" to drop all lines that start with (e.g.) #.
read_csv(
  "# The first line of metadata
  # The second line of metadata
  x,y,z
  1,2,3", 
  comment = "#")

# na: this specifies the value (or values) that are used to represent missing values in your file
read_csv("a, b, c \n 1, 2, .", na = ".")
```

```{r parse}
# parse_*() functions take a character vector and return a more specialised vector like a logical, integer, or date
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))

parse_double("1,23", locale = locale(decimal_mark = ","))

# parse_number() ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text.
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45.")
parse_number("$123,456,789") # ignore the grouping mark

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)

# Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors.
challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
# This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
type_convert(df)

# If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats.

# write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS.

# The feather package implements a fast binary file format that can be shared across programming languages. " .feather"

# library(haven) reads SPSS, Stata, and SAS files.
# library(readxl) reads excel files (both .xls and .xlsx).
# library(DBI), along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.
# For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. 
```

# Tidy data

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

## Pivoting

```{r pivot}
# pivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. pivot_longer() makes wide tables narrower and longer
table4a = table4a %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "cases")
table4b = table4b %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "population")
left_join(table4a, table4b)

# pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows. pivot_wider() makes long tables shorter and wider
stocks <- tibble(
  year   = c(2015, 2015, 2016, 2016),
  half  = c(   1,    2,     1,    2),
  return = c(1.88, 0.59, 0.92, 0.17)
)
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer("2015":"2016", names_to = "year", values_to = "return", names_ptype = list(year = double()))
```

## Separating and uniting

```{r separate}
# it leaves the type of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better types using convert = TRUE.
table3 %>% 
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)

# You can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into.
table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
table3 %>% 
  separate(year, into = c("century", "year"), sep = -2)
```

```{r unite}
# In this case we also need to use the sep argument. The default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ""
table5 %>% 
  unite(new, century, year)
table5 %>% 
  unite(new, century, year, sep = "")
```

## Missing values

```{r missing}
(stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
))

# The way that a dataset is represented can make implicit values explicit. 
stocks %>% 
  pivot_wider(names_from = year, values_from = return)

# set values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer(
    cols = c("2015", "2016"), 
    names_to = "year", 
    values_to = "return", 
    values_drop_na = TRUE
  )

# complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary.
stocks %>% 
  complete(year, qtr)

# There’s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward.
(treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
))
# You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward, LOCF).
treatment %>% 
  fill(person)
```

## Case study

```{r case}
(who_tidy = who %>% 
  pivot_longer(
    cols = new_sp_m014:newrel_f65, 
    names_to = "key", 
    values_to = "cases", 
    values_drop_na = TRUE
  ) %>% 
  mutate(key = str_replace(key, "newrel", "new_rel")) %>% 
  separate(key, c("new", "type", "sexage"), sep = "_") %>% 
  select(-iso2, -iso3, -new) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>% 
  mutate(age_group = case_when(
    age == "014" ~ "0-14",
    age == "1524" ~ "15-24",
    age == "2534" ~ "25-34", 
    age == "3544" ~ "35-44",
    age == "4554" ~ "45-54",
    age == "5564" ~ "55-64",
    age == "65" ~ "65+",
    TRUE ~ NA_character_
  )) %>% 
  select(-age))

who_tidy %>% 
  group_by(country, year, sex) %>% 
  summarise(total_cases = sum(cases)) %>% 
  mutate(sex = ifelse(sex == "f", "female", "male")) %>% 
  ggplot(mapping = aes(x = year, y = total_cases, fill = sex)) +
  geom_col() +
  coord_cartesian(xlim = c(1995, 2013))
```

# Relational data

## Keys

* A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.

* A foreign key uniquely identifies an observation in another table. For example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.

If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key.

A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.

```{r key}
flights %>% 
  mutate(surrogate = row_number()) %>% 
  select(surrogate, everything())
```

## Mutating joins

```{r mutating join}
(flights2 <- flights %>% 
  select(year:day, hour, origin, dest, tailnum, carrier))

flights2 %>%
  select(-origin, -dest) %>% 
  left_join(airlines, by = "carrier")

# full_join()
```

```{r duplicate keys}
# One table has duplicate keys: This is useful when you want to add in additional information as there is typically a one-to-many relationship. Here, "key" is a primary key in y and a foreign key in x.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     1, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2"
))
left_join(x, y, by = "key")

# Both tables have duplicate keys: This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     3, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     2, "y3",
     3, "y4"
))
left_join(x, y, by = "key")
```

```{r define key columns}
# The default, by = NULL, uses all variables that appear in both tables, the so called natural join. e.g., the flights and weather tables match on their common variables: year, month, day, hour and origin.
flights2 %>% 
  left_join(weather) # Joining, by = c("year", "month", "day", "hour", "origin")

# A character vector, by = "x". This is like a natural join, but uses only some of the common variables. e.g., flights and planes have year variables, but they mean different things so we only want to join by tailnum.
flights2 %>% 
  left_join(planes, by = "tailnum") # year.x, year.y

# A named character vector: by = c("a" = "b"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. e.g., if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to.
flights2 %>% 
  left_join(airports, by = c("dest" = "faa"))
flights2 %>% 
  left_join(airports, by = c("origin" = "faa"))
```

```{r exercise}
(flights_avgdelay = flights %>% 
  group_by(dest) %>% 
  summarise(avgdelay = mean(arr_delay, na.rm = T)))

airports %>%
  inner_join(flights_avgdelay, by = c("faa" = "dest")) %>%
  ggplot(mapping = aes(x = lon, y = lat)) +
    borders("state") +
    geom_point(mapping = aes(size = avgdelay, color = avgdelay, alpha = 0.5)) +
    coord_quickmap()

# Joining different variables between the tables, e.g. inner_join(x, y, by = c("a" = "b")) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins) ?
```

## Filtering joins

```{r filtering join}
# Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables.

# semi_join(x, y) keeps all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows.
(top_desc <- flights %>% 
   count(dest, sort = TRUE) %>% 
   head(10))
semi_join(flights, top_desc)

# anti_join(x, y) drops all observations in x that have a match in y. Anti-joins are useful for diagnosing join mismatches. 
```

## Set operations

```{r set}
# All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets.
# intersect(x, y): return only observations in both x and y.
# union(x, y): return unique observations in x and y.
# setdiff(x, y): return observations in x, but not in y.

(df1 <- tribble(
  ~x, ~y,
   1,  1,
   2,  1
))
(df2 <- tribble(
  ~x, ~y,
   1,  1,
   1,  2
))
intersect(df1, df2)
union(df1, df2)
setdiff(df1, df2)
setdiff(df2, df1)
```

# Strings

## String basics

```{r basic}
# To include a literal single or double quote in a string you can use \ to “escape” it.
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
# if you want to include a literal backslash, you’ll need to double it up: "\\"

# the printed representation shows the escapes. To see the raw contents of the string, use writeLines()
(x <- c("\"", "\\"))
writeLines(x)

# "\n": newline; "\t": tab. complete list: ?'"', or ?"'". like "\u00b5", this is a way of writing non-English characters that works on all platforms.
(x <- "\u00b5")
writeLines(x)
```

### String length

```{r length}
# str_length() tells you the number of characters in a string.
str_length(c("a", "R for data science", NA)) # include space

# The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions.
```

### Combining strings

```{r combine}
str_c("x", "y")
str_c("x", "y", "z")
str_c("x", "y", sep = ", ")

# missing values are contagious. If you want them to print as "NA", use str_replace_na().
x <- c("abc", NA)
str_c("|-", x, "-|")
str_c("|-", str_replace_na(x), "-|")

# As shown above, str_c() is vectorised, and it automatically recycles shorter vectors to the same length as the longest.
str_c("prefix-", c("a", "b", "c"), "-suffix")

# Objects of length 0 are silently dropped. This is particularly useful in conjunction with if.
name <- "Hadley"
time_of_day <- "morning"
birthday <- FALSE
str_c(
  "Good ", time_of_day, " ", name,
  if (birthday) " and HAPPY BIRTHDAY",
  "."
)

# To collapse a vector of strings into a single string, use collapse.
str_c(c("x", "y", "z"), collapse = ", ")
```

### Subsetting strings

```{r subset}
# str_sub() takes start and end arguments which give the (inclusive) position of the substring.
(x <- c("Apple", "Banana", "Pear"))
str_sub(x, 1, 3) # vectorized
str_sub(x, -3, -1) # negative numbers count backwards from end

# str_sub() won’t fail if the string is too short: it will just return as much as possible.
str_sub("ab", 1, 5)

str_sub(x, 1, 1) <- str_to_lower(str_sub(x, 1, 1))
```

### Locales

```{r locale}
dog <- "The quick brown dog in the rain"
str_to_upper(dog)
str_to_lower(dog)
str_to_title(dog)
str_to_sentence("the quick brown dog in the rain")

# different languages have different rules for changing case. You can pick which set of rules to use by specifying a locale.
# Turkish has two i's: with and without a dot, and it has a different rule for capitalising them.
str_to_upper(c("i", "ı"))
str_to_upper(c("i", "ı"), locale = "tr")

# The base R order() and sort() functions sort strings using the current locale. If you want robust behaviour across different computers, you may want to use str_sort() and str_order() which take an additional locale argument.
x <- c("apple", "eggplant", "banana")
str_sort(x, locale = "en")  # English
str_sort(x, locale = "haw") # Hawaiian

?str_wrap
?str_trim
```

## Matching patterns with regular expressions

### Basic matches

```{r basic match}
# the simplest patterns match exact strings
x <- c("apple", "banana", "pear")
str_view(x, "an")

# .: which matches any character (except a newline)
str_view(x, ".a.")

# So to match an ".", you need the regexp "\.". Unfortunately this creates a problem. We use strings to represent regular expressions, and "\" is also used as an escape symbol in strings. So to create the regular expression "\." we need the string "\\."
# To create the regular expression, we need \\
(dot <- "\\.")
# But the expression itself only contains one:
writeLines(dot)
# And this tells R to look for an explicit.
str_view(c("abc", "a.c", "bef"), "a\\.c")

# to match a literal \ you need to write "\\\\"
(x <- "a\\b")
writeLines(x)
str_view(x, "\\\\")
```

### Anchors

```{r anchor}
# ^ to match the start of the string.
# $ to match the end of the string.
x <- c("apple", "banana", "pear")
str_view(x, "^a")
str_view(x, "a$")
# if you begin with power (^), you end up with money ($).

# To force a regular expression to only match a complete string, anchor it with both ^ and $.
x <- c("apple pie", "apple", "apple cake")
str_view(x, "apple")
str_view(x, "^apple$")

# You can also match the boundary between words with \b. when I want to find the name of a function that’s a component of other functions
x <- c("sum", "summarise", "summary", "rowsum")
str_view(x, "\bsum\b") # ???

str_view(x, "^sum$", match = TRUE)
```

### Character classes and alternatives

```{r character class}
# \d: matches any digit.
# \s: matches any whitespace (e.g. space, tab, newline).
# [abc]: matches a, b, or c.
# [^abc]: matches anything except a, b, or c.

# A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex.
str_view(c("abc", "a.c", "a*c", "a c"), "a[.]c")
str_view(c("abc", "a.c", "a*c", "a c"), ".[*]c")
str_view(c("abc", "a.c", "a*c", "a c"), "a[ ]")

# You can use alternation to pick between one or more alternative patterns. e.g., abc|d..f will match either ‘“abc”’, or "deaf". Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz.
str_view(c("grey", "gray"), "gr(e|a)y")
str_view(c("grey", "gray"), "gray|grey")
```

### Repetition

```{r repeat}
# control how many times a pattern matches:
# ?: 0 or 1
# +: 1 or more
# *: 0 or more
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?")
str_view(x, "CC+")
str_view(x, "C[LX]+")

# Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+.

# You can also specify the number of matches precisely:
# {n}: exactly n
# {n,}: n or more
# {,m}: at most m
# {n,m}: between n and m
str_view(x, "C{2}")
str_view(x, "C{2,}")
str_view(x, "C{2,3}")

# By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them.
str_view(x, "C{2,3}?")
str_view(x, "C[LX]+?")
```

### Grouping and backreferences

```{r group}
# Parentheses create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \1, \2 etc. e.g., the following regular expression finds all fruits that have a repeated pair of letters.
str_view(fruit, "(..)\\1", match = TRUE)
str_view(fruit, "(.)(.)\\2\\1", match = T)
str_view(fruit, "(.).\\1.\\1", match = T)
```

## Tools

### Detect matches

```{r detect}
x <- c("apple", "banana", "pear")
str_detect(x, "e")

# How many common words start with t?
sum(str_detect(words, "^t"))
# What proportion of common words end with a vowel?
mean(str_detect(words, "[aeiou]$"))

words[str_detect(words, "x$")]
str_subset(words, "x$")

(df <- tibble(
  word = words, 
  i = seq_along(word) # i = row_number(word)
))
df %>% 
  filter(str_detect(word, "x$"))

# rather than a simple yes or no, it tells you how many matches there are in a string
x <- c("apple", "banana", "pear")
str_count(x, "a")
# On average, how many vowels per word?
mean(str_count(words, "[aeiou]"))

df %>% 
  mutate(
    vowels = str_count(word, "[aeiou]"),
    consonants = str_count(word, "[^aeiou]")
  )

# many stringr functions come in pairs: one function works with a single match, and the other works with all matches. The second function will have the suffix _all.
str_view_all("abababa", "aba")
```

### Extract matches

```{r extract}
colours <- c("red", "orange", "yellow", "green", "blue", "purple")
(colour_match <- str_c(colours, collapse = "|"))
has_colour <- str_subset(sentences, colour_match)
(matches <- str_extract(has_colour, colour_match)) # str_extract() only extracts the first match.

more <- sentences[str_count(sentences, colour_match) > 1]
str_view_all(more, colour_match)
# str_extract(more, colour_match) # str_extract() only extracts the first match
str_extract_all(more, colour_match) # return a list
# If you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest.
str_extract_all(more, colour_match, simplify = TRUE)
x <- c("a", "a b", "a b c")
str_extract_all(x, "[a-z]", simplify = TRUE)
```

### Grouped matches

```{r group}
# You can also use parentheses to extract parts of a complex match. e.g., we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space.
noun <- "(a|the) ([^ ]+)"
has_noun <- sentences %>%
  str_subset(noun) %>%
  head(10)
has_noun %>% 
  str_extract(noun) # str_extract() gives us the complete match

# str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group。
has_noun %>% 
  str_match(noun)
# Like str_extract(), if you want all matches for each string, you’ll need str_match_all().

# If your data is in a tibble, it’s often easier to use tidyr::extract(). It works like str_match() but requires you to name the matches, which are then placed in new column.
tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("article", "noun"), "(a|the) ([^ ]+)", 
    remove = FALSE
  )
```

### Replacing matches

```{r replace}
x <- c("apple", "pear", "banana")
str_replace(x, "[aeiou]", "-")
str_replace_all(x, "[aeiou]", "-")

# With str_replace_all() you can perform multiple replacements by supplying a named vector.
x <- c("1 house", "2 cars", "3 people")
str_replace_all(x, c("1" = "one", "2" = "two", "3" = "three"))

# you can use backreferences to insert components of the match. In the following code, flip the order of the second and third words.
sentences %>% 
  str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") %>% 
  head(5)
```

### Splitting

```{r split}
sentences %>%
  head(5) %>% 
  str_split(" ") # each component might contain a different number of pieces, return a list

"a|b|c|d" %>% 
  str_split("\\|") %>% 
  .[[1]] # working with a length-1 vector, just extract the first element of the list

sentences %>%
  head(5) %>% 
  str_split(" ", simplify = TRUE) # return a matrix

# request a maximum number of pieces
fields <- c("Name: Hadley", "Country: NZ", "Age: 35")
fields %>% 
  str_split(": ", n = 2, simplify = TRUE)

# Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary().
x <- "This is a sentence.  This is another sentence."
str_view_all(x, boundary()) # each character
str_view_all(x, boundary("word")) # each word
# str_view_all(x, boundary("line_break"))
str_view_all(x, boundary("sentence")) # each sentence
str_split(x, " ")[[1]] # including extra " " between
str_split(x, boundary("word"))[[1]] # no extra " ", no punctuation
```

### Find matches

str_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them.

## Other types of pattern

```{r other pattern}
# The regular call: str_view(fruit, "nana") is shorthand for str_view(fruit, regex("nana")).

# ignore_case = TRUE allows characters to match either their uppercase or lowercase forms.
bananas <- c("banana", "Banana", "BANANA")
str_view(bananas, regex("banana", ignore_case = TRUE))

# multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string.
x <- "Line 1\nLine 2\nLine 3"
writeLines(x)
str_extract_all(x, "^Line")[[1]]
str_extract_all(x, regex("^Line", multiline = TRUE))[[1]]

# comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: "\\ ".
phone <- regex("
  \\(?     # optional opening parens
  (\\d{3}) # area code
  [) -]?   # optional closing parens, space, or dash
  (\\d{3}) # another three numbers
  [ -]?    # optional space or dash
  (\\d{4}) # four more numbers
  ", comments = TRUE)
str_match("514-791-8141", phone)
str_match("(347)937-0461", phone)

# dotall = TRUE allows . to match everything, including \n.

# fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example.
microbenchmark(
  fixed = str_detect(sentences, fixed("the")),
  regex = str_detect(sentences, "the"),
  times = 20
)

# coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. Unfortunately different parts of the world use different rules!

x <- "This is a sentence."
str_view_all(x, boundary("word"))
str_extract_all(x, boundary("word"))
```

## Other uses of regular expressions

```{r regular expression}
# apropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function.
apropos("replace")

# dir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with:
dir(pattern = "\\.Rmd$")

# (If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx()). ?
```

## Stringi

Stringi package contains almost every function you might ever need: stringi has 244 functions to stringr’s 49.

If you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_.

# Factors

## Creating factors

```{r create factor}
x1 <- c("Dec", "Apr", "Jan", "Mar")
(y1 <- factor(x1, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")))
sort(y1)

# any values not in the set will be silently converted to NA.
(y2 <- factor(c("Dec", "Apr", "Jam", "Mar"), levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))) 
parse_factor(c("Dec", "Apr", "Jam", "Mar"), levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) # warning

# If you omit the levels, they’ll be taken from the data in alphabetical order.

# Sometimes you’d prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to unique(x), or after the fact, with fct_inorder().
(f1 <- factor(x1, levels = unique(x1)))
(f2 <- x1 %>% factor() %>% fct_inorder())

# If you ever need to access the set of valid levels directly, you can do so with levels().
levels(f2)
```

## General Social Survey

```{r GSS}
gss_cat

ggplot(gss_cat, aes(race)) +
  geom_bar() # By default, ggplot2 will drop levels that don’t have any values. 
# You can force them to display with:
ggplot(gss_cat, aes(race)) +
  geom_bar() +
  scale_x_discrete(drop = FALSE)

ggplot(gss_cat, aes(rincome)) + 
  geom_bar() +
  coord_flip()

gss_cat %>% 
  count_by(relig) %>% 
  arrange(desc(n))

gss_cat %>% 
  count_by(partyid) %>% 
  arrange(desc(n))

gss_cat %>% 
  select(relig, denom)
gss_cat %>% 
  count(relig, denom) %>% 
  arrange(desc(n))
gss_cat %>% 
  ggplot(aes(x = relig, y = denom)) +
  geom_point()
gss_cat %>% 
  filter(denom != "Not applicable") %>% 
  count(relig, denom)
# denom is related to "Protestant" in relig
```

## Modifying factor order

```{r modify order}
relig_summary <- gss_cat %>%
  group_by(relig) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(relig_summary, aes(tvhours, relig)) + geom_point()

# fct_reorder() takes three arguments:
# f, the factor whose levels you want to modify.
# x, a numeric vector that you want to use to reorder the levels.
# Optionally, fun, a function that’s used if there are multiple values of x for each value of f. The default value is median.
ggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +
  geom_point()

# fct_relevel(): It takes a factor, f, and then any number of levels that you want to move to the front of the line.
rincome_summary <- gss_cat %>%
  group_by(rincome) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(rincome_summary, aes(age, fct_relevel(rincome, "Not applicable"))) +
  geom_point()

# fct_reorder2() reorders the factor by the y values associated with the largest x values. This makes the plot easier to read because the line colours line up with the legend.
by_age <- gss_cat %>%
  filter(!is.na(age)) %>%
  count(age, marital) %>%
  group_by(age) %>%
  mutate(prop = n / sum(n))
ggplot(by_age, aes(age, prop, colour = marital)) +
  geom_line(na.rm = TRUE)
ggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +
  geom_line() +
  labs(colour = "marital") # change the name of the legend from "fct_reorder2(marital, age, prop)" to "marital"
# fct_reorder() is useful for 1d displays where the factor is mapped to position; fct_reorder2() for 2d displays where the factor is mapped to a non-position aesthetic. fct_reorder2(.f, .x, .y, .fun = last2, ..., .desc = TRUE), last2() finds the last value of y when sorted by x. 

Usage

# For bar plots, you can use fct_infreq() to order levels in increasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. You may want to combine with fct_rev().
gss_cat %>%
  mutate(marital = marital %>% fct_infreq()) %>%
  ggplot(aes(marital)) +
    geom_bar() # decreasing
gss_cat %>%
  mutate(marital = marital %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(marital)) +
    geom_bar() # increasing
```

## Modifying factor levels

```{r modify level}
# More powerful than changing the orders of the levels is changing their values. This allows you to clarify labels for publication, and collapse levels for high-level displays.
gss_cat %>% count(partyid)
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat"
  )) %>%
  count(partyid)
# fct_recode() will leave levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.

# To combine groups, you can assign multiple old levels to the same new level.
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat",
    "Other"                 = "No answer",
    "Other"                 = "Don't know",
    "Other"                 = "Other party"
  )) %>%
  count(partyid)

# If you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels.
gss_cat %>%
  mutate(partyid = fct_collapse(partyid,
    "other" = c("No answer", "Don't know", "Other party"),
    "rep" = c("Strong republican", "Not str republican"),
    "ind" = c("Ind,near rep", "Independent", "Ind,near dem"),
    "dem" = c("Not str democrat", "Strong democrat")
  )) %>%
  count(partyid)

# Sometimes you just want to lump together all the small groups to make a plot or table simpler.
gss_cat %>%
  mutate(relig = fct_lump(relig)) %>%
  count(relig)
# The default behaviour is to progressively lump together the smallest groups, ensuring that the aggregate is still the smallest group. In this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’ve probably over collapsed. Instead, we can use the n parameter to specify how many groups (excluding other) we want to keep.
gss_cat %>%
  mutate(relig = fct_lump(relig, n = 10)) %>%
  count(relig, sort = TRUE)
  # %>% print(n = Inf)
```

# Dates and times

## Creating date/times

### From strings

```{r date-time from string}
today()
now()

ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")
ymd(20170131)

ymd_hms("2017-01-31 20:11:59")
mdy_hm("01/31/2017 08:01")

# force the creation of a date-time from a date by supplying a timezone
ymd(20170131, tz = "UTC")
str(ymd(20170131, tz = "UTC"))
```

### From individual components

```{r date-time from components}
flights %>% 
  select(year, month, day) %>% 
  mutate(depart_day = make_date(year, month, day))
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute))

make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}
(flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time")))

flights_dt %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day
flights_dt %>% 
  filter(dep_time < ymd(20130102)) %>% # date: 20130101
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 600) # 600 seconds = 10 minutes
# Note that when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means 1 day. For dates, 1 means 1 day.
```

### From other types

```{r date-time from other}
# switch between a date-time and a date
as_datetime(today())
as_date(now())

# Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().
as_datetime(60 * 60 * 10)
as_date(365 * 10 + 2)

d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
mdy(d1)
ymd(d2)
dmy(d3)
mdy(d4)
mdy(d5)
```

## Date-time components

### Getting components

```{r get components}
# pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second().
mday(today())
yday(today())
wday(today()) # 1-7, Sunday is the 1st day
hour(now())
minute(now())
second(now())

# For month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.
month(today(), label = TRUE, abbr = FALSE)
wday(today(), label = TRUE, abbr = FALSE)

flights_dt %>% 
  mutate(wday = wday(dep_time, label = TRUE)) %>% 
  ggplot(aes(x = wday)) +
    geom_bar()

flights_dt %>% 
  mutate(minute = minute(dep_time)) %>% 
  group_by(minute) %>% 
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    n = n()) %>% 
  ggplot(aes(minute, avg_delay)) +
    geom_line()

sched_dep <- flights_dt %>% 
  mutate(minute = minute(sched_dep_time)) %>% 
  group_by(minute) %>% 
  summarise(
    avg_delay = mean(arr_delay, na.rm = TRUE),
    n = n())
ggplot(sched_dep, aes(minute, avg_delay)) +
  geom_line()
ggplot(sched_dep, aes(minute, n)) +
  geom_line()
```

### Rounding

```{r round}
# floor_date(), round_date(), and ceiling_date(): takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to.
flights_dt %>% 
  count(week = floor_date(dep_time, "week")) %>% # plot the number of flights per week
  ggplot(aes(week, n)) +
    geom_line()
```

### Setting components

```{r set components}
(datetime <- ymd_hms("2016-07-08 12:34:56"))
year(datetime) <- 2020
datetime
month(datetime) <- 01
datetime
hour(datetime) <- hour(datetime) + 1
datetime

update(datetime, year = 2020, month = 2, mday = 2, hour = 2)

# If values are too big, they will roll-over.
ymd("2015-02-01") %>% 
  update(mday = 30)
ymd("2015-02-01") %>% 
  update(hour = 400)
ymd("2020-01-31") %>% 
  update(month = 2)

# show the distribution of flights across the course of the day for every day of the year
flights_dt %>% 
  mutate(dep_hour = update(dep_time, yday = 1)) %>% 
  ggplot(aes(dep_hour)) +
    geom_freqpoly(binwidth = 300)
# Setting larger components of a date to a constant is a powerful technique that allows you to explore patterns in the smaller components.
```

## Time spans

### Durations

Durations: represent an exact number of seconds.

```{r duration}
# How old are you?
(h_age <- today() - ymd(19941021)) # a difftime class object

as.duration(h_age) # in second
dseconds(15)
dminutes(10)
dhours(c(12, 24))
ddays(0:5)
dweeks(3)
dyears(1)
# Larger units are created by converting minutes, hours, days, weeks, and years to seconds at the standard rate (60 seconds in a minute, 60 minutes in an hour, 24 hours in day, 7 days in a week, 365 days in a year).

2 * dyears(1)
dyears(1) + dweeks(12) + dhours(15)
(tomorrow <- today() + ddays(1))
(last_year <- today() - dyears(1)) # be careful of leap year
(one_pm <- ymd_hms("2016-03-12 13:00:00", tz = "America/New_York"))
one_pm + ddays(1) # The time zone has changed. Because of DST, March 12 only has 23 hours, so if we add a full days worth of seconds we end up with a different time.
```

### Periods

Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months.

```{r period}
seconds(15)
minutes(10)
hours(c(12, 24))
days(7)
months(1:6)
weeks(3)
years(1)

10 * (months(6) + days(1))
days(50) + hours(25) + minutes(2)

# A leap year
ymd("2016-01-01") + dyears(1) # 2016-12-31
ymd("2016-01-01") + years(1)
# # Daylight Savings Time
one_pm + ddays(1)
one_pm + days(1)

flights_dt %>% 
  filter(arr_time < dep_time) 
# Overnight flights: We used the same date information for both the departure and the arrival times, but these flights arrived on the following day.
flights_dt <- flights_dt %>% 
  mutate(
    overnight = arr_time < dep_time, # TRUE/FALSE = 0/1
    arr_time = arr_time + days(overnight * 1),
    sched_arr_time = sched_arr_time + days(overnight * 1)
  )
flights_dt %>% 
  filter(overnight, arr_time < dep_time) 
```

### Intervals

```{r interval}
years(1)/days(1)

# An interval is a duration with a starting point: that makes it precise so you can determine exactly how long it is.
next_year <- today() + years(1)
(today() %--% next_year) / ddays(1)
(today() %--% next_year) %/% days(1)
```

### Summary

If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.

## Time zones

```{r time zone}
Sys.timezone()

length(OlsonNames())
head(OlsonNames())

# the time zone is an attribute of the date-time that only controls printing
(x1 <- ymd_hms("2015-06-01 12:00:00", tz = "America/New_York"))
(x2 <- ymd_hms("2015-06-01 18:00:00", tz = "Europe/Copenhagen"))
(x3 <- ymd_hms("2015-06-02 04:00:00", tz = "Pacific/Auckland"))
x1-x2
x2-x3

#  Operations that combine date-times, like c(), will often drop the time zone. In that case, the date-times will display in your local time zone.
(x4 <- c(x1, x2, x3))

# Keep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display.
(x4a <- with_tz(x4, tzone = "Australia/Lord_Howe"))
x4a - x4
# Change the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.
(x4b <- force_tz(x4, tzone = "Australia/Lord_Howe"))
x4b - x4
```

# Pipes

## Piping alternatives

### Intermediate steps

The simplest approach is to save each step as a new object.

### Overwrite the original

### Function composition

Another approach is to abandon assignment and just string the function calls together.

### Use the pipe

```{r pipe}
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mice) %>%
  bop(on = head)
# The pipe works by performing a “lexical transformation”: behind the scenes, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. When you run a pipe like the one above, magrittr does something like this:
my_pipe <- function(.) {
  . <- hop(., through = forest)
  . <- scoop(., up = field_mice)
  bop(., on = head)
}
my_pipe(foo_foo)

# pipe won’t work for two classes of functions:
# 1. Functions that use the current environment. e.g., assign() will create a new variable with the given name in the current environment:
assign("x", 10)
x
"x" %>% assign(100)
x
# The use of assign with the pipe does not work because it assigns it to a temporary environment used by %>%. If you do want to use assign with the pipe, you must be explicit about the environment:
env <- environment()
"x" %>% assign(100, envir = env)
x
# Other functions with this problem include get() and load().
# 2. Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behaviour.
# One place that this is a problem is tryCatch(), which lets you capture and handle errors:
tryCatch(stop("!"), error = function(e) "An error")
stop("!") %>% 
  tryCatch(error = function(e) "An error")
# There are a relatively wide class of functions with this behaviour, including try(), suppressMessages(), and suppressWarnings() in base R.
```

## When not to use the pipe

* Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent.

* You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe.

* You are starting to think about a directed graph with a complex dependency structure. Pipes are fundamentally linear and expressing complex relationships with them will typically yield confusing code.

## Other tools from magrittr

```{r other tool}
# When working with more complex pipes, it’s sometimes useful to call a function for its side-effects. Maybe you want to print out the current object, or plot it, or save it to disk. Many times, such functions don’t return anything, effectively terminating the pipe. you can use the “tee” pipe. %T>% works like %>% except that it returns the left-hand side instead of the right-hand side. ?
rnorm(100) %>%
  matrix(ncol = 2) %>%
  plot() %>%
  str()
#>  NULL
rnorm(100) %>%
  matrix(ncol = 2) %T>%
  plot() %>%
  str()

# If you’re working with functions that don’t have a data frame based API (i.e. you pass them individual vectors, not a data frame and expressions to be evaluated in the context of that data frame), you might find %$% useful. It “explodes” out the variables in a data frame so that you can refer to them explicitly.
mtcars %$%
  cor(disp, mpg)

# For assignment magrittr provides the %<>% operator.
mtcars <- mtcars %>% 
  transform(cyl = cyl * 2)
mtcars %<>% transform(cyl = cyl * 2)
```

# Functions

## When should you write a function?

```{r write function}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(c(0, 5, 10))

rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```

## Functions are for humans and computers

```{r human computer}
# Another important use of comments is to break up your file into easily readable chunks. Use long lines of - and = to make it easy to spot the breaks.

# Load data --------------------------------------

# Plot data --------------------------------------

# RStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl + Shift + R), and will display them in the code navigation drop-down at the bottom-left of the editor. ?
```

## Conditional execution

### Conditions

```{r condition}
# The condition must evaluate to either TRUE or FALSE. If it’s a vector, you’ll get a warning message; if it’s an NA, you’ll get an error. 
if (c(TRUE, FALSE)) {}
if (NA) {}

# You can use || (or) and && (and) to combine multiple logical expressions. These operators are “short-circuiting”: as soon as || sees the first TRUE it returns TRUE without computing anything else. As soon as && sees the first FALSE it returns FALSE.
# You should never use | or & in an if statement: these are vectorised operations that apply to multiple values (that’s why you use them in filter()). If you do have a logical vector, you can use any() or all() to collapse it to a single value.

# Be careful when testing for equality. == is vectorised, which means that it’s easy to get more than one output. Either check the length is already 1, collapse with all() or any(), or use the non-vectorised identical(). identical() is very strict: it always returns either a single TRUE or a single FALSE, and doesn’t coerce types. This means that you need to be careful when comparing integers and doubles.
identical(0L, 0)
```

### Multiple conditions

```{r multiple conditions}
if (this) {
  # do that
} else if (that) {
  # do something else
} else {
  # 
}

op = function(x, y, op) {
   switch(op,
     plus = x + y,
     minus = x - y,
     times = x * y,
     divide = x / y,
     stop("Unknown op!")
   )
}
op(1, 2, "plus")
op(1, 2, "minus")
op(1, 2, "times")
op(1, 2, "divide")
op(1, 2, "squareroot")

# cut(): discretise continuous variables
# Convert Numeric to Factor, divides the range of x into intervals and codes the values in x according to which interval they fall.
Z <- rnorm(10000)
table(cut(Z, breaks = -6:6))
table(cut(Z, breaks = -6:6, labels = FALSE))
hist(Z, breaks = -6:6, plot = FALSE)$counts
```

### Code style

```{r code style}
if (y < 0 && debug) {
  message("Y is negative")
}

if (y == 0) {
  log(x)
} else {
  y ^ x
}

# It’s ok to drop the curly braces if you have a very short if statement that can fit on one line.
y <- 10
x <- if (y < 20) "Too low" else "Too high"
if (y < 20) {
  "Too low"
} else {
  "Too high"
}

switch(x, 
  a = ,
  b = "ab",
  c = ,
  d = "cd"
)
```

## Function arguments

```{r argument}
# Generally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument.
# Compute confidence interval around mean using normal approximation
mean_ci <- function(x, conf = 0.95) {
  se <- sd(x) / sqrt(length(x))
  alpha <- 1 - conf
  mean(x) + se * qnorm(c(alpha/2, 1 - alpha/2))
}
x <- runif(100)
mean_ci(x)
mean_ci(x, conf = 0.99)
```

### Choosing names

* x, y, z: vectors.

* w: a vector of weights.

* df: a data frame.

* i, j: numeric indices (typically rows and columns).

* n: length, or number of rows.

* p: number of columns.

### Checking values

```{r check value}
# It’s good practice to check important preconditions, and throw an error with stop(), if they are not true.
wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  sum(w*x)/sum(w)
}
wt_mean(1:6, 1:3)

# stopifnot(): checks that each argument is TRUE, and produces a generic error message if not.
wt_mean <- function(x, w, na.rm = FALSE) {
  stopifnot(is.logical(na.rm), length(na.rm) == 1)
  stopifnot(length(x) == length(w))
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
wt_mean(1:6, 6:1, na.rm = "foo")
```

### Dot-dot-dot (...)

```{r ...}
# Many functions in R take an arbitrary number of inputs.
# ... (pronounced dot-dot-dot): captures any number of arguments that aren’t otherwise matched. 
# It’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function.
commas <- function(...) {
  str_c(..., collapse = ", ")
}
commas(letters[1:10])

rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
```

## Return values

### Explicit return statements

```{r explicit return}
# The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). A common reason to do this is because the inputs are empty.
complicated_function <- function(x, y, z) {
  if (length(x) == 0 || length(y) == 0) {
    return(0)
  }
    
  # Complicated code here
}

f <- function() {
  if (!x) {
    return(something_short)
  }

  # Do 
  # something
  # that
  # takes
  # many
  # lines
  # to
  # express
}
```

### Writing pipeable functions

```{r pipeable}
# With transformations, an object is passed to the function’s first argument and a modified object is returned. 
# With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline.
show_missings <- function(df) {
  n <- sum(is.na(df))
  cat("Missing values: ", n, "\n", sep = "")
  
  invisible(df)
}
# If we call it interactively, the invisible() means that the input df doesn’t get printed out.
show_missings(mtcars)
# But it’s still there, it’s just not printed by default.
x <- show_missings(mtcars)
class(x) # data.frame
dim(x) # 32*11
mtcars %>% 
  show_missings() %>% 
  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% 
  show_missings() 
```

# Vectors

## Vector basics

```{r basic}
# NULL is often used to represent the absence of a vector (as opposed to NA which is used to represent the absence of a value in a vector). NULL typically behaves like a vector of length 0.
typeof(letters)
typeof(1:10)
typeof(c(1:10))
x <- list("a", "b", 1:10)
length(x)

# Vectors can also contain arbitrary additional metadata in the form of attributes. These attributes are used to create augmented vectors which build on additional behaviour.
# Factors are built on top of integer vectors.
# Dates and date-times are built on top of numeric vectors.
# Data frames and tibbles are built on top of lists.
```

## Important types of atomic vector

### Logical

```{r logical}
# they can take only three possible values: FALSE, TRUE, and NA.
typeof(1:10 %% 3 == 0)
typeof(c(TRUE, TRUE, FALSE, NA))
```

### Numeric

```{r numeric}
# Numbers are doubles by default. To make an integer, place an L after the number:
typeof(1)
typeof(1L)
1.5L # integer literal 1.5L contains decimal; using numeric value

# Doubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory.
(x <- sqrt(2) ^ 2)
x - 2 # 4.440892e-16
# Instead of comparing floating point numbers using ==, you should use dplyr::near() which allows for some numerical tolerance.

# Integers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf. All three special values NaN, Inf and -Inf can arise during division:
c(-1, 0, 1)/0
# Avoid using == to check for these other special values. Instead use the helper functions is.finite(), is.infinite(), and is.nan():
x <- c(0, -Inf, Inf, NA, NaN)
is.finite(x)
is.infinite(x)
is.na(x)
is.nan(x)
```

### Character

### Missing values

```{r missing}
typeof(NA)
typeof(NA_integer_)
typeof(NA_real_)
typeof(NA_character_)
```

## Using atomic vectors

### Coercion

```{r coercion}
# from integer to logical:
if (length(x)) {
  # do something
}
# 0 is converted to FALSE and everything else is converted to TRUE
# Instead be explicit: length(x) > 0

# create a vector containing multiple types with c(): the most complex type always wins.
typeof(c(TRUE, 1L))
typeof(c(1L, 1.5))
typeof(c(1.5, "a"))
```

### Test functions

```{r test function}
is_logical()	
is_integer()			
is_double()		
is_numeric() # integer & double		
is_character()
is_atomic()	# everything but list	
is_list()
is_vector() # atomic & list

# Each predicate also comes with a “scalar” version, like is_scalar_atomic(), which checks that the length is 1.
```

### Scalars and recycling rules

```{r scalar recycle}
# vector recycling: because the shorter vector is repeated, or recycled, to the same length as the longer vector.
1:10 + 1:2
# This is silent except when the length of the longer is not an integer multiple of the length of the shorter.
1:10 + 1:3

# the vectorised functions in tidyverse will throw errors when you recycle anything other than a scalar. If you do want to recycle, you’ll need to do it yourself with rep().
tibble(x = 1:4, y = 1:2)
tibble(x = 1:4, y = 1)
tibble(x = 1:4, y = rep(1:2, 2))
tibble(x = 1:4, y = rep(1:2, each = 2))
```

### Naming vectors

```{r name}
c(x = 1, y = 2, z = 4)
set_names(1:3, c("a", "b", "c"))
```

### Subsetting

```{r subset}
# x[]: A numeric vector containing only integers. The integers must either be all positive, all negative, or zero.
# Subsetting with positive integers keeps the elements at those positions.
x <- c("one", "two", "three", "four", "five")
x[c(3, 2, 5)]
x[c(1, 1, 5, 5, 5, 2)]
x[6]
# Negative values drop the elements at the specified positions.
x[c(-1, -3, -5)]
x[c(1, -2)]
x[0]
x[-6]

# x[]: Subsetting with a logical vector keeps all values corresponding to a TRUE value. This is most often useful in conjunction with the comparison functions.
x <- c(10, 3, NA, 5, 8, 1, NA)
# All non-missing values of x
x[!is.na(x)]
# All even (or missing!) values of x
x[x %% 2 == 0]

# x[]: If you have a named vector, you can subset it with a character vector.
x <- c(abc = 1, def = 2, xyz = 5)
x[c("xyz", "def")]
x[c("xyz", "xyz")]
x["efg"]

# if x is 2d, x[1, ] selects the first row and all the columns, and x[, -1] selects all rows and all columns except the first.

# There is an important variation of [ called [[. [[ only ever extracts a single element, and always drops names. It’s a good idea to use it whenever you want to make it clear that you’re extracting a single item, as in a for loop.

x <- c(-1, -2, 0, 1, 2)
x[-which(x > 0)]
x[x <= 0]
```

## Recursive vectors (lists)

```{r list}
# lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures.
(x <- list(1, 2, 3))
str(x)
(x_named <- list(a = 1, b = 2, c = 3))
str(x_named)
y <- list("a", 1L, 1.5, TRUE)
str(y)

# Lists can even contain other lists!
z <- list(list(1, 2), list(3, 4))
str(z)
```

### Subsetting

```{r list subset}
(a <- list(a = 1:3, b = "a string", c = pi, d = list(-1, -5)))

# [ extracts a sub-list. The result will always be a list.
str(a[1:2])
str(a[4])
# Like with vectors, you can subset with a logical, integer, or character vector.

# [[ extracts a single component from a list. It removes a level of hierarchy from the list.
str(a[[1]])
str(a[[4]])

# $ is a shorthand for extracting named elements of a list. It works similarly to [[ except that you don’t need to use quotes.
a$a
a[["a"]]
```

## Atrrtibutes

```{r attribute}
# Any vector can contain arbitrary additional metadata through its attributes. You can think of attributes as named list of vectors that can be attached to any object. You can get and set individual attribute values with attr() or see them all at once with attributes().
x <- 1:10
attr(x, "greeting")
attr(x, "greeting") <- "Hi!"
attr(x, "farewell") <- "Bye!"
attributes(x)
```

## Augmented vectors

### Factors

```{r factor}
# Factors are designed to represent categorical data that can take a fixed set of possible values. Factors are built on top of integers, and have a levels attribute.
x <- factor(c("ab", "cd", "ab"), levels = c("ab", "cd", "ef"))
typeof(x) # integer
attributes(x)
# $levels
# [1] "ab" "cd" "ef"

# $class
# [1] "factor"
```

### Dates and date-times

```{r date}
# Dates in R are numeric vectors that represent the number of days since 1 January 1970.
x <- as.Date("1971-01-01")
unclass(x)
typeof(x) # double
attributes(x)
# $class
# [1] "Date"

# Date-times are numeric vectors with class POSIXct that represent the number of seconds since 1 January 1970. (In case you were wondering, “POSIXct” stands for “Portable Operating System Interface”, calendar time.)
x <- ymd_hm("1970-01-01 01:00")
unclass(x)
#> [1] 3600
#> attr(,"tzone")
#> [1] "UTC"
typeof(x) # double
attributes(x)
#> $class
#> [1] "POSIXct" "POSIXt" 
#> 
#> $tzone
#> [1] "UTC"
# The tzone attribute is optional. It controls how the time is printed, not what absolute time it refers to.

# There is another type of date-times called POSIXlt. These are built on top of named lists.
y <- as.POSIXlt(x)
typeof(y) # list
attributes(y)
#> $names
#>  [1] "sec"    "min"    "hour"   "mday"   "mon"    "year"   "wday"   "yday"  
#>  [9] "isdst"  "zone"   "gmtoff"
#> 
#> $class
#> [1] "POSIXlt" "POSIXt" 
#> 
#> $tzone
#> [1] "US/Eastern" "EST"        "EDT"
# POSIXct’s are always easier to work with, so if you find you have a POSIXlt, you should always convert it to a regular data time lubridate::as_date_time().

hms(3600)
```

## Tibbles

```{r tibble}
# Tibbles are augmented lists: they have class “tbl_df” + “tbl” + “data.frame”, and names (column) and row.names attributes.
tb <- tibble(x = 1:5, y = 5:1)
typeof(tb) # list
attributes(tb)
# The difference between a tibble and a list is that all the elements of a data frame must be vectors with the same length.
```

# Iteration

## For loops

```{r for loop}
# The output: output <- vector("double", length(x)). Before you start the loop, you must always allocate sufficient space for the output. 

# The sequence: i in seq_along(df). This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”. It’s a safe version of the familiar 1:length(l), with an important difference: if you have a zero-length vector, seq_along() does the right thing.
y <- vector("double", 0)
seq_along(y)
1:length(y) # you’re likely to get a confusing error message.

# The body: output[[i]] <- median(df[[i]]). This is the code that does the work. It’s run repeatedly, each time with a different value for i. 

out <- ""
for (x in letters) {
  out <- str_c(out, x)
}

x <- sample(100)
sd <- 0
for (i in seq_along(x)) {
  sd <- sd + (x[i] - mean(x)) ^ 2
}
sd <- sqrt(sd / (length(x) - 1))
# sd(x)

x <- runif(100)
out <- vector("numeric", length(x))
out[1] <- x[1]
for (i in 2:length(x)) {
  out[i] <- out[i - 1] + x[i]
}

output <- vector("integer", 0)
for (i in seq_along(x)) {
  output <- c(output, lengths(x[[i]]))
}
output
```

## For loop variation

* Modifying an existing object, instead of creating a new object.

* Looping over names or values, instead of indices.

* Handling outputs of unknown length.

* Handling sequences of unknown length.

## Modifying an existing object

```{r modify existing}
# original rescale:
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)

# for loop modification: 
# Output: we already have the output — it’s the same as the input!
# Sequence: we can think about a data frame as a list of columns, so we can iterate over each column with seq_along(df).
# Body: apply rescale01().
for (i in seq_along(df)) { # seq_along(df) = 1 2 3 4
  df[[i]] <- rescale01(df[[i]])
}
# Typically you’ll be modifying a list or data frame with this sort of loop, so remember to use [[, not [. You might have spotted that I used [[ in all my for loops: I think it’s better to use [[ even for atomic vectors because it makes it clear that I want to work with a single element.
typeof(df[[1]]) # vector
typeof(df[1]) # list
```

### Looping patterns

```{r loop pattern}
# There are three basic ways to loop over a vector. looping over the numeric indices with for (i in seq_along(xs)), and extracting the value with x[[i]].
# Loop over the elements: for (x in xs). This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently.
# Loop over the names: for (nm in names(xs)). This gives you name, which you can use to access the value with x[[nm]]. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so:
results <- vector("list", length(x))
names(results) <- names(x)

for (i in seq_along(x)) {
  name <- names(x)[[i]]
  value <- x[[i]]
}
```

### Unknown output length

```{r unknown output}
# A better solution to save the results in a list, and then combine into a single vector after the loop is done:
means <- c(0, 1, 2)
out <- vector("list", length(means))
for (i in seq_along(means)) {
  n <- sample(100, 1)
  out[[i]] <- rnorm(n, means[[i]])
}
str(out) # list
str(unlist(out)) # vector
# use unlist() to flatten a list of vectors into a single vector. A stricter option is to use purrr::flatten_dbl() — it will throw an error if the input isn’t a list of doubles.

# You might be generating a long string. Instead of paste()ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with paste(output, collapse = "").
# You might be generating a big data frame. Instead of sequentially rbind()ing in each iteration, save the output in a list, then use bind_rows(output) to combine the output into a single data frame.
# switch to a more complex result object, and then combine in one step at the end.
```

### Unknown sequence length

```{r unknown sequence}
# Sometimes you don’t even know how long the input sequence should run for. This is common when doing simulations. "until". A while loop is simpler than for loop because it only has two components, a condition and a body.
for (i in seq_along(x)) {
  # body
}
# Equivalent to
i <- 1
while (i <= length(x)) {
  # body
  i <- i + 1 
}

# use a while loop to find how many tries it takes to get three heads in a row:
flip <- function() {
  sample(c("T", "H"), 1)
}
flips <- 0
nheads <- 0
while (nheads < 3) {
  if (flip() == "H") {
    nheads <- nheads + 1
  } else {
    nheads <- 0
  }
  flips <- flips + 1
}
flips

trans <- list( 
  disp = function(x) x * 0.0163871,
  am = function(x) {
    factor(x, labels = c("auto", "manual"))
  }
)
for (var in names(trans)) {
  mtcars[[var]] <- trans[[var]](mtcars[[var]])
}

trans <- list( 
  disp = function(x) x * 0.0163871,
  am = function(x) {
    factor(x, labels = c("auto", "manual"))
  }
)
for (var in names(trans)) {
  mtcars[[var]] <- trans[[var]](mtcars[[var]]) 
  # mtcars[[disp]] <- trans[[disp(mtcars[[disp]])]]
  # mtcars[[am]] <- trans[[am(mtcars[[am]])]]
}
```

## For loops vs. functionals

```{r functional}
f1 <- function(x) abs(x - mean(x)) ^ 1
f2 <- function(x) abs(x - mean(x)) ^ 2
f3 <- function(x) abs(x - mean(x)) ^ 3
f <- function(x, i) abs(x - mean(x)) ^ i

col_summary <- function(df, fun) {
  out <- vector("double", length(df))
  for (i in seq_along(df)) {
    if (is.numeric(df[[i]]) == T){ # only apply to numeric columns
      out[[i]] <- fun(df[[i]]) # for list/data.frame, [[]]: vector; []: list
    } else {
      out[i] <- NA_real_
    }
  }
  out
}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10),
  e = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j")
)
col_summary(df, mean)
col_summary(df, sd)
# The idea of passing a function to another function is an extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language.

num_summary <- function(var) {
  out <- vector("double", 4)
  out[[1]] <- mean(var)
  out[[2]] <- sd(var)
  out[[3]] <- median(var)
  out[[4]] <- IQR(var)
  out
}
num_summary(df$a)
num_summary(df$d)

desp_num <- function(df) {
  out <- vector("list", length(df))
  for (i in seq_along(df)) {
    if (is.numeric(df[[i]])) {
    out[[i]] <- list(name = names(df[i]),
                  mean = mean(df[[i]], na.rm = T),
                  sd = sd(df[[i]], na.rm = T),
                  median = median(df[[i]], na.rm = T),
                  iqr = quantile(df[[i]], probs = c(0.25, 0.75), na.rm = T))
    } else {
    out[[i]] <- NA
  }
  }
  out
}
desp_num(df)

desp_num1 <- function(df) {
  out <- vector("list", length(df))
  for (i in seq_along(df)) {
    if (is.numeric(df[[i]])) {
    out[[i]] <- df %>% 
      summarise(name = names(df[i]),
                mean = mean(df[[i]], na.rm = T),
                sd = sd(df[[i]], na.rm = T),
                median = median(df[[i]], na.rm = T),
                q1 = quantile(df[[i]], probs = 0.25, na.rm = T),
                q3 = quantile(df[[i]], probs = 0.75, na.rm = T))
    } else {
    out[[i]] <- NA
  }
  }
  out
}
desp_num1(df)

desp_char <- function(df) {
  out <- vector("list", length(df))
  for (i in seq_along(df)) {
    if (is.character(df[[i]])) {
    out[[i]] <- list(name = names(df[i]),
                  df %>% count_by(df[[i]]))
    } else {
    out[[i]] <- NA
  }
  }
  out
}
desp_char(df)

# apply(X, MARGIN, FUN, ...): Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.
```

## The map functions

```{r map func}
# map() makes a list.
# map_lgl() makes a logical vector.
# map_int() makes an integer vector.
# map_dbl() makes a double vector.
# map_chr() makes a character vector.

df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
map_dbl(df, mean)
map_dbl(df, median)
map_dbl(df, sd)
df %>% map_dbl(mean)
df %>% map_dbl(median)
df %>% map_dbl(sd)

# The second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector.

# map_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called.
map_dbl(df, mean, trim = 0.5)

# The map functions also preserve names.
z <- list(x = 1:3, y = 4:5)
map_int(z, length)
```

### Shortcuts

```{r shortcut}
#  split up the mtcars dataset into three pieces (one for each value of cylinder) and fit the same linear model to each piece:
models <- mtcars %>% 
  split(.$cyl) %>% 
  purrr::map(function(df) lm(mpg ~ wt, data = df)) # create an anonymous function inside map()

models <- mtcars %>% 
  split(.$cyl) %>% 
  purrr::map(~lm(mpg ~ wt, data = .))
# "." as a pronoun: it refers to the current list element (in the same way that i referred to the current index in the for loop).

# extract a common summary statistics
models %>% 
  purrr::map(summary) %>% 
  map_dbl(~.$r.squared)
models %>% 
  purrr::map(summary) %>% 
  map_dbl("r.squared")

# use an integer to select elements by position
x <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))
x %>% map_dbl(2)
```

### Base R

```{r base r}
# lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
# sapply is a user-friendly version and wrapper of lapply by default returning a vector, matrix or, if simplify = "array", an array if appropriate, by applying simplify2array(). sapply(x, f, simplify = FALSE, USE.NAMES = FALSE) is the same as lapply(x, f).
# vapply is similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.

# lapply() is basically identical to map(), except that map() is consistent with all the other functions in purrr, and you can use the shortcuts for .f.

# Base sapply() is a wrapper around lapply() that automatically simplifies the output. This is useful for interactive work but is problematic in a function because you never know what sort of output you’ll get:
x1 <- list(
  c(0.27, 0.37, 0.57, 0.91, 0.20),
  c(0.90, 0.94, 0.66, 0.63, 0.06), 
  c(0.21, 0.18, 0.69, 0.38, 0.77)
)
x2 <- list(
  c(0.50, 0.72, 0.99, 0.38, 0.78), 
  c(0.93, 0.21, 0.65, 0.13, 0.27), 
  c(0.39, 0.01, 0.38, 0.87, 0.34)
)

threshold <- function(x, cutoff = 0.8) x[x > cutoff]
x1 %>% sapply(threshold) %>% str()

# vapply() is a safe alternative to sapply() because you supply an additional argument that defines the type. vapply(df, is.numeric, logical(1)) is equivalent to map_lgl(df, is.numeric). One advantage of vapply() over purrr’s map functions is that it can also produce matrices — the map functions only ever produce vectors.

purrr::map(1:5, runif)
purrr::map(-2:2, rnorm, n = 5) 
map_dbl(-2:2, rnorm, n = 5) # Error: Result 1 must be a single double, not a double vector of length 5
```

## Dealing with failure

```{r failure}
# safely() is an adverb: it takes a function (a verb) and returns a modified version. In this case, the modified function will never throw an error. Instead, it always returns a list with two elements: result is the original result. If there was an error, this will be NULL; error is an error object. If the operation was successful, this will be NULL.
safe_log <- safely(log)
str(safe_log(10))
str(safe_log("a"))

x <- list(1, 10, "a")
y <- x %>% purrr::map(safely(log))
str(y)

# This would be easier to work with if we had two lists: one of all the errors and one of all the output.
y <- y %>% 
  transpose() # row <-> column
str(y)

# look at the values of x where y is an error, or work with the values of y that are ok
is_ok <- y$error %>% map_lgl(is_null)
x[!is_ok]
y$result[is_ok] %>% flatten_dbl()

# possibly(): you give it a default value to return when there is an error.
x %>% map_dbl(possibly(log, NA_real_))

# quietly() performs a similar role to safely(), but instead of capturing errors, it captures printed output, warnings, and messages
x <- list(1, -1)
x %>% purrr::map(quietly(log)) %>% str()
```

## Mapping over multiple arguments

```{r map argument}
mu <- list(5, 10, -3)
sigma <- list(1, 5, 10)
# map2() iterates over two vectors in parallel. The arguments that vary for each call come before the function; arguments that are the same for every call come after.
map2(mu, sigma, rnorm, n = 5) %>% str()
# rnorm(mean = 5, sd = 1, n = 5)
# rnorm(mean = 10, sd = 5, n = 5)
# rnorm(mean = -3, sd = 10, n = 5)

# pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples.
n <- list(1, 3, 5)
args1 <- list(n, mu, sigma)
args1 %>%
  pmap(rnorm) %>% 
  str()

# If you don’t name the list’s elements, pmap() will use positional matching when calling the function. It’s better to name the arguments.
args2 <- list(mean = mu, sd = sigma, n = n)
args2 %>% 
  pmap(rnorm) %>% 
  str()

params <- tribble(
  ~mean, ~sd, ~n,
    5,     1,  1,
   10,     5,  3,
   -3,    10,  5
)
params %>% 
  pmap(rnorm)
```

### Invoking different functions

```{r invoke functions}
f <- c("runif", "rnorm", "rpois")
param <- list(
  list(min = -1, max = 1), 
  list(sd = 5), 
  list(lambda = 10)
)
invoke_map(f, param, n = 5) %>% str()
# The first argument is a list of functions or character vector of function names. The second argument is a list of lists giving the arguments that vary for each function. The subsequent arguments are passed on to every function.

sim <- tribble(
  ~f,      ~params,
  "runif", list(min = -1, max = 1),
  "rnorm", list(sd = 5),
  "rpois", list(lambda = 10)
)
sim %>% 
  mutate(sim = invoke_map(f, params, n = 10)) %>% 
  str() # unlist(sim)
```

## Walk

```{r walk}
# Walk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk. 
x <- list(1, "a", 3)
x %>% 
  walk(print)

# if you had a list of plots and a vector of file names, you could use pwalk() to save each file to the corresponding location on disk:
plots <- mtcars %>% 
  split(.$cyl) %>% 
  purrr::map(~ ggplot(., aes(mpg, wt)) + geom_point())
paths <- str_c(names(plots), ".pdf")

pwalk(list(paths, plots), ggsave, path = tempdir())

# walk(), walk2() and pwalk() all invisibly return .x, the first argument. This makes them suitable for use in the middle of pipelines.
```

## Other patterns of for loops

### Predicate functions

```{r predicate}
# keep() and discard() keep elements of the input where the predicate is TRUE or FALSE respectively.
iris %>% 
  keep(is.factor) %>% 
  str()

iris %>% 
  discard(is.factor) %>% 
  str()
iris %>% 
  keep(is.numeric) %>% 
  str()

# some() and every() determine if the predicate is true for any or for all of the elements.
x <- list(1:5, letters, list(10))

x %>% 
  some(is_character)
x %>% 
  every(is_character)
x %>% 
  every(is_vector)

# detect() finds the first element where the predicate is true; detect_index() returns its position.
(x <- sample(10))

x %>% 
  detect(~ . > 5)
x %>% 
  detect_index(~ . > 5)

# head_while() and tail_while() take elements from the start or end of a vector while a predicate is true.
x %>% 
  head_while(~ . > 4)
x %>% 
  tail_while(~ . > 4)
```

### Reduce and accumulate

```{r reduce accumulate}
# Sometimes you have a complex list that you want to reduce to a simple list by repeatedly applying a function that reduces a pair to a singleton. 
# The reduce function takes a “binary” function (i.e. a function with two primary inputs), and applies it repeatedly to a list until there is only a single element left.
dfs <- list(
  age = tibble(
    name = "John", 
    age = 30
    ),
  sex = tibble(
    name = c("John", "Mary"), 
    sex = c("M", "F")
    ),
  trt = tibble(
    name = "Mary", 
    treatment = "A"
    )
)

dfs %>% reduce(full_join)
dfs %>% reduce(left_join)
dfs %>% reduce(right_join)

vs <- list(
  c(1, 3, 5, 6, 10),
  c(1, 2, 3, 7, 8, 10),
  c(1, 2, 3, 4, 8, 9, 10)
)

vs %>% reduce(intersect)

# Accumulate is similar but it keeps all the interim results. You could use it to implement a cumulative sum:
(x <- sample(10))
x %>% 
  accumulate(`+`)
x %>% 
  accumulate(sum)

col_sum3 <- function(df, f) {
  is_num <- sapply(df, is.numeric)
  df_num <- df[, is_num]

  sapply(df_num, f)
}
df <- tibble(
  x = 1:3, 
  y = 3:1,
  z = c("a", "b", "c")
)
col_sum3(df, mean)
col_sum3(df[1:2], mean)
col_sum3(df[1], mean)
col_sum3(df[0], mean)
```


# Model introduction

If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis:

* 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.

* 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.

* 20% is held back for a test set. You can only use this data ONCE, to test your final model.

# Model Basics

## A simple model

```{r simple model}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point() 

model1 <- function(a, data) {
  a[1] + data$x * a[2]
} # predict
model1(c(7, 1.5), sim1)

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
} # least mean square
measure_distance(c(7, 1.5), sim1)

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
} # a helper function because distance function expects the model as a numeric vector of length 2.
models <- models %>% 
  mutate(dist = map2_dbl(a1, a2, sim1_dist))
models

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = - dist), # color = - dist: the smallest distance get the brighest colour
    data = filter(models, rank(dist) <= 10)
  )

# a numerical minimisation tool called Newton-Raphson search. you pick a starting point and look around for the steepest slope. You then ski down that slope a little way, and then repeat again and again, until you can’t go any lower. In R, we can do that with optim()
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
# If you have a function that defines the distance between a model and a dataset, an algorithm that can minimise that distance by modifying the parameters of the model, you can find the best model. The neat thing about this approach is that it will work for any family of models that you can write an equation for.

# lm() doesn’t use optim() but instead takes advantage of the mathematical structure of linear models. Using some connections between geometry, calculus, and linear algebra, lm() actually finds the closest model in a single step, using a sophisticated algorithm. This approach is both faster, and guarantees that there is a global minimum.
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

## Visualizing models

### Predictions

```{r prediction}
# generate an evenly spaced grid of values that covers the region where our data lies. The easiest way to do that is to use modelr::data_grid(). Its first argument is a data frame, and for each subsequent argument it finds the unique variables and then generates all combinations:
grid <- sim1 %>% 
  data_grid(x)
grid <- grid %>% 
  add_predictions(sim1_mod)

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

### Residuals

```{r residual}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)

ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
# the spread of residuals is not that helpful. the average of the residual will always be 0.

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) + # reference line: residual = 0
  geom_point() 
# This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset.
```

## Formulas and model families

```{r formula}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
model_matrix(df, y ~ x1)
# The way that R adds the intercept to the model is just by having a column that is full of ones. By default, R will always add this column. If you don’t want, you need to explicitly drop it with -1.
model_matrix(df, y ~ x1 - 1)
model_matrix(df, y ~ x1 + x2)
```

### Categorical variables

```{r categorical var}
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)
model_matrix(df, response ~ sex) # dummy variables

ggplot(sim2) + 
  geom_point(aes(x, y))
mod2 <- lm(y ~ x, data = sim2)
grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), color = "red", size = 4)

tibble(x = "e") %>% 
  add_predictions(mod2)
```

### Interactions (continuous and categorical)

```{r interaction cont and cat}
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(color = x2))

mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3) # x1 + x2 + x1 * x2

# We have two predictors, so we need to give data_grid() both variables. It finds all the unique values of x1 and x2 and then generates all combinations.
# To generate predictions from both models simultaneously, we can use gather_predictions() which adds each prediction as a row. The complement of gather_predictions() is spread_predictions() which adds each prediction to a new column.
(grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2))
sim3 %>% 
  data_grid(x1, x2) %>% 
  spread_predictions(mod1, mod2)
ggplot(sim3, aes(x1, y, color = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model) # facet_grid(~ model)
# The model that uses + has the same slope for each line, but different intercepts. The model that uses * has a different slope and intercept for each line.

# Which model is better for this data? We can take look at the residuals. Here I’ve facetted by both model and x2 because it makes it easier to see the pattern within each group.
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)
sim3 %>% 
  spread_residuals(mod1, mod2)

ggplot(sim3, aes(x = x1, y = resid, color = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2) # row by model, column by x2
# There is little obvious pattern in the residuals for mod2. The residuals for mod1 show that the model has clearly missed some pattern in b, and less so, but still present is pattern in c, and d. Here, we’re interested in a qualitative assessment of whether or not the model has captured the pattern that we’re interested in.
```

### Interactions (two continuous)

```{r interaction cont and cont}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), 
    x2 = seq_range(x2, 5) 
  ) %>% 
  gather_predictions(mod1, mod2)
grid

# Note the use of seq_range() inside data_grid(). Instead of using every unique value of x, going to use a regularly spaced grid of five values between the minimum and maximum numbers.
# pretty = TRUE will generate a “pretty” sequence, i.e. something that looks nice to the human eye. This is useful if you want to produce tables of output:
seq_range(c(0.0123, 0.923423), n = 5)
seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)
# trim = 0.1 will trim off 10% of the tail values. This is useful if the variables have a long tailed distribution and you want to focus on generating values near the center:
x1 <- rcauchy(100)
seq_range(x1, n = 5)
seq_range(x1, n = 5, trim = 0.05)
seq_range(x1, n = 5, trim = 0.10)
# expand = 0.1 is in some sense the opposite of trim() it expands the range by 10%:
x2 <- c(0, 1)
seq_range(x2, n = 5)
seq_range(x2, n = 5, expand = 0.10)
seq_range(x2, n = 5, expand = 0.25)

ggplot(grid, aes(x1, x2)) + 
  geom_tile(aes(fill = pred)) + 
  facet_wrap(~ model)
ggplot(grid, aes(x1, pred, color = x2, group = x2)) + 
  geom_line() +
  facet_wrap(~ model)
ggplot(grid, aes(x2, pred, color = x1, group = x1)) + 
  geom_line() +
  facet_wrap(~ model)
# An interaction says that there’s not a fixed offset: you need to consider both values of x1 and x2 simultaneously in order to predict y.

sim4 <- sim4 %>% 
  gather_residuals(mod1, mod2)
sim3 %>% 
  spread_residuals(mod1, mod2)

ggplot(sim4, aes(x = x1, y = resid, color = x2)) + 
  geom_point() + 
  facet_grid(~ model)
```

### Transformations

```{r transformation}
# If your transformation involves +, *, ^, or -, you’ll need to wrap it in I() so R doesn’t treat it like part of the model specification. e.g., y ~ x + I(x ^ 2) is translated to y = a_1 + a_2 * x + a_3 * x^2. If you specify y ~ x ^ 2 + x, R will compute y ~ x * x + x. x * x means the interaction of x with itself, which is the same as x. R automatically drops redundant variables so x + x become x, meaning that y ~ x ^ 2 + x specifies the function y = a_1 + a_2 * x. That’s probably not what you intended!
df <- tribble(
  ~y, ~x,
   1,  1,
   2,  2, 
   3,  3
)
model_matrix(df, y ~ x^2 + x)
model_matrix(df, y ~ I(x^2) + x)

# Taylor’s theorem: you can approximate any smooth function with an infinite sum of polynomials. That means you can use a polynomial function to get arbitrarily close to a smooth function by fitting an equation like y = a_1 + a_2 * x + a_3 * x^2 + a_4 * x ^ 3.
model_matrix(df, y ~ poly(x, 2))

# However there’s one major problem with using poly(): outside the range of the data, polynomials rapidly shoot off to positive or negative infinity. One safer alternative is to use the natural spline, splines::ns().
model_matrix(df, y ~ ns(x, 2))

sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)
ggplot(sim5, aes(x, y)) +
  geom_point()

mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)

grid <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y")

ggplot(sim5, aes(x, y)) + 
  geom_point() +
  geom_line(data = grid, color = "red") +
  facet_wrap(~ model)
```

## Missing values

```{r missing}
# R’s default behaviour is to silently drop them, but options(na.action = na.warn) (run in the prerequisites), makes sure you get a warning.
options(na.action = na.warn)
df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3,
  NA, 10
)
mod <- lm(y ~ x, data = df)
# summary(mod): (2 observations deleted due to missingness)

# suppress the warning
mod <- lm(y ~ x, data = df, na.action = na.exclude)

# You can always see exactly how many observations were used with nobs():
nobs(mod)
```

## Other model families

* Linear models assume that the response is continuous and the error has a normal distribution. Linear models assume that the residuals have a normal distribution.

* Generalized linear models, e.g. stats::glm().  Generalized linear models extend linear models to include non-continuous responses (e.g. binary data or counts). They work by defining a distance metric based on the statistical idea of likelihood.

* Generalized additive models, e.g. mgcv::gam(), extend generalized linear models to incorporate arbitrary smooth functions. That means you can write a formula like y ~ s(x) which becomes an equation like y = f(x) and let gam() estimate what that function is (subject to some smoothness constraints to make the problem tractable).

* Penalized linear models, e.g. glmnet::glmnet(), add a penalty term to the distance that penalizes complex models (as defined by the distance between the parameter vector and the origin). This tends to make models that generalize better to new datasets from the same population.

* Robust linear models, e.g. MASS::rlm(), tweak the distance to downweight points that are very far away. This makes them less sensitive to the presence of outliers, at the cost of being not quite as good when there are no outliers.

* Trees, e.g. rpart::rpart(), attack the problem in a completely different way than linear models. They fit a piece-wise constant model, splitting the data into progressively smaller and smaller pieces. Trees aren’t terribly effective by themselves, but they are very powerful when used in aggregate by models like random forests (e.g. randomForest::randomForest()) or gradient boosting machines (e.g. xgboost::xgboost.)

# Model building

## Why are low quality diamonds more expensive?

```{r low quality diamond}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
ggplot(diamonds, aes(color, price)) + geom_boxplot()
ggplot(diamonds, aes(clarity, price)) + geom_boxplot()
```

### Price and carat

```{r carat}
ggplot(diamonds, aes(carat, price)) + 
  geom_hex(bins = 50)

diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))

ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50)

mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

grid <- diamonds2 %>% 
  data_grid(carat = seq_range(carat, 20)) %>% 
  mutate(lcarat = log2(carat)) %>% 
  add_predictions(mod_diamond, "lprice") %>% 
  mutate(price = 2 ^ lprice)

ggplot(diamonds2, aes(carat, price)) + 
  geom_hex(bins = 50) + 
  geom_line(data = grid, color = "red", size = 1)

# Now we can look at the residuals, which verifies that we’ve successfully removed the strong linear pattern:
diamonds2 <- diamonds2 %>% 
  add_residuals(mod_diamond, "lresid")

ggplot(diamonds2, aes(lcarat, lresid)) + 
  geom_hex(bins = 50)

ggplot(diamonds2, aes(cut, lresid)) + geom_boxplot()
ggplot(diamonds2, aes(color, lresid)) + geom_boxplot()
ggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot()
# A residual of -1 indicates that lprice was 1 unit lower than a prediction based solely on its weight. 2^(−1) is 1/2, points with a value of -1 are half the expected price, and residuals with value 1 are twice the predicted price.
```

### A more complicated model

```{r complicated model}
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)

# If the model needs variables that you haven’t explicitly supplied, data_grid() will automatically fill them in with “typical” value. For continuous variables, it uses the median, and categorical variables it uses the most common value (or values, if there’s a tie).
grid <- diamonds2 %>% 
  data_grid(cut, .model = mod_diamond2) %>% 
  add_predictions(mod_diamond2)
grid

ggplot(grid, aes(cut, pred)) + 
  geom_point()

diamonds2 <- diamonds2 %>% 
  add_residuals(mod_diamond2, "lresid2")

ggplot(diamonds2, aes(lcarat, lresid2)) + 
  geom_hex(bins = 50)

# unusual values
diamonds2 %>% 
  filter(abs(lresid2) > 1) %>% # smaller than 1/2x or larger than 2x the price that we expected
  add_predictions(mod_diamond2) %>% 
  mutate(pred = round(2 ^ pred)) %>% 
  select(price, pred, carat:table, x:z) %>% 
  arrange(price)
```

## What affects the number of daily flights?

```{r daily flights}
daily <- flights %>% 
  mutate(date = make_date(year, month, day)) %>% 
  group_by(date) %>% 
  summarise(n = n())
daily

ggplot(daily, aes(date, n)) + 
  geom_line()
```

### Day of week

```{r day of week}
# Understanding the long-term trend is challenging because there’s a very strong day-of-week effect that dominates the subtler patterns. The distribution of flight numbers by day-of-week:
daily <- daily %>% 
  mutate(wday = wday(date, label = TRUE)) # label: Sun = 1, Mon = 2, ...
ggplot(daily, aes(wday, n)) + 
  geom_boxplot()

# predictions
mod <- lm(n ~ wday, data = daily)

grid <- daily %>% 
  data_grid(wday) %>% 
  add_predictions(mod, "n")

ggplot(daily, aes(wday, n)) + 
  geom_boxplot() +
  geom_point(data = grid, color = "red", size = 4)

# residuals: the deviation from the expected number of flights, given the day of week. After removing much of the large day-of-week effect, we can see some of the subtler patterns that remain:
daily <- daily %>% 
  add_residuals(mod)
daily %>% 
  ggplot(aes(date, resid)) + 
  geom_ref_line(h = 0) + 
  geom_line()
daily %>% 
  ggplot(aes(date, resid)) + 
  geom_ref_line(h = 0) + 
  geom_point()
# Our model seems to fail starting in June: you can still see a strong regular pattern that our model hasn’t captured. Our model fails to accurately predict the number of flights on Saturday: during summer there are more flights than we expect, and during Fall there are fewer. 
ggplot(daily, aes(date, resid, colour = wday)) + 
  geom_ref_line(h = 0) + 
  geom_line()
# There are some days with far fewer flights than expected:
daily %>% 
  filter(resid < -100) # new year, national, thanksgiving, christmas
# There seems to be some smoother long term trend over the course of a year. We can highlight that trend with geom_smooth(): There are fewer flights in January (and December), and more in summer (May-Sep).
daily %>% 
  ggplot(aes(date, resid)) + 
  geom_ref_line(h = 0) + 
  geom_line(color = "grey50") + 
  geom_smooth(se = FALSE, span = 0.20)
```

### Seasonal Saturday effect

```{r seasonal saturday}
daily %>% 
  filter(wday == "Sat") %>% 
  ggplot(aes(date, n)) + 
    geom_point() + 
    geom_line() +
    scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")

# Lets create a “term” variable that roughly captures the three school terms.
term <- function(date) {
  cut(date, 
    breaks = ymd(20130101, 20130605, 20130825, 20140101),
    labels = c("spring", "summer", "fall") 
  )
}

daily <- daily %>% 
  mutate(term = term(date)) 

daily %>% 
  filter(wday == "Sat") %>% 
  ggplot(aes(date, n, color = term)) +
  geom_point(alpha = 1/3) + 
  geom_line() +
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")

# how this new variable affects the other days of the week:
daily %>% 
  ggplot(aes(wday, n, colour = term)) +
    geom_boxplot()

# It looks like there is significant variation across the terms, so fitting a separate day of week effect for each term is reasonable. 
mod1 <- lm(n ~ wday, data = daily)
mod2 <- lm(n ~ wday * term, data = daily)

daily %>% 
  gather_residuals(without_term = mod1, with_term = mod2) %>% 
  ggplot(aes(date, resid, color = model)) +
  geom_line(alpha = 0.75) +
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")

grid <- daily %>% 
  data_grid(wday, term) %>% 
  add_predictions(mod2, "n")

# Our model is finding the mean effect, but we have a lot of big outliers, so mean tends to be far away from the typical value. 
ggplot(daily, aes(wday, n)) +
  geom_boxplot() + 
  geom_point(data = grid, color = "red") + 
  facet_wrap(~ term)

# We can alleviate this problem by using a model that is robust to the effect of outliers: MASS::rlm(). This greatly reduces the impact of the outliers on our estimates, and gives a model that does a good job of removing the day of week pattern:
mod3 <- MASS::rlm(n ~ wday * term, data = daily)
# Fit a linear model by robust regression using an M estimator. Fitting is done by iterated re-weighted least squares (IWLS). The idea is to weigh the observations differently based on how well behaved these observations are. In Huber weighting, observations with small residuals get a weight of 1 and the larger the residual, the smaller the weight. With bisquare weighting, all cases with a non-zero residual get down-weighted at least a little. 
# https://stats.idre.ucla.edu/r/dae/robust-regression/

daily %>% 
  add_residuals(mod3, "resid") %>% 
  ggplot(aes(date, resid)) + 
  geom_hline(yintercept = 0, size = 2, color = "white") + # reference line: resid (y) = 0
  geom_line() +
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")
```

### Computed variables

```{r computed var}
# it’s a good idea to bundle the creation of variables up into a function so there’s no chance of accidentally applying a different transformation in different places. e.g., 
compute_vars <- function(data) {
  data %>% 
    mutate(
      term = term(date), 
      wday = wday(date, label = TRUE)
    )
}
# Another option is to put the transformations directly in the model formula:

wday2 <- function(x) wday(x, label = TRUE)
mod3 <- lm(n ~ wday2(date) * term(date), data = daily)
```

### Time of year: an alternative approach

```{r time of year}
# An alternative to using domain knowledge explicitly in the model is to use a more flexible model. A simple linear trend isn’t adequate, so we could try using a natural spline to fit a smooth curve across the year:
mod <- MASS::rlm(n ~ wday * ns(date, 5), data = daily)
# natural spline: 

daily %>% 
  data_grid(wday, date = seq_range(date, n = 13)) %>% 
  add_predictions(mod) %>% 
  ggplot(aes(date, pred, color = wday)) + 
  geom_line() +
  geom_point() +
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")
# We see a strong pattern in the numbers of Saturday flights. We also saw that pattern in the raw data. It’s a good sign when you get the same signal from different approaches.
```

# Many models

## gapminder

```{r gapminder}
gapminder %>% 
  ggplot(aes(x = year, y = lifeExp, group = country)) +
    geom_line(alpha = 1/3)

nz <- filter(gapminder, country == "New Zealand")
nz %>% 
  ggplot(aes(year, lifeExp)) + 
  geom_line() + 
  ggtitle("Full data (New Zealand) = ")

nz_mod <- lm(lifeExp ~ year, data = nz)
nz %>% 
  add_predictions(nz_mod) %>%
  ggplot(aes(year, pred)) + 
  geom_line() + 
  ggtitle("Linear trend + ")

nz %>% 
  add_residuals(nz_mod) %>% 
  ggplot(aes(year, resid)) + 
  geom_hline(yintercept = 0, color = "white", size = 3) + 
  geom_line() + 
  ggtitle("Remaining pattern")
```

### Nested data 

```{r nested}
# group on both continent and country: given country, continent is fixed, so this doesn’t add any more groups, but it’s an easy way to carry an extra variable along for the ride.
by_country <- gapminder %>% 
  group_by(country, continent) %>% 
  nest()

by_country
by_country$data[1]
by_country$data[[1]]
# in a grouped data frame, each row is an observation; in a nested data frame, each row is a group. Another way to think about a nested dataset is we now have a meta-observation: a row that represents the complete time course for a country, rather than a single point in time.
```

### List-columns

```{r list-column}
country_model <- function(df) {
  lm(lifeExp ~ year, data = df)
}

# instead of creating a new object in the global environment, we’re going to create a new variable in the by_country data frame. 
by_country <- by_country %>% 
  mutate(model = purrr::map(data, country_model))
by_country$model[[1]]

by_country %>% 
  filter(continent == "Europe")
by_country %>% 
  arrange(continent, country)
```

### Unnesting

```{r unnest}
by_country <- by_country %>% 
  mutate(resids = map2(data, model, add_residuals))
by_country$resids[[1]]

resids <- unnest(by_country, resids)
resids
# Note that each regular column is repeated one for each row in the nested column.

resids %>% 
  ggplot(aes(year, resid)) +
    geom_line(aes(group = country), alpha = 1 / 3) + 
    geom_smooth(se = FALSE)

resids %>% 
  ggplot(aes(year, resid, group = country)) +
  geom_line(alpha = 1 / 3) + 
  facet_wrap(~continent)
```

### Model quality

```{r model quality}
# Instead of looking at the residuals from the model, we could look at some general measurements of model quality. to extract some model quality metrics:
broom::glance(nz_mod)

by_country %>% 
  mutate(glance = purrr::map(model, broom::glance)) %>% 
  unnest(glance)

# This isn’t quite the output we want, because it still includes all the list columns. This is default behaviour when unnest() works on single row data frames. To suppress these columns we use .drop = TRUE. ?
glance <- by_country %>% 
  mutate(glance = purrr::map(model, broom::glance)) %>% 
  unnest(glance, .drop = TRUE)
glance

glance %>% 
  arrange(r.squared)

# Here we have a relatively small number of observations and a discrete variable, so geom_jitter() is effective:
glance %>% 
  ggplot(aes(continent, r.squared)) + 
    geom_jitter(width = 0.5)

bad_fit <- filter(glance, r.squared < 0.25)

gapminder %>% 
  semi_join(bad_fit, by = "country") %>% 
  ggplot(aes(year, lifeExp, color = country)) +
  geom_line()
# We see two main effects here: the tragedies of the HIV/AIDS epidemic and the Rwandan genocide.
```

## List columns

```{r list column}
# data.frame() treats a list as a list of columns
data.frame(x = list(1:3, 3:5))

# prevent data.frame() from doing this with I()
data.frame(
  x = I(list(1:3, 3:5)), 
  y = c("1, 2", "3, 4, 5")
)

# Tibble alleviates this problem by being lazier (i.e., tibble() doesn’t modify its inputs) and by providing a better print method:
tibble(
  x = list(1:3, 3:5), 
  y = c("1, 2", "3, 4, 5")
)

# tribble() can automatically work out that you need a list:
tribble(
   ~x, ~y,
  1:3, "1, 2",
  3:5, "3, 4, 5"
)

# List-columns are often most useful as intermediate data structure. Generally there are three parts of an effective list-column pipeline:
# You create the list-column using one of nest(), summarise() + list(), or mutate() + a map function, as described in Creating list-columns.
# You create other intermediate list-columns by transforming existing list columns with map(), map2() or pmap(). For example, in the case study above, we created a list-column of models by transforming a list-column of data frames.
# You simplify the list-column back down to a data frame or atomic vector, as described in Simplifying list-columns.
```

## Creating list-columns

1. With tidyr::nest() to convert a grouped data frame into a nested data frame where you have list-column of data frames.

2. With mutate() and vectorized functions that return a list.

3. With summarise() and summary functions that return multiple results.

Alternatively, you might create them from a named list, using tibble::enframe().

Generally, when creating list-columns, you should make sure they’re homogeneous: each element should contain the same type of thing.

### With nesting

```{r nest}
# In a nested data frame each row is a meta-observation: the other columns give variables that define the observation, and the list-column of data frames gives the individual observations that make up the meta-observation.

# When applied to a grouped data frame, nest() keeps the grouping columns as is, and bundles everything else into the list-column:
gapminder %>% 
  group_by(country, continent) %>% 
  nest()
# You can also use it on an ungrouped data frame, specifying which columns you want to nest:
gapminder %>% 
  nest(year:gdpPercap) # what kept in the nested list-column
```

### From vectorized functions

```{r vectorized func}
df <- tribble(
  ~x1,
  "a,b,c", 
  "d,e,f,g"
) 

df %>% 
  mutate(x2 = str_split(x1, ",")) 

df %>% 
  mutate(x2 = str_split(x1, ",")) %>% 
  unnest() # unnest(x2)
# If you find yourself using this pattern a lot, make sure to check out tidyr::separate_rows() which is a wrapper around this common pattern.

sim <- tribble(
  ~f,      ~params,
  "runif", list(min = -1, max = 1),
  "rnorm", list(sd = 5),
  "rpois", list(lambda = 10)
)

sim %>%
  mutate(sims = invoke_map(f, params, n = 10))
```

### From multivalued summaries

```{r multivalued summary}
# One restriction of summarise() is that it only works with summary functions that return a single value. That means that you can’t use it with functions like quantile() that return a vector of arbitrary length:
mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = quantile(mpg))
# wrap the result in a list:
mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg)))

probs <- c(0.01, 0.25, 0.5, 0.75, 0.99)
mtcars %>% 
  group_by(cyl) %>% 
  summarise(p = list(probs), q = list(quantile(mpg, probs))) %>% 
  unnest() # unnest(p, q), or unnest(c(p, q))
```

### From a named list

```{r named list}
x <- list(
  a = 1:5,
  b = 3:4, 
  c = 5:6
) 

df <- enframe(x)
df

# Now if you want to iterate over names and values in parallel, you can use map2():
df %>% 
  mutate(
    smry = map2_chr(name, value, ~ str_c(.x, ": ", .y[1]))
  )

mtcars %>% 
  group_by(cyl) %>% 
  summarise_each(funs(mean, median))
mtcars %>% 
  group_by(cyl) %>% 
  summarise_each(funs(list))
?summarise_each
```

## Simplifying list-columns

### List to vector

```{r list to vector}
df <- tribble(
  ~x,
  letters[1:5],
  1:3,
  runif(5)
)
  
df %>% mutate(
  type = map_chr(x, typeof),
  length = map_int(x, length)
)

# use map_chr(x, "apple") to extract the string stored in apple for each element of x. This is useful for pulling apart nested lists into regular columns. Use the .null argument to provide a value to use if the element is missing (instead of returning NULL):
df <- tribble(
  ~x,
  list(a = 1, b = 2),
  list(a = 2, c = 4)
)

df %>% mutate(
  a = map_dbl(x, "a"),
  b = map_dbl(x, "b", .null = NA_real_)
)
```

### Unnesting

```{r unnest}
# unnest() works by repeating the regular columns once for each element of the list-column.
tibble(x = 1:2, y = list(1:4, 1)) %>% unnest(y)

# This means that you can’t simultaneously unnest two columns that contain different numbers of elements:
df1 <- tribble(
  ~x, ~y,           ~z,
   1, c("a", "b"), 1:2,
   2, "c",           3
)
df1 %>% 
  unnest(y, z)

df2 <- tribble(
  ~x, ~y,           ~z,
   1, "a",         1:2,  
   2, c("b", "c"),   3
)
df2 %>% 
  unnest(y, z)
```

## Making tidy data with broom

1. broom::glance(model) returns a row for each model. Each column gives a model summary: either a measure of model quality, or complexity, or a combination of the two.

2. broom::tidy(model) returns a row for each coefficient in the model. Each column gives information about the estimate or its variability.

3. broom::augment(model, data) returns a row for each row in data, adding extra values like residuals, and influence statistics.

**Communicate**

# R Markdown

*R Markdown Cheat Sheet: Help > Cheatsheets > R Markdown Cheat Sheet,

*R Markdown Reference Guide: Help > Cheatsheets > R Markdown Reference Guide.

Both cheatsheets are also available at http://rstudio.com/cheatsheets.

## Code chunks

### Table

```{r kable}
knitr::kable(
  mtcars[1:5, ], 
  caption = "A knitr kable."
)

# For even deeper customisation, consider the xtable, stargazer, pander, tables, and ascii packages. Each provides a set of tools for returning formatted tables from R code.
```

### Caching

cache = TRUE: When set, this will save the output of the chunk to a specially named file on disk. On subsequent runs, knitr will check to see if the code has changed, and if it hasn’t, it will reuse the cached results.

The caching system must be used with care, because by default it is based on the code only, not its dependencies. For example, here the processed_data chunk depends on the raw_data chunk:

```{r raw_data}
rawdata <- readr::read_csv("a_very_large_file.csv")
```

```{r processed_data, cache = TRUE}
processed_data <- rawdata %>% 
  filter(!is.na(import_var)) %>% 
  mutate(new_variable = complicated_transformation(x, y, z))
```

Caching the processed_data chunk means that it will get re-run if the dplyr pipeline is changed, but it won’t get rerun if the read_csv() call changes. You can avoid that problem with the dependson chunk option:

```{r processed_data, cache = TRUE, dependson = "raw_data"}
processed_data <- rawdata %>% 
  filter(!is.na(import_var)) %>% 
  mutate(new_variable = complicated_transformation(x, y, z))
```

dependson should contain a character vector of every chunk that the cached chunk depends on. Knitr will update the results for the cached chunk whenever it detects that one of its dependencies have changed.

Note that the chunks won’t update if a_very_large_file.csv changes, because knitr caching only tracks changes within the .Rmd file. If you want to also track changes to that file you can use the cache.extra option. This is an arbitrary R expression that will invalidate the cache whenever it changes. A good function to use is file.info(): it returns a bunch of information about the file including when it was last modified. Then you can write:

```{r raw_data, cache.extra = file.info("a_very_large_file.csv")}
rawdata <- readr::read_csv("a_very_large_file.csv")
```

As your caching strategies get progressively more complicated, it’s a good idea to regularly clear out all your caches with knitr::clean_cache().

### Inline code

```{r inline code}
# There is one other way to embed R code into an R Markdown document: directly into the text, with: `r `. This can be very useful if you mention properties of your data in the text. 

# When inserting numbers into text, format() is your friend. It allows you to set the number of digits so you don’t print to a ridiculous degree of accuracy, and a big.mark to make numbers easier to read. I’ll often combine these into a helper function:
comma <- function(x){
  format(x, digits = 2, big.mark = ",")
}
comma(3452345)
comma(.12358124331)
```

## YAML header

### Parameters

R Markdown documents can include one or more parameters whose values can be set when you render the report. Parameters are useful when you want to re-render the same report with distinct values for various key inputs. For example, you might be producing sales reports per branch, exam results by student, or demographic summaries by country. To declare one or more parameters, use the params field.

This example uses a my_class parameter to determine which class of cars to display:

---
output: html_document
params:
  my_class: "suv"
---

```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)

class <- mpg %>% filter(class == params$my_class)
```

# Fuel economy for `r params$my_class`s

```{r, message = FALSE}
ggplot(class, aes(displ, hwy)) + 
  geom_point() + 
  geom_smooth(se = FALSE)
```

# Graphics for communication

## Label

```{r label}
# The purpose of a plot title is to summarise the main finding. Avoid titles that just describe what the plot is, e.g. “A scatterplot of engine displacement vs. fuel economy”. hhh
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight", # subtitle adds additional detail in a smaller font beneath the title.
    caption = "Data from fueleconomy.gov" # caption adds text at the bottom right of the plot, often used to describe the source of the data.
  )

# You can also use labs() to replace the axis and legend titles. It’s usually a good idea to replace short variable names with more detailed descriptions, and to include the units.
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    color = "Car type"
  )

# It’s possible to use mathematical equations instead of text strings. Just switch "" out for quote() and read about the available options in ?plotmath:
df <- tibble(
  x = runif(10),
  y = runif(10)
)
ggplot(df, aes(x, y)) +
  geom_point() +
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  )
?plotmath
demo(plotmath)
```

## Annotations

```{r annotation}
# 1. you might have a tibble that provides labels. The plot below illustrates a useful approach: pull out the most efficient car in each class with dplyr, and then label it on the plot:
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1) # 1st in each class

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_text(aes(label = model), data = best_in_class)

# This is hard to read because the labels overlap with each other, and with the points. We can make things a little better by switching to geom_label() which draws a rectangle behind the text. We also use the nudge_y parameter to move the labels slightly above the corresponding points:
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)

#  we can use the ggrepel package by Kamil Slowikowski. This useful package will automatically adjust labels so that they don’t overlap:
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_point(size = 3, shape = 1, data = best_in_class) + # add a second layer of large, hollow points to highlight the labelled points
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)

# You can sometimes use the same idea to replace the legend with labels placed directly on the plot. theme(legend.position = "none") turns the legend off
class_avg <- mpg %>%
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )

ggplot(mpg, aes(displ, hwy, color = class)) + # color as global aes
  ggrepel::geom_label_repel(aes(label = class),
    data = class_avg,
    size = 6, # font size
    label.size = 0, # boundary width of rectangle
    segment.color = NA
  ) +
  geom_point() +
  theme(legend.position = "none")

# Alternatively, you might just want to add a single label to the plot, but you’ll still need to create a data frame.
# Often, you want the label in the corner of the plot, so it’s convenient to create a new data frame using summarise() to compute the maximum values of x and y.
label <- mpg %>%
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right") # "top" "center" "bottom" "left" "center" "right"

# If you want to place the text exactly on the borders of the plot, you can use +Inf and -Inf. Since we’re no longer computing the positions from mpg, we can use tibble() to create the data frame:
label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")

# use stringr::str_wrap() to automatically add line breaks, given the number of characters you want per line:
"Increasing engine size is related to decreasing fuel economy." %>%
  stringr::str_wrap(width = 20) %>%
  writeLines()
"Increasing engine size is related to decreasing fuel economy." %>%
  stringr::str_wrap(width = 40) %>%
  writeLines()

# Use geom_hline() and geom_vline() to add reference lines. make them thick (size = 2) and white (colour = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.
# Use geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax.
# Use geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.
```

