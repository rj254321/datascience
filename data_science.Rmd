---
title: "visualization"
author: "rj2543"
date: "2/29/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(maps)
library(mapproj)
library(nycflights13)
library(lvplot)
library(hexbin)
library(modelr)
library(hms)
```

```{r data load}
data(mpg)
mpg
```

## Aesthetic

```{r aes}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
  
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy), color = "blue")
# mpg %>% 
#   ggplot() + 
#   geom_point(aes(x = displ, y = hwy, shape = cty))
# Error: A continuous variable can not be mapped to shape
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, size = cty))
mpg %>% 
  ggplot() +
  geom_point(aes(x = displ, y = hwy, color = cty, size = cty))
mpg %>% 
  ggplot() + 
  geom_point(aes(x = displ, y = hwy, colour = displ < 5))
# ?geom_point
# vignette("ggplot2-specs")
# The "munsell" package makes it easy to specific colours using a system designed by Alfred Munsell. If you invest a little in learning the system, it provides a convenient way of specifying aesthetically pleasing colours.
```

## Facets

One way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.

```{r facet}
# To facet your plot by a single variable, use facet_wrap(). The variable that you pass to facet_wrap() should be discrete.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 3)
# To facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(. ~ cyl)
mpg %>% 
  ggplot() +
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ .)
# When using facet_grid() you should usually put the variable with more unique levels in the columns.
```

## Geometric objects

```{r geom}
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
# In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.
mpg %>% 
  ggplot() + 
  geom_smooth(mapping = aes(x = displ, y = hwy, color = drv),
              show.legend = F)
# If you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
```

## Statistical transformation

```{r trans}
# You can generally use geoms and stats interchangeably. For example, you can recreate the plot using stat_count() instead of geom_bar().
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
# You might want to override the default stat. In the code below, I change the stat of geom_bar() from count (the default) to identity. This lets me map the height of the bars to the raw values of a y variable. Unfortunately when people talk about bar charts casually, they might be referring to this type of bar chart, where the height of the bar is already present in the data, or the previous bar chart where the height of the bar is generated by counting rows.
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)
demo %>% 
  mutate(cut = factor(cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))) %>% 
  ggplot() +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
# You might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportion, rather than count.
# To find the variables computed by the stat, look for the help section titled “computed variables”.
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
# You might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarises the y values for each unique x value, to draw attention to the summary that you’re computing.
# ggplot2 provides over 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g. ?stat_bin. To see a complete list of stats, try the ggplot2 cheatsheet.
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )
?stat_summary
?geom_col # geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col() instead. geom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.
?stat_smooth
?geom_bar
vignette("ggplot2-specs")
```

## Position adjustment

```{r position}
# if you map the "fill" aesthetic to another variable, like "clarity": the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
# position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
# position = "dodge" places overlapping objects directly beside one another. This makes it easier to compare individual values.
diamonds %>% 
  ggplot() + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
# position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.
mpg %>% 
  ggplot() + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
mpg %>% 
  ggplot() +
  geom_jitter(mapping = aes(x = displ, y = hwy))
?geom_count #counts the number of observations at each location, then maps the count to point area. It useful when you have discrete data and overplotting.
mpg %>% 
  ggplot() +
  geom_count(mapping = aes(x = displ, y = hwy))
mpg %>% 
  ggplot() +
  geom_boxplot(mapping = aes(y = displ, x = trans, fill = trans), show.legend = F, position = "dodge")
# the default position adjustment for geom_boxplot() is "dodge"
```

## Coordinate systems

```{r coord}
# coord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
# coord_quickmap() sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2.
nz <- map_data("nz")
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_map()
# coord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  #labs(x = NULL, y = NULL) + 
  coord_flip()
ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)+ 
  coord_polar()
# coord_map projects a portion of the earth, which is approximately spherical, onto a flat 2D plane using any projection defined by the mapproj package. Map projections do not, in general, preserve straight lines, so this requires considerable computation. coord_quickmap is a quick approximation that does preserve straight lines. It works best for smaller areas closer to the equator.
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

## Layered grammar of graphics

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>

## Work flow

Don’t be lazy and use =: it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). 

```{r}
# This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen.
(y <- seq(1, 10, length.out = 5))
library(tidyverse)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
# Press Alt + Shift + K. Keyboard Shortcut Quick Reference
```

## Data Transformation

```{r filter}
(dec25 <- flights %>% 
   filter(month == 12, day == 25))
# floating point numbers: Computers use finite precision arithmetic, so remember that every number you see is an approximation.
sqrt(2)^2 == 2
near(sqrt(2) ^ 2,  2)
1/49*49 == 1
near(1 / 49 * 49, 1)
# almost any operation involving an unknown value will also be unknown
# 1. Had an arrival delay of two or more hours
flights %>% 
  filter(arr_delay >= 120)
# 2. Flew to Houston (IAH or HOU)
flights %>% 
  filter(dest %in% c("IAH", "HOU"))
# 3. Were operated by United, American, or Delta
flights %>% 
  filter(carrier %in% c("UA", "AA", "DL"))
# 4. Departed in summer (July, August, and September)
flights %>% 
  filter(month %in% c(7, 8, 9))
flights %>% 
  filter(between(month, 7, 9))
# 5. Arrived more than two hours late, but didn’t leave late
flights %>% 
  filter(arr_delay > 120 & dep_delay <= 0)
# 6. Were delayed by at least an hour, but made up over 30 minutes in flight
flights %>% 
  filter(dep_delay >= 60 & (dep_delay - arr_delay) >= 30)
# 7. Departed between midnight and 6am (inclusive)
flights %>% 
  filter(dep_time == 2400 | dep_time <= 0600)
flights %>% 
  filter(between(dep_time, 0, 600) | dep_time == 2400)
?between # This is a shortcut for x >= left & x <= right, between(x, left, right)
flights %>% 
  filter(is.na(dep_time)) # canceled flights
NA ^ 0 # = 1
NA | TRUE # TRUE
FALSE & NA # FALSE
NA * 0 # NA
```

```{r arrange}
# Missing values are always sorted at the end.
```

```{r select}
flights %>% 
  select(-(year:day))
# starts_with("abc"): matches names that begin with “abc”.
# ends_with("xyz"): matches names that end with “xyz”.
# contains("ijk"): matches names that contain “ijk”.
# matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. 
# num_range("x", 1:3): matches x1, x2 and x3.
# one_of(): Matches variable names in a character vector.
# everything(): if you have a handful of variables you’d like to move to the start of the data frame.
# last_col(): Select last variable, possibly with an offset.
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
flights %>% 
  select(one_of(vars))
flights %>% 
  select(contains("TIME")) # default in the contains() helper is: ignore.case = TRUE
flights %>% 
  select(contains("TIME", ignore.case = FALSE))
```

```{r mutate}
# you can refer to columns that you’ve just created
# If you only want to keep the new variables, use transmute():
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
# Modular arithmetic: %/% (integer division) and %% (remainder), where x == y * (x %/% y) + (x %% y).
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
# Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)). They are most useful in conjunction with group_by().
(x <- 1:10)
lag(x) # previous
lead(x) # next
# Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); and dplyr provides cummean() for cumulative means. If you need rolling aggregates (i.e. a sum computed over a rolling window), try the RcppRoll package.
# what is "a rolling window"?
cumsum(x)
cummean(x)
# Ranking: min_rank() does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks.
y <- c(1, 2, 2, NA, 3, 4)
min_rank(y)
min_rank(desc(y))
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
# ntile(y)
flights %>% 
  transmute(air_time, arr_time - dep_time)
flights %>% 
  transmute(air_time, arr_time - dep_time, (arr_time %/% 100 * 60 + arr_time %% 100) - (dep_time %/% 100 * 60 + dep_time %% 100))
```

```{r summarise}
# grouped summaries: It collapses a data frame to a single row
# Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data.
# median absolute deviation: mad(x)
# Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)]
# To count the number of distinct (unique) values, use n_distinct(x).

# When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
((per_year   <- summarise(per_month, flights = sum(flights))))

flights %>% group_by(carrier, dest) %>% summarise(n())

count(sort = TRUE)

vignette("window-functions")
# A window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The output of a window function depends on all its input values, so window functions don’t include functions that work element-wise, like + or round(). Window functions include variations on aggregate functions, like cumsum() and cummean(), functions for ranking and ordering, like rank(), and functions for taking offsets, like lead() and lag().
# lead() and lag() have an optional argument order_by. If set, instead of using the row order to determine which value comes before another, they will use another variable. This is important if you have not already sorted the data, or you want to sort one way and lag another.
```

# Exploratory Data Analysis

## Variation

```{r variation}
diamonds %>% 
  count(cut_width(carat, 0.5))

smaller <- diamonds %>% 
  filter(carat < 3)
smaller %>% 
  ggplot(aes(x = carat, color = cut)) +
  geom_freqpoly(binwidth = 0.1)

smaller %>% 
  ggplot(aes(x = carat)) + 
  geom_histogram(binwidth = 0.01)

# outlier: To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian()
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) + # count:0 - 12000
  coord_cartesian(ylim = c(0, 50))
# It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.
```

## Missing values

```{r missing value}
# Instead of dropping the entire observation/row with the strange values, replace the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA:
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))

flights %>% 
  transmute(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
    geom_histogram(mapping = aes(fill = cancelled), binwidth = 1/4)

flights %>% 
  transmute(cancelled = is.na(dep_time),
            sched_dep_time = (sched_dep_time %/% 100) + (sched_dep_time %% 100)/60) %>% 
  ggplot(mapping = aes(x = sched_dep_time)) + 
  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 1/4)
```

## Covariation

If variation describes the behavior within a variable, covariation describes the behavior between variables. 
Covariation is the tendency for the values of two or more variables to vary together in a related way. 
The best way to spot covariation is to visualise the relationship between two or more variables.

```{r categorical vs continuous}
# Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one.
diamonds %>% 
  ggplot(mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(color = cut), binwidth = 500)

# IQR. In the middle of the box is a line that displays the median. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.
# Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.
# A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.

ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +
  coord_flip() # if variable names are long

# For large datasets (10,000 - 100,000), the letter-value box plot addresses both these shortcomings: it conveys more detailed information in the tails using letter values, only out to the depths where the letter values are reliable estimates of their corresponding quantiles (corresponding to tail areas of roughly 2^{-i}); “outliers” are defined as a function of the most extreme letter value shown.
diamonds %>% 
  ggplot(mapping = aes(x = cut, y = price)) + 
  geom_lv(mapping = aes(fill = cut))

ggplot(data = mpg) +
  geom_violin(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class))

ggplot(data = mpg) +
  geom_jitter(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy, color = class), alpha = 0.4)
```

```{r categorical vs categorical}
# To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination.
# Covariation will appear as a strong correlation between specific x values and specific y values.
diamonds %>% 
  ggplot() +
  geom_count(mapping = aes(x = cut, y = color))

diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = n))

flights %>% 
  mutate(month = factor(month)) %>%
  group_by(dest, month) %>% 
  summarise(mean_delay = mean(arr_delay)) %>% 
  ggplot(mapping = aes(x = dest, y = month)) +
  geom_tile(mapping = aes(fill = mean_delay), na.rm = T)
```

```{r continuous vs continuous}
# geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().
ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))

# Another option is to bin one continuous variable so it acts like a categorical variable.
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)), varwidth = T) # divide carat into bins of width = 0.1; make the width of the boxplot proportional to the number of points

ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20))) # display approximately the same number of points in each bin

# Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) # zoom
```

## Patterns and models

```{r patterns and models}
#  The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

# Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

# Tibbles

```{r tibble}
vignette("tibble")

as_tibble()

# tibble() will automatically recycle inputs of length 1, and allows you to refer to variables that you just created.
tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)

# transposed tibble: tribble() is customised for data entry in code: column headings are defined by formulas (i.e. they start with ~), and entries are separated by commas. This makes it possible to lay out small amounts of data in easy-to-read form.
tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 2, 3.6,
  "b", 1, 8.5
)

# First, you can explicitly print() the data frame and control the number of rows (n) and the width of the display. width = Inf will display all columns:
flights %>% 
  print(n = 10, width = Inf)

?tibble::enframe() # convert named atomic vectors or lists to one- or two-column data frames
```

# Data import

```{r intro}
# read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter.

# Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = "#" to drop all lines that start with (e.g.) #.
read_csv(
  "# The first line of metadata
  # The second line of metadata
  x,y,z
  1,2,3", 
  comment = "#")

# na: this specifies the value (or values) that are used to represent missing values in your file
read_csv("a, b, c \n 1, 2, .", na = ".")
```

```{r parse}
# parse_*() functions take a character vector and return a more specialised vector like a logical, integer, or date
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))

parse_double("1,23", locale = locale(decimal_mark = ","))

# parse_number() ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text.
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45.")
parse_number("$123,456,789") # ignore the grouping mark

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)

# Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors.
challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
# This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
type_convert(df)

# If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats.

# write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS.

# The feather package implements a fast binary file format that can be shared across programming languages. " .feather"

# library(haven) reads SPSS, Stata, and SAS files.
# library(readxl) reads excel files (both .xls and .xlsx).
# library(DBI), along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.
# For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. 
```

# Tidy data

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

## Pivoting

```{r pivot}
# pivot_longer() makes datasets longer by increasing the number of rows and decreasing the number of columns. pivot_longer() makes wide tables narrower and longer
table4a = table4a %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "cases")
table4b = table4b %>% 
  pivot_longer(c("1999", "2000"), names_to = "year", values_to = "population")
left_join(table4a, table4b)

# pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows. pivot_wider() makes long tables shorter and wider
stocks <- tibble(
  year   = c(2015, 2015, 2016, 2016),
  half  = c(   1,    2,     1,    2),
  return = c(1.88, 0.59, 0.92, 0.17)
)
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer("2015":"2016", names_to = "year", values_to = "return", names_ptype = list(year = double()))
```

## Separating and uniting

```{r separate}
# it leaves the type of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better types using convert = TRUE.
table3 %>% 
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)

# You can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into.
table3 %>% 
  separate(year, into = c("century", "year"), sep = 2)
table3 %>% 
  separate(year, into = c("century", "year"), sep = -2)
```

```{r unite}
# In this case we also need to use the sep argument. The default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use ""
table5 %>% 
  unite(new, century, year)
table5 %>% 
  unite(new, century, year, sep = "")
```

## Missing values

```{r missing}
(stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
))

# The way that a dataset is represented can make implicit values explicit. 
stocks %>% 
  pivot_wider(names_from = year, values_from = return)

# set values_drop_na = TRUE in pivot_longer() to turn explicit missing values implicit
stocks %>% 
  pivot_wider(names_from = year, values_from = return) %>% 
  pivot_longer(
    cols = c("2015", "2016"), 
    names_to = "year", 
    values_to = "return", 
    values_drop_na = TRUE
  )

# complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary.
stocks %>% 
  complete(year, qtr)

# There’s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward.
(treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
))
# You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward, LOCF).
treatment %>% 
  fill(person)
```

## Case study

```{r case}
(who_tidy = who %>% 
  pivot_longer(
    cols = new_sp_m014:newrel_f65, 
    names_to = "key", 
    values_to = "cases", 
    values_drop_na = TRUE
  ) %>% 
  mutate(key = str_replace(key, "newrel", "new_rel")) %>% 
  separate(key, c("new", "type", "sexage"), sep = "_") %>% 
  select(-iso2, -iso3, -new) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>% 
  mutate(age_group = case_when(
    age == "014" ~ "0-14",
    age == "1524" ~ "15-24",
    age == "2534" ~ "25-34", 
    age == "3544" ~ "35-44",
    age == "4554" ~ "45-54",
    age == "5564" ~ "55-64",
    age == "65" ~ "65+",
    TRUE ~ NA_character_
  )) %>% 
  select(-age))

who_tidy %>% 
  group_by(country, year, sex) %>% 
  summarise(total_cases = sum(cases)) %>% 
  mutate(sex = ifelse(sex == "f", "female", "male")) %>% 
  ggplot(mapping = aes(x = year, y = total_cases, fill = sex)) +
  geom_col() +
  coord_cartesian(xlim = c(1995, 2013))
```

# Relational data

## Keys

* A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table.

* A foreign key uniquely identifies an observation in another table. For example, flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane.

If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key.

A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines.

```{r key}
flights %>% 
  mutate(surrogate = row_number()) %>% 
  select(surrogate, everything())
```

## Mutating joins

```{r mutating join}
(flights2 <- flights %>% 
  select(year:day, hour, origin, dest, tailnum, carrier))

flights2 %>%
  select(-origin, -dest) %>% 
  left_join(airlines, by = "carrier")

# full_join()
```

```{r duplicate keys}
# One table has duplicate keys: This is useful when you want to add in additional information as there is typically a one-to-many relationship. Here, "key" is a primary key in y and a foreign key in x.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     1, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2"
))
left_join(x, y, by = "key")

# Both tables have duplicate keys: This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product.
(x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     2, "x3",
     3, "x4"
))
(y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     2, "y3",
     3, "y4"
))
left_join(x, y, by = "key")
```

```{r define key columns}
# The default, by = NULL, uses all variables that appear in both tables, the so called natural join. e.g., the flights and weather tables match on their common variables: year, month, day, hour and origin.
flights2 %>% 
  left_join(weather) # Joining, by = c("year", "month", "day", "hour", "origin")

# A character vector, by = "x". This is like a natural join, but uses only some of the common variables. e.g., flights and planes have year variables, but they mean different things so we only want to join by tailnum.
flights2 %>% 
  left_join(planes, by = "tailnum") # year.x, year.y

# A named character vector: by = c("a" = "b"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. e.g., if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to.
flights2 %>% 
  left_join(airports, by = c("dest" = "faa"))
flights2 %>% 
  left_join(airports, by = c("origin" = "faa"))
```

```{r exercise}
(flights_avgdelay = flights %>% 
  group_by(dest) %>% 
  summarise(avgdelay = mean(arr_delay, na.rm = T)))

airports %>%
  inner_join(flights_avgdelay, by = c("faa" = "dest")) %>%
  ggplot(mapping = aes(x = lon, y = lat)) +
    borders("state") +
    geom_point(mapping = aes(size = avgdelay, color = avgdelay, alpha = 0.5)) +
    coord_quickmap()

# Joining different variables between the tables, e.g. inner_join(x, y, by = c("a" = "b")) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins) ?
```

## Filtering joins

```{r filtering join}
# Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables.

# semi_join(x, y) keeps all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows.
(top_desc <- flights %>% 
   count(dest, sort = TRUE) %>% 
   head(10))
semi_join(flights, top_desc)

# anti_join(x, y) drops all observations in x that have a match in y. Anti-joins are useful for diagnosing join mismatches. 
```

## Set operations

```{r set}
# All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets.
# intersect(x, y): return only observations in both x and y.
# union(x, y): return unique observations in x and y.
# setdiff(x, y): return observations in x, but not in y.

(df1 <- tribble(
  ~x, ~y,
   1,  1,
   2,  1
))
(df2 <- tribble(
  ~x, ~y,
   1,  1,
   1,  2
))
intersect(df1, df2)
union(df1, df2)
setdiff(df1, df2)
setdiff(df2, df1)
```

# Strings

## String basics

```{r basic}
# To include a literal single or double quote in a string you can use \ to “escape” it.
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
# if you want to include a literal backslash, you’ll need to double it up: "\\"

# the printed representation shows the escapes. To see the raw contents of the string, use writeLines()
(x <- c("\"", "\\"))
writeLines(x)

# "\n": newline; "\t": tab. complete list: ?'"', or ?"'". like "\u00b5", this is a way of writing non-English characters that works on all platforms.
(x <- "\u00b5")
writeLines(x)
```

## String length

```{r length}
# str_length() tells you the number of characters in a string.
str_length(c("a", "R for data science", NA)) # include space

# The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions.
```

